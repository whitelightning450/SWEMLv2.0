{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script v2 for the SWEML v2.0\n",
    "This .ipynb script uses python module for processing predownloaded NASA ASO observations by Water Year, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1ee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "HOME = os.getcwd()\n",
    "\n",
    "#If you get a proj.db error below, run the following and put the following into the terminal\n",
    "# import pyproj\n",
    "# # Get the PROJ data directory\n",
    "# proj_data_dir = pyproj.datadir.get_data_dir()\n",
    "# proj_db_path = proj_data_dir + \"/proj.db\"\n",
    "# os.environ['PROJ_LIB'] =pyproj.datadir.get_data_dir()\n",
    "# os.environ['PROJ_LIB']\n",
    "#set multiprocessing limits\n",
    "CPUS = len(os.sched_getaffinity(0))\n",
    "CPUS = int((CPUS/2)-2)\n",
    "\n",
    "#set home to the head of the SWEMLv2.0 directory\n",
    "HOME = os.chdir('..')\n",
    "HOME = os.getcwd()\n",
    "\n",
    "#Add your module here\n",
    "# from utils.ASOget import ASODataProcessing_v2\n",
    "import utils.get_InSitu_obs as get_InSitu_obs\n",
    "import utils.GeoDF as GeoDF \n",
    "import utils.Obs_to_DF as Obs_to_DF \n",
    "import utils.get_VIIRS_SCA as get_VIIRS_SCA\n",
    "import utils.get_Precip as get_Precip\n",
    "import utils.get_Seasonality as get_Seasonality\n",
    "import utils.vegetation_processer as vegpro\n",
    "import utils.sturm_processer as stpro\n",
    "from utils import ASOget\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#make SWEMLv2.0 modeling domain for western USA\n",
    "WY_list = np.arange(2013,2025)\n",
    "# WY_list = [2013]\n",
    "output_res = 750 #desired spatial resolution in meters (m)\n",
    "threshold = 10\n",
    "print(f\"The current session is using {WY_list} years, {output_res}m resolution, and {CPUS} CPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469889",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inputs for fetching ASO data for a region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "directory = \"Raw_ASO_Data\"\n",
    "\n",
    "#Get ASO data, sometime sites will give error and break code, most times you can just rerun it using the data_processor sections below (e.g., comment out other parts\n",
    "for WY in WY_list:\n",
    "    #Convert ASO tifs to parquet\n",
    "    print(f\"Converting ASO images for WY: {WY}\")\n",
    "    folder_name = f\"{WY}/{directory}\"\n",
    "    data_processor = ASOget.ASODataProcessing_v2() #note, 2019-5-1, 2019-06-11 seems to be bad, manually removed from SW region\n",
    "    data_processor.convert_tiff_to_parquet_multiprocess(folder_name, output_res, WY) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0597",
   "metadata": {},
   "source": [
    "## Get Snotel and CDEC in situ observations\n",
    "- Ideas - add nearest sites elevation, distance from cell, then can bypass sites with bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2a3c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only needed once. Other spatial resolutions can use the same data\n",
    "#Get in situ observations\n",
    "\n",
    "#make a list of dates to align with the ASO observations (they go as early as Jan-29 and as far out as the July-17)\n",
    "years = np.arange(2013,2025,1)\n",
    "start_month_day = '10-01'\n",
    "end_month_day = '08-31'\n",
    "\n",
    "# observations \n",
    "get_InSitu_obs.Get_Monitoring_Data_Threaded_Updated(years, start_month_day, end_month_day, WY = True)\n",
    "\n",
    "#combine years\n",
    "get_InSitu_obs.combine_dfs(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "## Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        #load snotel meta location data, use haversine function\n",
    "        GeoDF.fetch_snotel_sites_for_cellids(WY, output_res) # Using known up to date sites\n",
    "\n",
    "        # Get geophysical attributes for each site, need to see how to add output resolution\n",
    "        gdf = GeoDF.GeoSpatial(WY, output_res)\n",
    "\n",
    "        #use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "        metadf = GeoDF.extract_terrain_data_threaded(gdf, WY, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72349ac4",
   "metadata": {},
   "source": [
    "## Connect Snotel to each ASO obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a5df6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Connect nearest snotel observations with ASO data, makes a parquet file for each date  -  test to see if this works - need to just load the SNOTEL file, not collect them as in the function\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        dates = []\n",
    "        manual = False\n",
    "        Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(WY, output_res, manual, dates) \n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7816b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Connect cell ids with ASO obs and snotel obs to geospatial features\n",
    "for WY in WY_list:\n",
    "    #path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        GeoDF.add_geospatial_threaded(WY, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facee8dc",
   "metadata": {},
   "source": [
    "### Get NASA VIIRS fraction snow covered area for each location \n",
    "\n",
    "* Make sure the code grabs all dates for each region, may have to run multiple times\n",
    "* run until \"No granules found for DATE, requesting data from NSIDC...\" no longer occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fe64b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check to see if the VIIRS data is available locally, if not, get from CIROH AWS - I think all of this data is for the incorrect year...\n",
    "#get_VIIRS_SCA.get_VIIRS_from_AWS()\n",
    "\n",
    "#Connect VIIRS data to dataframes\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/data/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        get_VIIRS_SCA.augment_SCA_multiprocessing(WY, output_res, threshold)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96494a-ba0a-4376-9a98-2b9a632d4749",
   "metadata": {},
   "source": [
    "### Add seasonality metrics to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e511",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for WY in WY_list:\n",
    "    #process snotel sites to make \"snow hydrograph features\" to determine above/below average WY conditions\n",
    "    get_Seasonality.seasonal_snotel()\n",
    "\n",
    "    #get the Day of season metric for each dataframe\n",
    "    get_Seasonality.add_Seasonality(WY, output_res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad085308",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use Sturm's snow classification as features within model framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4c09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#download sturm data\n",
    "stpro.get_Sturm_data()\n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    input_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    sturm_file = f\"{HOME}/data/SnowClassification/SnowClass_NA_300m_10.0arcsec_2021_v01.0.tif\" #https://nsidc.org/data/nsidc-0768/versions/1\n",
    "    output_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Sturm_Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    stpro.process_sturm_data_for_files(input_directory, sturm_file, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90eacf-432a-4d0b-a804-5cdfd62bfa0a",
   "metadata": {},
   "source": [
    "### Add vegetation data to the dataframe from the North American land Cover Management Systemoutput_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bed310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get data\n",
    "url = \"https://www.cec.org/files/atlas_layers/1_terrestrial_ecosystems/1_01_0_land_cover_2020_30m/usa_land_cover_2020v2_30m_tif.zip\"\n",
    "output_path = f\"{HOME}/data/LandCover/\"\n",
    "file = \"usa_land_cover_2020v2_30m_tif.zip\" \n",
    "# vegpro.get_data(url, output_path, file)\n",
    "#unzip the file is not already done\n",
    "# vegpro.unzip_LC_data(output_path, file)\n",
    "#output = 1000 \n",
    "\n",
    "for WY in WY_list:\n",
    "    print(WY)\n",
    "    input_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Sturm_Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    vegetation_file = f\"{HOME}/data/LandCover/usa_land_cover_2020v2_30m_tif/USA_NALCMS_landcover_2020v2_30m/data/USA_NALCMS_landcover_2020v2_30m.tif\"\n",
    "    output_directory = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    vegpro.process_vegetation_data_for_files(input_directory, vegetation_file, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf01e14-94ac-4c2e-a143-dc9215dda1c3",
   "metadata": {},
   "source": [
    "### Get Daymet, NLDAS, gridMET Precipitation for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d07e61-a124-415c-b2d6-7063c9afbe16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = [# 'Daymet',\n",
    "             'NLDAS',\n",
    "            #  'gridMET'\n",
    "           ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for WY in WY_list:\n",
    "        path = f\"{HOME}/../../SWEMLdata/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "        if os.path.isdir(path) == True:\n",
    "            print(WY)\n",
    "            get_Precip.get_precip(WY, output_res, threshold, dataset)\n",
    "        else:\n",
    "            print(f\"No ASO data for {WY}, {path}\")\n",
    "\n",
    "        #Connect precipitation to processed DFs\n",
    "        get_Precip.Make_Precip_DF(WY, output_res, threshold, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc3fab-ed8e-465c-bc6d-cc647edc1f90",
   "metadata": {},
   "source": [
    "### Get AORC Precip for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41690974-8129-42ff-8332-1af16a671285",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'AORC'\n",
    "for WY in WY_list:\n",
    "    path = f\"{HOME}/../../SWEMLdata/ASO/{WY}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(WY)\n",
    "        get_Precip.get_aorc_precip(WY, output_res, threshold)\n",
    "    else:\n",
    "        print(f\"No ASO data for {WY}, {path}\")\n",
    "\n",
    "    #Connect precipitation to processed DFs\n",
    "    get_Precip.Make_Precip_DF(WY, output_res, threshold, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26bff36",
   "metadata": {},
   "source": [
    "### Get PRISM Precip for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5281511",
   "metadata": {},
   "outputs": [],
   "source": [
    "for WY in WY_list:\n",
    "    # get_Precip.get_prism(WY)\n",
    "    get_Precip.add_prism_df(WY,output_res,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e51b09",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "* Explore why errors in precip sites above\n",
    "* add in situ obs - seasonality based on the historical neareste x monitoring stations - like a historical average to-date swe value unit hydrograph based on the day of year? This will include a historical time of year of normal swe value and a swe value of year compared to normal\n",
    "* albedo metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "region = 'Southwest'\n",
    "output_res = '300'\n",
    "\n",
    "dfpath = f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution\"\n",
    "\n",
    "SWmeta = pd.read_parquet(f\"{dfpath}/{region}_metadata.parquet\")\n",
    "\n",
    "import UpdateDataFrame\n",
    "\n",
    "#need to update the topographic features for every dataframe\n",
    "output_res = '300'\n",
    "training_cats = ['Obsdf']\n",
    "fSCA = '' #'20_fSCA_Thresh'\n",
    "\n",
    "\n",
    "for training_cat in training_cats:\n",
    "    print(training_cat)\n",
    "\n",
    "    for region in region_list:\n",
    "        print(region)\n",
    "        dfpath = f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution\"\n",
    "        #file to be used to updated training DF\n",
    "        updatefile = pd.read_parquet(f\"{dfpath}/{region}_metadata.parquet\")\n",
    "\n",
    "\n",
    "        #Update Dataframe\n",
    "        UpdateDataFrame.updateTrainingDF(region, output_res, training_cat, fSCA, updatefile)\n",
    "\n",
    "trainfile = pd.read_parquet(f\"{dfpath}/{training_cat}/{fSCA}/Sturm_Season_Precip_VIIRS_GeoObsDF_20150406.parquet\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def SpatialAnalysis(EvalDF):\n",
    "    #Convert to a geopandas DF\n",
    "    Pred_Geo = gpd.GeoDataFrame(EvalDF, geometry = gpd.points_from_xy(EvalDF.cen_lon, EvalDF.cen_lat))\n",
    "\n",
    "    Pred_Geo.plot(column='Elevation_m',\n",
    "                  legend=False,\n",
    "                )\n",
    "    \n",
    "SpatialAnalysis(trainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f70f64-8097-451d-ac30-e511a895e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nans\n",
    "years = np.arange(2013,2024)\n",
    "# years = [2020,2021,2022]\n",
    "cols = ['cen_lat', 'cen_lon', 'Elevation_m', 'Slope_Deg',\n",
    "       'Aspect_Deg', 'swe_m', 'ns_1', 'ns_2', 'ns_3', 'ns_4', 'ns_5', 'ns_6',\n",
    "       'VIIRS_SCA', 'hasSnow', 'DOS', 'WY_week', 'ns_1_week_mean',\n",
    "       'ns_2_week_mean', 'ns_3_week_mean', 'ns_4_week_mean', 'ns_5_week_mean',\n",
    "       'ns_6_week_mean', 'ns_1_anomoly', 'ns_2_anomoly', 'ns_3_anomoly',\n",
    "       'ns_4_anomoly', 'ns_5_anomoly', 'ns_6_anomoly', \n",
    "        # 'sturm_value',       'vegetation_value', 'Daymet', 'NLDAS'\n",
    "       ]\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    files = [filename for filename in os.listdir(f\"{HOME}/data/TrainingDFs/{year}/1000M_Resolution/NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh/\")]\n",
    "    # files = [filename for filename in os.listdir(f\"{HOME}/../Johnson/SWEMLv2.0/data/TrainingDFs/{year}/1000M_Resolution/Sturm_Seasonality_PrecipVIIRSGeoObsDFs/10_fSCA_Thresh/\")]\n",
    "\n",
    "    for file in files:\n",
    "        test = pd.read_parquet(f\"{HOME}/data/TrainingDFs/{year}/1000M_Resolution/NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh/{file}\")\n",
    "    # test = test.drop(columns='Date')\n",
    "    # test.reset_index().drop(columns='index').head()\n",
    "    # test.columns\n",
    "        for column in cols:\n",
    "            # print(len(test[test[column]<1]))\n",
    "            # if len(test[test[column]<1]) > 500:\n",
    "                # print(column)\n",
    "            if np.isnan(test[column]).any():\n",
    "                print('nan',file, column)\n",
    "            if np.isinf(test[column]).any():\n",
    "                print('inf',file,column)\n",
    "\n",
    "# test.drop(columns='Elevation_m')[test.drop(columns='Elevation_m')>250].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b3ac9-60c4-4e2c-9c5d-2126cb730a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_parquet('data/TrainingDFs/2021/750M_Resolution/AORCgridMETNLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh/AORC_gridMET_NLDAS_PrecipDaymet_Vegetation_Sturm_Season_VIIRS_GeoObsdf_Animas_20210419.parquet')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cf135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
