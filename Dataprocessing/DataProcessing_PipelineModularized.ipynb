{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script for the NSM/SWEML v2.0\n",
    "This .ipynb script uses python module for retrieving NASA ASO observations, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8bd58",
   "metadata": {},
   "source": [
    "# Next steps \n",
    "\n",
    "- the SE and SW rockies have the same number of sites, make sure they are not the same...\n",
    "- process ASO data, e.g. swe_m < 0.1 = 0, convert to cm to be consistent with monitoring sites and traditional mesurement. \n",
    "- document scripts\n",
    "- add new sites (e.g., regionval) to training DF with all the respective spatial resolution information\n",
    "- add precipitation phase features (seasonal accumulated rain precip, seasonal accumulated snow precip as a function of temperature)\n",
    "- explore adding other features stemming from SNOTEL, remote sensing (LULC), Snow Classifications (Sturms), energy balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b1ee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Southwest', 'Northwest', 'NorthernRockies', 'SouthernRockies']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import model_Domain\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "#make SWEMLv2.0 modeling domain for western USA\n",
    "region_list = model_Domain.modeldomain()\n",
    "region_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13469889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southwest\n",
      "Bounding Box collected for Southwest: -122.801796139143,36.29256774541929,-112.4801535246097,43.48412459980747\n",
      "Fetching file URLs in progress for Southwest from 2013-04-02T00:00:00Z to 2019-07-19T23:59:59Z\n",
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=ASO_50M_SWE&version=001&version=01&version=1&temporal[]=2013-04-02T00:00:00Z,2019-07-19T23:59:59Z&bounding_box=-122.801796139143,36.29256774541929,-112.4801535246097,43.48412459980747\n",
      "\n",
      "Found 131 matches.\n",
      "getting credentials NSIDC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e5064a9a0d4ad39a66179643ac1d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NASA ASO data collected for given date range and can be found in /home/jovyan/SWEMLv2.0/data/ASO/Southwest/Raw_ASO_Data...\n",
      "Files with .xml extension moved to the destination folder.\n",
      "Converting .tif to parquet\n",
      "Converting 131 ASO tif files to parquet'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330bc44630a8428cb898a5c8deb86368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking to make sure all files successfully converted...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e403fb69d04d6aa470768aaa42e288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad file conversion for ASO_300M_SWE_20170727.parquet, attempting to reprocess\n",
      "ASO_50M_SWE_USCACE_20170727.tif\n",
      "Attempt 2\n",
      "Bad file conversion for ASO_50M_SWE_USCACE_20170727.tif, attempting to reprocess\n",
      "Northwest\n",
      "Bounding Box collected for Northwest: -123.3407853096148,42.07988450615146,-120.65482261009741,48.92977030870274\n",
      "Fetching file URLs in progress for Northwest from 2013-04-02T00:00:00Z to 2019-07-19T23:59:59Z\n",
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=ASO_50M_SWE&version=001&version=01&version=1&temporal[]=2013-04-02T00:00:00Z,2019-07-19T23:59:59Z&bounding_box=-123.3407853096148,42.07988450615146,-120.65482261009741,48.92977030870274\n",
      "\n",
      "Found 2 matches.\n",
      "getting credentials NSIDC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f987b802b543448bd8ed9b454105bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NASA ASO data collected for given date range and can be found in /home/jovyan/SWEMLv2.0/data/ASO/Northwest/Raw_ASO_Data...\n",
      "Files with .xml extension moved to the destination folder.\n",
      "Converting .tif to parquet\n",
      "Converting 2 ASO tif files to parquet'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae43258af40436b950e26696ebf6d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking to make sure all files successfully converted...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db871a2baaf4566823efb284fa37034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NorthernRockies\n",
      "Bounding Box collected for NorthernRockies: -119.92718722996061,42.57135262910201,-107.1261944312574,48.97106570807965\n",
      "Fetching file URLs in progress for NorthernRockies from 2013-04-02T00:00:00Z to 2019-07-19T23:59:59Z\n",
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=ASO_50M_SWE&version=001&version=01&version=1&temporal[]=2013-04-02T00:00:00Z,2019-07-19T23:59:59Z&bounding_box=-119.92718722996061,42.57135262910201,-107.1261944312574,48.97106570807965\n",
      "\n",
      "Found no matches.\n",
      "getting credentials NSIDC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbb30a5ebc141a186e4cdf3d9578e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NASA ASO data collected for given date range and can be found in /home/jovyan/SWEMLv2.0/data/ASO/NorthernRockies/Raw_ASO_Data...\n",
      "Files with .xml extension moved to the destination folder.\n",
      "Converting .tif to parquet\n",
      "The folder 'NorthernRockies/Raw_ASO_Data' is empty.\n",
      "SouthernRockies\n",
      "Bounding Box collected for SouthernRockies: -113.0550753064462,33.35825378630481,-105.0780355834649,42.57135262910201\n",
      "Fetching file URLs in progress for SouthernRockies from 2013-04-02T00:00:00Z to 2019-07-19T23:59:59Z\n",
      "Querying for data:\n",
      "\thttps://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]=producer_granule_id&scroll=true&page_size=2000&short_name=ASO_50M_SWE&version=001&version=01&version=1&temporal[]=2013-04-02T00:00:00Z,2019-07-19T23:59:59Z&bounding_box=-113.0550753064462,33.35825378630481,-105.0780355834649,42.57135262910201\n",
      "\n",
      "Found 19 matches.\n",
      "getting credentials NSIDC\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7513fd0d3447c493c4cff17f67b634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NASA ASO data collected for given date range and can be found in /home/jovyan/SWEMLv2.0/data/ASO/SouthernRockies/Raw_ASO_Data...\n",
      "Files with .xml extension moved to the destination folder.\n",
      "Converting .tif to parquet\n",
      "Converting 19 ASO tif files to parquet'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5109c7cac2d44f3b892eb63a937153a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: '/home/jovyan/SWEMLv2.0/data/ASO/SouthernRockies/Processed_300M_SWE/ASO_300M_20190407.tif' not recognized as a supported file format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: Deleting /home/jovyan/SWEMLv2.0/data/ASO/SouthernRockies/Processed_300M_SWE/ASO_300M_20150602.tif failed:\n",
      "No such file or directory\n",
      "ERROR 1: Deleting /home/jovyan/SWEMLv2.0/data/ASO/SouthernRockies/Processed_300M_SWE/ASO_300M_20190407.tif failed:\n",
      "No such file or directory\n",
      "ERROR 6: Unable to determine files associated with /home/jovyan/SWEMLv2.0/data/ASO/SouthernRockies/Processed_300M_SWE/ASO_300M_20160403.tif, delete fails.\n",
      "ERROR 1: TIFFResetField:/home/jovyan/SWEMLv2.0/data/ASO/SouthernRockies/Processed_300M_SWE/ASO_300M_20190407.tif: Could not find tag 273.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking to make sure all files successfully converted...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1842431fa2524339ad072c6c9c5cf948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ASOget import ASODownload, ASODataProcessing\n",
    "\n",
    "# Inputs for fetching ASO data for a region - there is code in the ASO get file to make your own .netrc file, make sure it goes to the correct directory \n",
    "# (chmod og-rw /home/jovyan/.netrc for your netrc file for cloud computing/2i2c)\n",
    "short_name = 'ASO_50M_SWE'\n",
    "version = '1'\n",
    "time_start = '2013-04-02T00:00:00Z'\n",
    "time_end = '2019-07-19T23:59:59Z'\n",
    "#region_list = ['S_Sierras']\n",
    "output_res = 300 #desired spatial resoultion in meters (m)\n",
    "directory = \"Raw_ASO_Data\"\n",
    "\n",
    "#Get ASO data\n",
    "for region in region_list:\n",
    "    print(region)\n",
    "    folder_name = f\"{region}/{directory}\"\n",
    "    data_tool = ASODownload(short_name, version)\n",
    "    b_box = data_tool.BoundingBox(region)  \n",
    "    url_list = data_tool.cmr_search(time_start, time_end, region, b_box)\n",
    "    data_tool.cmr_download(directory, region)\n",
    "\n",
    "    #Convert ASO tifs to parquet\n",
    "    data_processor = ASODataProcessing()\n",
    "    data_processor.convert_tiff_to_parquet_multiprocess(folder_name, output_res, region) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0597",
   "metadata": {},
   "source": [
    "## Get Snotel and CDEC in situ observations\n",
    "- clean in situ observations, specifically the CDEC sites, need a data processing method to remove outtliers and nan/0 obs\n",
    "- put snotel sites in SI vs current in\n",
    "- Ideas - add nearest sites elevation, distance from cell, then can bypass sites with bad data. - change nearest_sites name to ns, change snotel to cm or meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in situ observations\n",
    "import get_InSitu_obs\n",
    "import numpy as np\n",
    "\n",
    "#make a list of dates to aligns with the ASO observations (they go as early as Jan-29 and as far out as the July-17)\n",
    "years = np.arange(2013,2020,1)\n",
    "start_month_day = '10-01'\n",
    "end_month_day = '07-31'\n",
    "#datelist = get_InSitu_obs.make_dates(years, start_month_day, end_month_day, WY = True)\n",
    "\n",
    "# observations \n",
    "get_InSitu_obs.Get_Monitoring_Data_Threaded_dp(years, start_month_day, end_month_day, WY = True)\n",
    "\n",
    "#combine years\n",
    "get_InSitu_obs.combine_dfs(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fab43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "# Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        #load snotel meta location data, use haversive function\n",
    "        GeoDF.fetch_snotel_sites_for_cellids(region, output_res) # Using known up to date sites\n",
    "\n",
    "        # Get geophysical attributes for each site, need to see how to add output resolution\n",
    "        gdf = GeoDF.GeoSpatial(region, output_res)\n",
    "\n",
    "        #use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "        metadf = GeoDF.extract_terrain_data_threaded(gdf, region, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72349ac4",
   "metadata": {},
   "source": [
    "## Connect Snotel to each ASO obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Obs_to_DF\n",
    "output_res = 300\n",
    "\n",
    "#Connect nearest snotel observations with ASO data, makes a parquet file for each date  -  test to see if this works\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        dates = []\n",
    "        manual = False\n",
    "        Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(region, output_res, manual, dates) \n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7816b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "output_res = 300\n",
    "\n",
    "#Connect cell ids with ASO obs and snotel obs to geospatial features\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        GeoDF.add_geospatial_threaded(region, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facee8dc",
   "metadata": {},
   "source": [
    "# Get NASA VIIRS fraction snow covered area for each location \n",
    "\n",
    "sometimes have to run twice, fix...\n",
    "\n",
    "\n",
    "* Need to re-run, did not grab all of the N_Co observations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fe64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(get_VIIRS_SCA)\n",
    "\n",
    "import get_VIIRS_SCA\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "#check to see if the VIIRS data is available locally, if not, get from CIROH AWS - I think all of this data is for the incorrect year...\n",
    "#get_VIIRS_SCA.get_VIIRS_from_AWS()\n",
    "\n",
    "#Connect VIIRS data to dataframes\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        get_VIIRS_SCA.augment_SCA_mutliprocessing(region, output_res, threshold)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a59828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_Sierras\n",
      "['2013-04-03', '2013-04-29', '2013-05-03', '2013-05-25', '2013-06-01', '2013-06-08', '2014-03-20', '2014-03-23', '2014-03-24', '2014-04-06', '2014-04-07', '2014-04-14', '2014-04-20', '2014-04-23', '2014-04-28', '2014-04-29', '2014-05-02', '2014-05-03', '2014-05-11', '2014-05-12', '2014-05-17', '2014-05-27', '2014-05-31', '2014-06-05', '2015-02-17', '2015-03-05', '2015-03-25', '2015-03-26', '2015-04-03', '2015-04-09', '2015-04-12', '2015-04-15', '2015-04-26', '2015-04-27', '2015-04-28', '2015-05-03', '2015-05-27', '2015-05-28', '2015-05-31', '2015-06-08', '2015-06-09', '2016-03-26', '2016-04-01', '2016-04-07', '2016-04-16', '2016-04-26', '2016-05-09', '2016-05-27', '2016-06-07', '2016-06-14', '2016-06-21', '2016-06-26', '2016-07-08', '2017-01-28', '2017-01-29', '2017-07-17', '2017-07-18', '2017-07-19', '2017-07-27', '2017-08-15', '2017-08-16', '2018-03-04', '2018-04-22', '2018-04-23', '2018-04-25', '2018-04-26', '2018-05-28', '2018-06-01', '2018-06-02', '2019-03-09', '2019-03-15', '2019-03-16', '2019-03-17', '2019-03-24', '2019-03-25', '2019-03-26', '2019-03-29', '2019-04-17', '2019-04-18', '2019-04-21', '2019-04-27', '2019-04-28', '2019-05-01', '2019-05-02', '2019-05-03', '2019-06-04', '2019-06-05', '2019-06-08', '2019-06-09', '2019-06-11', '2019-06-13', '2019-06-14', '2019-07-03', '2019-07-04', '2019-07-05', '2019-07-13', '2019-07-14', '2019-07-15', '2019-07-16']\n",
      "[2013, 2014, 2015, 2016, 2017, 2018, 2019] 2012-09-30 2019-07-17\n",
      "Getting daily precipitation data for 299557 sites\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 24769/299557 [00:04<00:34, 7981.38it/s]"
     ]
    }
   ],
   "source": [
    "import get_Precip\n",
    "\n",
    "'''\n",
    "note*, if using python > 3.9, you will likely need to change the ee package to from io import StringIO\n",
    "'''\n",
    "\n",
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "#gets precipitation for each location, accumulates it through the water year\n",
    "\n",
    "#set start/end date for a water year\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "region_list = ['S_Sierras', 'GBasin']\n",
    "\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        get_Precip.get_precip_threaded(region, output_res, years)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n",
    "\n",
    "    #Connect precipitation to processed DFs\n",
    "    get_Precip.Make_Precip_DF(region, output_res, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_Seasonality\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "#get the Day of season metric for each dataframe\n",
    "get_Seasonality.get_DOS(region, output_res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e51b09",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "* Explore why errors in precip sites above\n",
    "* add in situ obs - seasonality based on the historical neareste x monitoring stations - like a historical average to-date swe value unit hydrograph based on the day of year? This will include a historical time of year of normal swe value and a swe value of year compared to normal\n",
    "* albedo metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "\n",
    "Precippath = f\"{HOME}/SWEMLv2.0/data/Precipitation/{region}/{output_res}M_NLDAS_Precip/sites/\"\n",
    "\n",
    "pptfiles = [filename for filename in os.listdir(Precippath)]\n",
    "\n",
    "ppt = pd.read_parquet(f\"{Precippath}NLDAS_PPT_N_Co_Rockies_300M_39.015_-107.027.parquet\")\n",
    "ppt.rename(columns={'datetime':'Date'}, inplace = True)\n",
    "#ppt.set_index('cell_id', inplace=True)\n",
    "\n",
    "ppt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19154fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFpath = '/home/rjohnson18/SWEMLv2.0/data/TrainingDFs/N_Co_Rockies/300M_Resolution/PrecipVIIRSGeoObsDFs_20_fSCA_Thresh'\n",
    "geofile = 'Precip_VIIRS_GeoObsDF_20160404.parquet'\n",
    "\n",
    "GDF = pd.read_parquet(os.path.join(DFpath, geofile))\n",
    "GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(get_Seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83acb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_Seasonality\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "#process snotel sites to make \"snow hydrograph features\" to determine above/below average WY conditions\n",
    "get_Seasonality.seasonal_snotel()\n",
    "\n",
    "\n",
    "#get the Day of season metric for each dataframe\n",
    "get_Seasonality.add_Seasonality(region, output_res, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b66c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81c762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e1041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0cdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd73bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import os\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "\n",
    "DFpath = f'{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution/Seasonality_PrecipVIIRSGeoObsDFs_{threshold}_fSCA_Thresh'\n",
    "files = [filename for filename in os.listdir(DFpath)]\n",
    "\n",
    "df = pd.read_parquet(os.path.join(DFpath, files[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a unit hydrograph ish meetric for each site\n",
    "\n",
    "#load data\n",
    "DFpath = f'{HOME}/SWEMLv2.0/data/SNOTEL_Data'\n",
    "snotel =  pd.read_parquet(os.path.join(DFpath, 'seasonal_snotel.parquet'))\n",
    "\n",
    "#find location average peak swe and divide dataframe by this number\n",
    "#snotel = snotel/snotel.max(0)\n",
    "snotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "DFpath = f'{HOME}/SWEMLv2.0/data/SNOTEL_Data'\n",
    "snotel =  pd.read_parquet(os.path.join(DFpath, 'ground_measures.parquet'))\n",
    "\n",
    "#find location average peak swe and divide dataframe by this number\n",
    "#snotel = snotel/snotel.max(0)\n",
    "snotel = snotel.T\n",
    "\n",
    "#change bad values = 7.65006, 9.60454, 27.139000,22.172265, 31.247021\t  change - values to 0\n",
    "cols = snotel.columns\n",
    "for col in cols:\n",
    "    snotel[col][(snotel[col]> 7.65) & (snotel[col]< 7.651)] = 0\n",
    "    snotel[col][(snotel[col]> 9.604) & (snotel[col]< 9.605)] = 0\n",
    "    snotel[col][(snotel[col]> 27.139) & (snotel[col]< 23.140)] = 0\n",
    "    snotel[col][(snotel[col]> 22.172265) & (snotel[col]< 22.172266)] = 0\n",
    "    snotel[col][(snotel[col]> 31.242265) & (snotel[col]< 31.242266)] = 0\n",
    "    snotel[col][snotel[col]<0] = 0\n",
    "snotel.reset_index(inplace = True)\n",
    "\n",
    "#build in data checking script to fix outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel.loc[250:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "site = cols[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 12))\n",
    "ax.plot(snotel.index, snotel[site])\n",
    "\n",
    "ax.set(xlabel='date', ylabel='SWE',\n",
    "       title=f'{site} SWE time series')\n",
    "#ax.grid()\n",
    "plt.xticks(rotation=70)\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc60848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(s, window, thresh=2, return_all=False):\n",
    "    roll = s.rolling(window=window, min_periods=1, center=True)\n",
    "    avg = roll.mean()\n",
    "    std = roll.std(ddof=0)\n",
    "    z = s.sub(avg).div(std)   \n",
    "    m = z.between(-thresh, thresh)\n",
    "    \n",
    "    if return_all:\n",
    "        return z, avg, std, m\n",
    "    return s.where(m, avg)\n",
    "\n",
    "\n",
    "N = 1000\n",
    "np.random.seed(1)\n",
    "#df = pd.DataFrame({'MW': np.sin(np.linspace(0, 10, num=N))+np.random.normal(scale=0.6, size=N)})\n",
    "\n",
    "df =pd.DataFrame(snotel[cols[0]])\n",
    "\n",
    "z, avg, std, m = zscore(df[cols[0]], window=2, return_all=True)\n",
    "\n",
    "ax = plt.subplots(figsize=(22, 12))\n",
    "\n",
    "df[cols[0]].plot(label='data')\n",
    "avg.plot(label='mean')\n",
    "df.loc[~m, cols[0]].plot(label='outliers', marker='o', ls='')\n",
    "avg[~m].plot(label='replacement', marker='o', ls='')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "np.random.seed(1)\n",
    "df = pd.DataFrame({'MW': np.sin(np.linspace(0, 10, num=N))+np.random.normal(scale=0.6, size=N)})\n",
    "\n",
    "z, avg, std, m = zscore(df['MW'], window=50, return_all=True)\n",
    "\n",
    "ax = plt.subplots(figsize=(22, 12))\n",
    "\n",
    "df['MW'].plot(label='data')\n",
    "avg.plot(label='mean')\n",
    "df.loc[~m, 'MW'].plot(label='outliers', marker='o', ls='')\n",
    "avg[~m].plot(label='replacement', marker='o', ls='')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377546f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "sweml_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
