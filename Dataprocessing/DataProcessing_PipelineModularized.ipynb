{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script for the NSM/SWEML v2.0\n",
    "This .ipynb script uses python module for retrieving NASA ASO observations, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASOget import ASODownload, ASODataProcessing\n",
    "\n",
    "# Inputs for fetching ASO data for a region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "version = '1'\n",
    "time_start = '2013-04-02T00:00:00Z'\n",
    "time_end = '2019-07-19T23:59:59Z'\n",
    "region = 'S_Sierras'\n",
    "folder_name = f\"SWE_Data/{region}\"\n",
    "output_res = 100 #desired spatial resoultion in meters (m)\n",
    "directory = \"SWE_Data\"\n",
    "\n",
    "#Get ASO data\n",
    "data_tool = ASODownload(short_name, version)\n",
    "selected_region = data_tool.select_region(region)  # Call select_region on the instance, S Sierras is #2. There may be a need to modify this in order to cover CONUS, create region folders...\n",
    "\n",
    "print(f\"Fetching file URLs in progress for {selected_region} from {time_start} to {time_end}\")\n",
    "url_list = data_tool.cmr_search(time_start, time_end, data_tool.bounding_box)\n",
    "data_tool.cmr_download(directory, region)\n",
    "\n",
    "print('Converting .tif to csv')\n",
    "data_processor = ASODataProcessing()\n",
    "data_processor.convert_tiff_to_csv(folder_name, output_res, region) #Takes ~3 mins, can it be multithreaded?\n",
    "\n",
    "#Applying polygon geometries, please be patient, this step can take a few minutes...Converting to GeoDataFrame\n",
    "input_folder = f\"ASO/{output_res}M_SWE_csv/{region}\"\n",
    "metadata_file = f\"grid_cells_meta.csv\"\n",
    "output_folder = f\"Processed_SWE/{region}\"\n",
    "data_processor.process_folder(input_folder, metadata_file, output_folder) #Takes ~20 minutes. Can this be multithreaded?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "# Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "region = 'S_Sierras' #Should be done in above code block\n",
    "output_res = 100\n",
    "\n",
    "#load snotel meta location data, use haversive function\n",
    "GeoDF.fetch_snotel_sites_for_cellids(region) # Using known up to date sites, can this be threaded?\n",
    "\n",
    "# Get geophysical attributes for each site, need to see how to add output resolution\n",
    "gdf = GeoDF.GeoSpatial(region)\n",
    "#gdf = gdf.head(100)\n",
    "#use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "metadf = GeoDF.extract_terrain_data_threaded(gdf, region)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573a5df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting site observations with nearest monitoring network obs\n",
      "Loading observations from 2013-2019\n",
      "Loading goeospatial meta data for grids in S_Sierras\n",
      "Loading 100M resolution grids for S_Sierras region\n",
      "Processing datetime component of SNOTEL observation dataframe\n",
      "Loading all available processed ASO observations for the S_Sierras at 100M resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 1281592.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting 99 timesteps of observations for S_Sierras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 61.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job complete for connecting SNOTEL obs to sites/dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 15524/120433 [00:26<02:59, 585.71it/s]\u001b[A\n",
      "100%|██████████| 3110/3110 [00:05<00:00, 542.12it/s]/s]\n",
      "100%|██████████| 2832/2832 [00:05<00:00, 542.01it/s]\n",
      "100%|██████████| 2832/2832 [00:05<00:00, 537.31it/s]\n",
      "100%|██████████| 2832/2832 [00:05<00:00, 537.18it/s]\n",
      "100%|██████████| 14139/14139 [00:25<00:00, 558.51it/s]\n",
      "100%|██████████| 2823/2823 [00:05<00:00, 547.68it/s]\n",
      "\n",
      "100%|██████████| 27475/27475 [00:51<00:00, 533.42it/s]]\n",
      "100%|██████████| 2823/2823 [00:05<00:00, 540.42it/s]/s]\n",
      "100%|██████████| 2823/2823 [00:05<00:00, 533.39it/s]\n",
      "100%|██████████| 50371/50371 [01:33<00:00, 538.41it/s]\n",
      "100%|██████████| 50371/50371 [01:34<00:00, 535.33it/s]\n",
      "100%|██████████| 50371/50371 [01:33<00:00, 536.15it/s]\n",
      "100%|██████████| 50371/50371 [01:34<00:00, 534.47it/s]\n",
      "100%|██████████| 50371/50371 [01:34<00:00, 533.20it/s]\n",
      "100%|██████████| 120433/120433 [03:38<00:00, 551.57it/s]\n",
      "100%|██████████| 120433/120433 [03:38<00:00, 550.68it/s]\n",
      "100%|██████████| 120433/120433 [03:39<00:00, 548.11it/s]\n",
      "100%|██████████| 120433/120433 [03:38<00:00, 549.98it/s]\n",
      "100%|██████████| 120433/120433 [03:38<00:00, 551.37it/s]\n",
      "100%|██████████| 120433/120433 [03:37<00:00, 552.74it/s]\n",
      "100%|██████████| 65839/65839 [02:01<00:00, 542.26it/s]\n",
      "100%|██████████| 90253/90253 [02:46<00:00, 543.56it/s]\n",
      "100%|██████████| 85232/85232 [02:37<00:00, 539.76it/s]\n",
      "100%|██████████| 120280/120280 [03:39<00:00, 547.06it/s]\n",
      "100%|██████████| 120433/120433 [03:38<00:00, 550.71it/s]\n",
      "100%|██████████| 120433/120433 [03:38<00:00, 549.97it/s]\n",
      "100%|██████████| 104992/104992 [03:09<00:00, 552.61it/s]\n",
      "100%|██████████| 156050/156050 [04:39<00:00, 557.88it/s]\n",
      "100%|██████████| 104992/104992 [03:09<00:00, 554.11it/s]\n",
      "100%|██████████| 136858/136858 [04:04<00:00, 559.18it/s]\n",
      "100%|██████████| 288777/288777 [08:20<00:00, 577.06it/s]\n",
      "100%|██████████| 288776/288776 [08:17<00:00, 579.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import Obs_to_DF\n",
    "region = \"S_Sierras\"\n",
    "output_res = 100\n",
    "\n",
    "#Connect nearest snotel observations with ASO data, makes a parquet file for each date\n",
    "finaldf = Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(region, output_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7816b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading goeospatial meta data for grids in S_Sierras\n",
      "Loading all available processed ASO observations for the S_Sierras at 100M resolution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 565716.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 99 with geospatial data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 75.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job complete for connecting obs with geospatial data, the files can be found in /home/rjohnson18/SWEMLv2.0/data/TrainingDFs/S_Sierras/GeoObsDFs\n"
     ]
    }
   ],
   "source": [
    "import GeoDF\n",
    "\n",
    "region = 'S_Sierras'\n",
    "output_res = 100\n",
    "\n",
    "#Connect cell ids with ASO obs and snotel obs to geospatial features\n",
    "GeoDF.add_geospatial_threaded(region, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7de7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de858a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81392905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcb6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with metadata\n",
    "req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "            'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "# Column renaming and ordering\n",
    "Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                    'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                    'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                    'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "# Save the merged data to a new file\n",
    "output_filename = f\"{HOME}/SWEML/data/NSMv2.0/data/TrainingDFs/Merged_aso_snotel_data.parquet\"\n",
    "Result.to_csv(output_filename, index=False)\n",
    "display(Result.head(10))\n",
    "print(\"Processed and saved data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'S_Sierras'\n",
    "ASO_meta_loc_DF = pd.read_csv(f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/ASO_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c57e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect nearest snotel with ASO data, this should be last for now, need to add geophysical characteristics to the site first, then this...\n",
    "finaldf = GeoDF.Nearest_Snotel_2_obs(region, output_res, dropna = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadb296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fcbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b02171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9ca2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b0d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ff590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662e5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277563f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236591a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "    print(lat, lon, regions, tiles)\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "    #print(signed_asset)\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "    #print(tilearray)\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/Merged_aso_nearest_sites1.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f6975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050495d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b59f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aee4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093705e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\"\"\"\n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    with rasterio.open(input_file_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.out_meta\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "                         \n",
    "        with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "def download_viirs_sca(input_dir, output_dir, shapefile_path):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(shapefile_path, 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    \n",
    "    # Iterate through each year directory in the input directory\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        if os.path.isdir(year_folder_path):\n",
    "            # Extract year from the folder name (assuming folder names like 'WY2013')\n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):        \n",
    "                if file_name.endswith('.tif'):   \n",
    "                    parts = file_name.split('_')\n",
    "                    output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                    output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                    input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                    crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                    print(f\"Processed and saved {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_directory = r\"/home/vgindi/VIIRS_Data\"\n",
    "    output_directory = r\"/home/vgindi/VIIRS_Sierras\"\n",
    "    shapefile_path = r\"/home/vgindi/Provided_Data/low_sierras_points.shp\"\n",
    "    download_viirs_sca(input_directory, output_directory, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\"\"\"\n",
    "def processing_VIIRS(input_file, output_res):\n",
    "    try:\n",
    "        # Define the output file path for TIFFs using the original file name\n",
    "        output_folder_tiff = os.path.join(\"/home/vgindi/Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(input_dir, output_res):\n",
    "    # Iterate over subdirectories in the input directory\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(\"/home/vgindi/Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/vgindi/VIIRS_Sierras\"\n",
    "    output_res = 100  # Desired resolution in meters\n",
    "    process_and_convert_viirs(input_directory, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(self, row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "    # Import the metadata into a pandas DataFrame\n",
    "    pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    # Drop coordinates columns\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                         'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                         'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                         'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        # Drop rows where 'cell_id' is NaN\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        # Save the processed DataFrame to a CSV file\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"\"\n",
    "    metadata_path = r\"\"\n",
    "    output_folder = r\"\"\n",
    "    process_folder(input_folder, metadata_path, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
