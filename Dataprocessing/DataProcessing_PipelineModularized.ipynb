{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script for the NSM/SWEML v2.0\n",
    "This .ipynb script uses python module for retrieving NASA ASO observations, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8bd58",
   "metadata": {},
   "source": [
    "# Next steps \n",
    "- Revisist other scripts and convert to PyArrow/parquet Brocli compressed file storage\n",
    "- connect precip to DF,\n",
    "- VIIRS\n",
    "- add new sites (e.g., regionval) to training DF with all the respective spatial resolution information\n",
    "- connect regional data together to train model\n",
    "- connect different regions\n",
    "- add precipitation phase features (seasonal accumulated rain precip, seasonal accumulated snow precip as a function of temperature)\n",
    "- explore adding other features stemming from SNOTEL, remote sensing (LULC), Snow Classifications (Sturms), energy balance\n",
    "- add snotel script to functions\n",
    "\n",
    "Put all units in SI, while it should not matter for model training since they are being normalized, they will be more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13469889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting .tif to parquet\n",
      "Converting 131 ASO tif files to parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:00<00:00, 360.55it/s]\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GeoASCIIParams\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GDALMetadata\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GDALNoDataValue\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n",
      "Warning 1: TIFFReadDirectory:Wrong \"StripByteCounts\" field, ignoring and calculating from imagelength\n",
      "ERROR 1: Deleting /home/rjohnson18/SWEMLv2.0/data/ASO/S_Sierras/Processed_100M_SWE/ASO_100M_20170129.tif failed:\n",
      "No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: /home/rjohnson18/SWEMLv2.0/data/ASO/S_Sierras/Processed_100M_SWE/ASO_100M_20160426.tif: No such file or directory\n",
      "An error occurred: '/home/rjohnson18/SWEMLv2.0/data/ASO/S_Sierras/Processed_100M_SWE/ASO_100M_20180601.tif' not recognized as a supported file format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 6: Unable to determine files associated with /home/rjohnson18/SWEMLv2.0/data/ASO/S_Sierras/Processed_100M_SWE/ASO_100M_20180601.tif, delete fails.\n",
      "ERROR 4: Unable to open /home/rjohnson18/SWEMLv2.0/data/ASO/S_Sierras/Processed_100M_SWE/ASO_100M_20180601.tif to obtain file list.\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GDALMetadata\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GDALMetadata\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n",
      "Warning 1: TIFFReadDirectory:Wrong \"StripByteCounts\" field, ignoring and calculating from imagelength\n",
      "Warning 1: TIFFFetchNormalTag:ASCII value for tag \"GDALMetadata\" contains null byte in value; value incorrectly truncated during reading due to implementation limitations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: '/home/rjohnson18/SWEMLv2.0/data/ASO/S_Sierras/Processed_100M_SWE/ASO_100M_20190501.tif' not recognized as a supported file format.\n",
      "Checking to make sure all files successfully converted...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 54/99 [00:02<00:03, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad file conversion for ASO_100M_SWE_20180423.parquet, attempting to reprocess\n",
      "ASO_100M_20180423.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 56/99 [00:03<00:10,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2\n",
      "Bad file conversion for ASO_100M_20180423.tif, attempting to reprocess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 80/99 [00:05<00:00, 20.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad file conversion for ASO_100M_SWE_20190324.parquet, attempting to reprocess\n",
      "ASO_100M_20190324.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 83/99 [00:07<00:03,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2\n",
      "Bad file conversion for ASO_100M_20190324.tif, attempting to reprocess\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:08<00:00, 11.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from ASOget import ASODownload, ASODataProcessing\n",
    "\n",
    "# Inputs for fetching ASO data for a region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "version = '1'\n",
    "time_start = '2013-04-02T00:00:00Z'\n",
    "time_end = '2019-07-19T23:59:59Z'\n",
    "region = 'S_Sierras'\n",
    "output_res = 100 #desired spatial resoultion in meters (m)\n",
    "directory = \"Raw_ASO_Data\"\n",
    "folder_name = f\"{region}/{directory}\"\n",
    "\n",
    "#Get ASO data\n",
    "data_tool = ASODownload(short_name, version)\n",
    "b_box = data_tool.BoundingBox(region)  \n",
    "url_list = data_tool.cmr_search(time_start, time_end, region, b_box)\n",
    "data_tool.cmr_download(directory, region)\n",
    "\n",
    "#Convert ASO tifs to parquet\n",
    "data_processor = ASODataProcessing()\n",
    "data_processor.convert_tiff_to_parquet_multiprocess(folder_name, output_res, region) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc73e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all Geospatial prediction/observation files and concatenating into one dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:04<00:00, 21.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "region = 'S_Sierras'\n",
    "output_res = 100\n",
    "\n",
    "#make cell_id\n",
    "def make_cell_id(region, output_res, cen_lat, cen_lon):\n",
    "    cell_id = f\"{region}_{output_res}M_{cen_lat}_{cen_lon}\"\n",
    "    return cell_id\n",
    "\n",
    "aso_swe_files_folder_path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "print('Loading all Geospatial prediction/observation files and concatenating into one dataframe')\n",
    "for aso_swe_file in tqdm(os.listdir(aso_swe_files_folder_path)):\n",
    "    try:\n",
    "        aso_file = pd.read_parquet(os.path.join(aso_swe_files_folder_path, aso_swe_file), engine='fastparquet')\n",
    "    except:\n",
    "        print(aso_swe_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e79a2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cen_lat</th>\n",
       "      <th>cen_lon</th>\n",
       "      <th>swe_m</th>\n",
       "      <th>cell_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103986</th>\n",
       "      <td>37.739871</td>\n",
       "      <td>-119.189549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_37.73987131307046_-119.18954900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103987</th>\n",
       "      <td>37.739871</td>\n",
       "      <td>-119.188549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_37.73987131307046_-119.18854900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103988</th>\n",
       "      <td>37.739871</td>\n",
       "      <td>-119.187549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_37.73987131307046_-119.18754900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105174</th>\n",
       "      <td>37.738871</td>\n",
       "      <td>-119.190549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_37.73887131307046_-119.19054900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105175</th>\n",
       "      <td>37.738871</td>\n",
       "      <td>-119.189549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_37.73887131307046_-119.18954900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995285</th>\n",
       "      <td>36.989871</td>\n",
       "      <td>-119.640549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_36.98987131307046_-119.64054900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995286</th>\n",
       "      <td>36.989871</td>\n",
       "      <td>-119.639549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_36.98987131307046_-119.63954900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995287</th>\n",
       "      <td>36.989871</td>\n",
       "      <td>-119.638549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_36.98987131307046_-119.63854900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995288</th>\n",
       "      <td>36.989871</td>\n",
       "      <td>-119.637549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_36.98987131307046_-119.63754900...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995289</th>\n",
       "      <td>36.989871</td>\n",
       "      <td>-119.636549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S_Sierras_100M_36.98987131307046_-119.63654900...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>430416 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cen_lat     cen_lon  swe_m  \\\n",
       "103986  37.739871 -119.189549    0.0   \n",
       "103987  37.739871 -119.188549    0.0   \n",
       "103988  37.739871 -119.187549    0.0   \n",
       "105174  37.738871 -119.190549    0.0   \n",
       "105175  37.738871 -119.189549    0.0   \n",
       "...           ...         ...    ...   \n",
       "995285  36.989871 -119.640549    0.0   \n",
       "995286  36.989871 -119.639549    0.0   \n",
       "995287  36.989871 -119.638549    0.0   \n",
       "995288  36.989871 -119.637549    0.0   \n",
       "995289  36.989871 -119.636549    0.0   \n",
       "\n",
       "                                                  cell_id  \n",
       "103986  S_Sierras_100M_37.73987131307046_-119.18954900...  \n",
       "103987  S_Sierras_100M_37.73987131307046_-119.18854900...  \n",
       "103988  S_Sierras_100M_37.73987131307046_-119.18754900...  \n",
       "105174  S_Sierras_100M_37.73887131307046_-119.19054900...  \n",
       "105175  S_Sierras_100M_37.73887131307046_-119.18954900...  \n",
       "...                                                   ...  \n",
       "995285  S_Sierras_100M_36.98987131307046_-119.64054900...  \n",
       "995286  S_Sierras_100M_36.98987131307046_-119.63954900...  \n",
       "995287  S_Sierras_100M_36.98987131307046_-119.63854900...  \n",
       "995288  S_Sierras_100M_36.98987131307046_-119.63754900...  \n",
       "995289  S_Sierras_100M_36.98987131307046_-119.63654900...  \n",
       "\n",
       "[430416 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aso_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ac692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "region = 'S_Sierras'\n",
    "output_res = 100\n",
    "aso_swe_files_folder_path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "aso_swe_file = \"ASO_100M_SWE_20170717.parquet\"\n",
    "\n",
    "aso_file = pd.read_parquet(os.path.join(aso_swe_files_folder_path, aso_swe_file), engine='fastparquet')\n",
    "aso_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "\n",
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b672d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "region = 'S_Sierras'\n",
    "output_res = 100\n",
    "\n",
    "#make cell_id\n",
    "def make_cell_id(region, output_res, cen_lat, cen_lon):\n",
    "    cell_id = f\"{region}_{output_res}M_{cen_lat}_{cen_lon}\"\n",
    "    return cell_id\n",
    "\n",
    "aso_swe_files_folder_path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "\n",
    "metadf = pd.DataFrame()\n",
    "print('Loading all Geospatial prediction/observation files and concatenating into one dataframe')\n",
    "for aso_swe_file in tqdm(os.listdir(aso_swe_files_folder_path)):\n",
    "    try:\n",
    "        aso_file = pd.read_parquet(os.path.join(aso_swe_files_folder_path, aso_swe_file))\n",
    "        metadf = pd.concat([metadf, aso_file])\n",
    "    except:\n",
    "        print(aso_swe_file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make cell ids\n",
    "print('Identifying unique locations')\n",
    "tqdm.pandas()\n",
    "ASO_meta_loc_DF['cell_id'] = ASO_meta_loc_DF.progress_apply(lambda row: make_cell_id(region, output_res, row['cen_lat'], row['cen_lon']), axis=1)\n",
    "\n",
    "#ASO_meta_loc_DF.to_csv(f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/ASO_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a212119",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadf = ASO_meta_loc_DF.drop_duplicates('cell_id').set_index('cell_id')\n",
    "metadf.pop('swe_m')\n",
    "metadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "100/111111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727f8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b733e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(x_coordinate, y_coordinate, output_res):\n",
    "\n",
    "    degs = (output_res/111111)/2 #general formulat is three are 111,111m to one degree, divide by two because the given point is the centeroid\n",
    "    \n",
    "    '''returns 'BL_Coord_Long', 'BL_Coord_Lat', \n",
    "             'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "             'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "              'UL_Coord_Long', 'UL_Coord_Lat']\n",
    "\n",
    "    '''\n",
    "    #Bottom left\n",
    "    BL_Coord_Long = x_coordinate-degs\n",
    "    BL_Coord_Lat = y_coordinate-degs\n",
    "\n",
    "    #Upper left\n",
    "    UL_Coord_Long = x_coordinate-degs\n",
    "    UL_Coord_Lat = y_coordinate+degs\n",
    "\n",
    "    #Upper right\n",
    "    UR_Coord_Long = x_coordinate+degs\n",
    "    UR_Coord_Lat = y_coordinate+degs\n",
    "\n",
    "    #Lower right\n",
    "    BR_Coord_Long = x_coordinate+degs\n",
    "    BR_Coord_Lat = y_coordinate-degs\n",
    "\n",
    "    return BL_Coord_Long, BL_Coord_Lat, BR_Coord_Long, BR_Coord_Lat, UR_Coord_Long, UR_Coord_Lat, UL_Coord_Long, UL_Coord_Lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6015b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = metadf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2714c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "def create_polygon(row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efec6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bee3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aso_swe_file = 'ASO_100M_SWE_20190324.parquet'\n",
    "\n",
    "pd.read_parquet(os.path.join(aso_swe_files_folder_path, aso_swe_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(cell_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "[res.append(x) for x in cell_ids if x not in res]\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65314b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b58a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "file = f\"{HOME}/SWEMLv2.0/data/ASO/S_Sierras/100M_SWE_parquet/ASO_100M_SWE_20180423.parquet\"\n",
    "\n",
    "file = pd.read_parquet(file)\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c394f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a32185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5352fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3630260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "region = 'S_Sierras'\n",
    "directory = \"Raw_ASO_Data\"\n",
    "folder = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{directory}\"\n",
    "\n",
    "\n",
    "for index, url in enumerate(url_list, start=1):\n",
    "    filename = os.path.join(folder, url.split('/')[-1])  # Specify the full path to the file\n",
    "    print(filename)\n",
    "    #print('{0}/{1}: {2}'.format(str().zfill(len(str(len(url_list)))), len(url_list), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name, output_res, region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "# Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "region = 'S_Sierras' #Should be done in above code block\n",
    "output_res = 100\n",
    "\n",
    "#load snotel meta location data, use haversive function\n",
    "GeoDF.fetch_snotel_sites_for_cellids(region) # Using known up to date sites, can this be threaded?\n",
    "\n",
    "# Get geophysical attributes for each site, need to see how to add output resolution\n",
    "gdf = GeoDF.GeoSpatial(region)\n",
    "#gdf = gdf.head(100)\n",
    "#use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "metadf = GeoDF.extract_terrain_data_threaded(gdf, region)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Obs_to_DF\n",
    "region = \"S_Sierras\"\n",
    "output_res = 100\n",
    "\n",
    "#Connect nearest snotel observations with ASO data, makes a parquet file for each date\n",
    "finaldf = Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(region, output_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7816b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "region = 'S_Sierras'\n",
    "output_res = 100\n",
    "\n",
    "#Connect cell ids with ASO obs and snotel obs to geospatial features\n",
    "GeoDF.add_geospatial_threaded(region, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_Precip\n",
    "\n",
    "#gets precipitation for each location, accumulates it through the water year\n",
    "\n",
    "#set start/end date for a water year\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "region = 'S_Sierras'\n",
    "output_res = 100\n",
    "for year in years:\n",
    "    get_Precip.get_precip_threaded(year, region, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "region = 'S_Sierras'\n",
    "year = 2013\n",
    "output_res = 100\n",
    "\n",
    "Precippath = f\"{HOME}/SWEMLv2.0/data/Precipitation/{region}/{output_res}M_NLDAS_Precip/{year}\"\n",
    "\n",
    "ppt = pd.read_csv(f\"{Precippath}/NLDAS_PPT_2013-04-03.parquet\")\n",
    "\n",
    "ppt.set_index('cell_id', inplace=True)\n",
    "\n",
    "ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2eed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert DataFrame to Apache Arrow Table\n",
    "table = pa.Table.from_pandas(ppt)\n",
    "\n",
    "# Parquet with Brotli compression\n",
    "pq.write_table(table, f\"{Precippath}/PYARROW_NLDAS_PPT_2013-04-03.parquet\", compression='BROTLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81392905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pptparquet = pd.read_parquet(f\"{Precippath}/PYARROW_NLDAS_PPT_2013-04-03.parquet\")\n",
    "pptparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with metadata\n",
    "req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "            'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "# Column renaming and ordering\n",
    "Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                    'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                    'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                    'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "# Save the merged data to a new file\n",
    "output_filename = f\"{HOME}/SWEML/data/NSMv2.0/data/TrainingDFs/Merged_aso_snotel_data.parquet\"\n",
    "Result.to_csv(output_filename, index=False)\n",
    "display(Result.head(10))\n",
    "print(\"Processed and saved data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'S_Sierras'\n",
    "ASO_meta_loc_DF = pd.read_csv(f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/ASO_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c57e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect nearest snotel with ASO data, this should be last for now, need to add geophysical characteristics to the site first, then this...\n",
    "finaldf = GeoDF.Nearest_Snotel_2_obs(region, output_res, dropna = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadb296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fcbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b02171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9ca2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b0d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ff590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662e5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277563f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236591a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "    print(lat, lon, regions, tiles)\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "    #print(signed_asset)\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "    #print(tilearray)\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/Merged_aso_nearest_sites1.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f6975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050495d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b59f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aee4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093705e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\"\"\"\n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    with rasterio.open(input_file_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.out_meta\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "                         \n",
    "        with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "def download_viirs_sca(input_dir, output_dir, shapefile_path):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(shapefile_path, 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    \n",
    "    # Iterate through each year directory in the input directory\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        if os.path.isdir(year_folder_path):\n",
    "            # Extract year from the folder name (assuming folder names like 'WY2013')\n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):        \n",
    "                if file_name.endswith('.tif'):   \n",
    "                    parts = file_name.split('_')\n",
    "                    output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                    output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                    input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                    crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                    print(f\"Processed and saved {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_directory = r\"/home/vgindi/VIIRS_Data\"\n",
    "    output_directory = r\"/home/vgindi/VIIRS_Sierras\"\n",
    "    shapefile_path = r\"/home/vgindi/Provided_Data/low_sierras_points.shp\"\n",
    "    download_viirs_sca(input_directory, output_directory, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\"\"\"\n",
    "def processing_VIIRS(input_file, output_res):\n",
    "    try:\n",
    "        # Define the output file path for TIFFs using the original file name\n",
    "        output_folder_tiff = os.path.join(\"/home/vgindi/Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(input_dir, output_res):\n",
    "    # Iterate over subdirectories in the input directory\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(\"/home/vgindi/Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/vgindi/VIIRS_Sierras\"\n",
    "    output_res = 100  # Desired resolution in meters\n",
    "    process_and_convert_viirs(input_directory, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(self, row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "    # Import the metadata into a pandas DataFrame\n",
    "    pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    # Drop coordinates columns\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                         'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                         'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                         'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        # Drop rows where 'cell_id' is NaN\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        # Save the processed DataFrame to a CSV file\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"\"\n",
    "    metadata_path = r\"\"\n",
    "    output_folder = r\"\"\n",
    "    process_folder(input_folder, metadata_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying polygon geometries\n",
    "# input_folder = f\"ASO/{region}/{output_res}M_SWE_parquet/\"\n",
    "# metadata_file = f\"grid_cells_meta.csv\"\n",
    "# output_folder = f\"ASO/{region}/Processed_SWE\"\n",
    "# data_processor = ASODataProcessing()\n",
    "# data_processor.process_folder(input_folder, metadata_file, output_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7bddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
