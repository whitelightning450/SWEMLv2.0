{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script for the NSM/SWEML v2.0\n",
    "This .ipynb script uses python module for retrieving NASA ASO observations, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8bd58",
   "metadata": {},
   "source": [
    "# Next steps \n",
    "\n",
    "- the SE and SW rockies have the same number of sites, make sure they are not the same...\n",
    "- process ASO data, e.g. swe_m < 0.1 = 0, convert to cm to be consistent with monitoring sites and traditional mesurement. \n",
    "- document scripts\n",
    "- add new sites (e.g., regionval) to training DF with all the respective spatial resolution information\n",
    "- add precipitation phase features (seasonal accumulated rain precip, seasonal accumulated snow precip as a function of temperature)\n",
    "- explore adding other features stemming from SNOTEL, remote sensing (LULC), Snow Classifications (Sturms), energy balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2b1ee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Southwest', 'Northwest', 'NorthernRockies', 'SouthernRockies']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import model_Domain\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "#make SWEMLv2.0 modeling domain for western USA\n",
    "region_list = model_Domain.modeldomain()\n",
    "region_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASOget import ASODownload, ASODataProcessing\n",
    "\n",
    "# Inputs for fetching ASO data for a region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "version = '1'\n",
    "time_start = '2013-04-02T00:00:00Z'\n",
    "time_end = '2019-07-19T23:59:59Z'\n",
    "#region_list = ['S_Sierras']\n",
    "output_res = 300 #desired spatial resoultion in meters (m)\n",
    "directory = \"Raw_ASO_Data\"\n",
    "\n",
    "#Get ASO data\n",
    "for region in region_list:\n",
    "    print(region)\n",
    "    folder_name = f\"{region}/{directory}\"\n",
    "    data_tool = ASODownload(short_name, version)\n",
    "    b_box = data_tool.BoundingBox(region)  \n",
    "    url_list = data_tool.cmr_search(time_start, time_end, region, b_box)\n",
    "    data_tool.cmr_download(directory, region)\n",
    "\n",
    "    #Convert ASO tifs to parquet\n",
    "    data_processor = ASODataProcessing()\n",
    "    data_processor.convert_tiff_to_parquet_multiprocess(folder_name, output_res, region) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e0597",
   "metadata": {},
   "source": [
    "## Get Snotel and CDEC in situ observations\n",
    "- clean in situ observations, specifically the CDEC sites, need a data processing method to remove outtliers and nan/0 obs\n",
    "- Ideas - add nearest sites elevation, distance from cell, then can bypass sites with bad data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in situ observations\n",
    "import get_InSitu_obs\n",
    "import numpy as np\n",
    "\n",
    "#make a list of dates to aligns with the ASO observations (they go as early as Jan-29 and as far out as the July-17)\n",
    "years = np.arange(2013,2020,1)\n",
    "start_month_day = '10-01'\n",
    "end_month_day = '08-31'\n",
    "#datelist = get_InSitu_obs.make_dates(years, start_month_day, end_month_day, WY = True)\n",
    "\n",
    "# observations \n",
    "get_InSitu_obs.Get_Monitoring_Data_Threaded_dp(years, start_month_day, end_month_day, WY = True)\n",
    "\n",
    "#combine years\n",
    "get_InSitu_obs.combine_dfs(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "# Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "output_res = 300\n",
    "\n",
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        #load snotel meta location data, use haversive function\n",
    "        GeoDF.fetch_snotel_sites_for_cellids(region, output_res) # Using known up to date sites\n",
    "\n",
    "        # Get geophysical attributes for each site, need to see how to add output resolution\n",
    "        gdf = GeoDF.GeoSpatial(region, output_res)\n",
    "\n",
    "        #use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "        metadf = GeoDF.extract_terrain_data_threaded(gdf, region, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72349ac4",
   "metadata": {},
   "source": [
    "## Connect Snotel to each ASO obs\n",
    "\n",
    "- change nearest_sites name to ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Obs_to_DF\n",
    "output_res = 300\n",
    "\n",
    "#Connect nearest snotel observations with ASO data, makes a parquet file for each date  -  test to see if this works - need to just load the SNOTEL file, not collect them as in the function\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        dates = []\n",
    "        manual = False\n",
    "        Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(region, output_res, manual, dates) \n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7816b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "output_res = 300\n",
    "\n",
    "#Connect cell ids with ASO obs and snotel obs to geospatial features\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        GeoDF.add_geospatial_threaded(region, output_res)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facee8dc",
   "metadata": {},
   "source": [
    "# Get NASA VIIRS fraction snow covered area for each location \n",
    "\n",
    "* Make sure the code grabs all dates for each region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fe64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_VIIRS_SCA\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "#check to see if the VIIRS data is available locally, if not, get from CIROH AWS - I think all of this data is for the incorrect year...\n",
    "#get_VIIRS_SCA.get_VIIRS_from_AWS()\n",
    "\n",
    "#Connect VIIRS data to dataframes\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        get_VIIRS_SCA.augment_SCA_mutliprocessing(region, output_res, threshold)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a59828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southwest\n",
      "['2013-04-03', '2013-04-29', '2013-05-03', '2013-05-25', '2013-06-01', '2013-06-08', '2014-03-20', '2014-03-23', '2014-03-24', '2014-04-06', '2014-04-07', '2014-04-14', '2014-04-20', '2014-04-23', '2014-04-28', '2014-04-29', '2014-05-02', '2014-05-03', '2014-05-11', '2014-05-12', '2014-05-17', '2014-05-27', '2014-05-31', '2014-06-05', '2015-02-17', '2015-03-05', '2015-03-25', '2015-03-26', '2015-04-03', '2015-04-09', '2015-04-12', '2015-04-15', '2015-04-26', '2015-04-27', '2015-04-28', '2015-05-03', '2015-05-27', '2015-05-28', '2015-05-31', '2015-06-08', '2015-06-09', '2016-03-26', '2016-04-01', '2016-04-07', '2016-04-16', '2016-04-26', '2016-05-09', '2016-05-27', '2016-06-07', '2016-06-14', '2016-06-21', '2016-06-26', '2016-07-08', '2017-01-28', '2017-01-29', '2017-07-17', '2017-07-18', '2017-07-19', '2017-07-27', '2017-08-15', '2017-08-16', '2018-03-04', '2018-04-22', '2018-04-23', '2018-04-25', '2018-04-26', '2018-05-28', '2018-06-01', '2018-06-02', '2019-03-09', '2019-03-15', '2019-03-16', '2019-03-17', '2019-03-24', '2019-03-25', '2019-03-26', '2019-03-29', '2019-04-17', '2019-04-18', '2019-04-21', '2019-04-27', '2019-04-28', '2019-05-01', '2019-05-02', '2019-05-03', '2019-06-04', '2019-06-05', '2019-06-08', '2019-06-09', '2019-06-11', '2019-06-13', '2019-06-14', '2019-07-03', '2019-07-04', '2019-07-05', '2019-07-13', '2019-07-14', '2019-07-15', '2019-07-16']\n",
      "[2013, 2014, 2015, 2016, 2017, 2018, 2019] 2012-09-30 2019-07-17\n",
      "Getting daily precipitation data for 299555 sites\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299555/299555 [00:45<00:00, 6618.10it/s]\n",
      "Sleeping 0.57 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json, after 429\n",
      "Sleeping 0.65 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json, after 429\n",
      "Sleeping 1.33 seconds before retry 2 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json, after 429\n",
      "Sleeping 1.01 seconds before retry 1 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json, after 429\n",
      "Sleeping 2.07 seconds before retry 2 of 5 for request: POST https://earthengine.googleapis.com/v1/projects/earthengine-legacy/value:compute?prettyPrint=false&alt=json, after 429\n"
     ]
    }
   ],
   "source": [
    "import get_Precip\n",
    "\n",
    "'''\n",
    "note*, if using python > 3.9, you will likely need to change the ee package to from io import StringIO\n",
    "'''\n",
    "\n",
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "#gets precipitation for each location, accumulates it through the water year\n",
    "\n",
    "#set start/end date for a water year\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "for region in region_list:\n",
    "    path = f\"{HOME}/SWEMLv2.0/data/ASO/{region}/{output_res}M_SWE_parquet\"\n",
    "    if os.path.isdir(path) == True:\n",
    "        print(region)\n",
    "        get_Precip.get_precip_threaded(region, output_res, years)\n",
    "    else:\n",
    "        print(f\"No ASO data for {region}\")\n",
    "\n",
    "    #Connect precipitation to processed DFs\n",
    "    get_Precip.Make_Precip_DF(region, output_res, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_Seasonality\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "#get the Day of season metric for each dataframe\n",
    "get_Seasonality.get_DOS(region, output_res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e51b09",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "* Explore why errors in precip sites above\n",
    "* add in situ obs - seasonality based on the historical neareste x monitoring stations - like a historical average to-date swe value unit hydrograph based on the day of year? This will include a historical time of year of normal swe value and a swe value of year compared to normal\n",
    "* albedo metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "\n",
    "Precippath = f\"{HOME}/SWEMLv2.0/data/Precipitation/{region}/{output_res}M_NLDAS_Precip/sites/\"\n",
    "\n",
    "pptfiles = [filename for filename in os.listdir(Precippath)]\n",
    "\n",
    "ppt = pd.read_parquet(f\"{Precippath}NLDAS_PPT_N_Co_Rockies_300M_39.015_-107.027.parquet\")\n",
    "ppt.rename(columns={'datetime':'Date'}, inplace = True)\n",
    "#ppt.set_index('cell_id', inplace=True)\n",
    "\n",
    "ppt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19154fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFpath = '/home/rjohnson18/SWEMLv2.0/data/TrainingDFs/N_Co_Rockies/300M_Resolution/PrecipVIIRSGeoObsDFs_20_fSCA_Thresh'\n",
    "geofile = 'Precip_VIIRS_GeoObsDF_20160404.parquet'\n",
    "\n",
    "GDF = pd.read_parquet(os.path.join(DFpath, geofile))\n",
    "GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(get_Seasonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83acb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_Seasonality\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "#process snotel sites to make \"snow hydrograph features\" to determine above/below average WY conditions\n",
    "get_Seasonality.seasonal_snotel()\n",
    "\n",
    "\n",
    "#get the Day of season metric for each dataframe\n",
    "get_Seasonality.add_Seasonality(region, output_res, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b66c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81c762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e1041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0cdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd73bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import os\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "region = 'N_Co_Rockies'\n",
    "output_res = 300\n",
    "threshold = 20\n",
    "\n",
    "\n",
    "DFpath = f'{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution/Seasonality_PrecipVIIRSGeoObsDFs_{threshold}_fSCA_Thresh'\n",
    "files = [filename for filename in os.listdir(DFpath)]\n",
    "\n",
    "df = pd.read_parquet(os.path.join(DFpath, files[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a unit hydrograph ish meetric for each site\n",
    "\n",
    "#load data\n",
    "DFpath = f'{HOME}/SWEMLv2.0/data/SNOTEL_Data'\n",
    "snotel =  pd.read_parquet(os.path.join(DFpath, 'seasonal_snotel.parquet'))\n",
    "\n",
    "#find location average peak swe and divide dataframe by this number\n",
    "#snotel = snotel/snotel.max(0)\n",
    "snotel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "snotel_path = f\"{HOME}/SWEMLv2.0/data/SNOTEL_Data\"\n",
    "year_df = pd.read_parquet(f\"{snotel_path}/2015_ground_measures_dp.parquet\")\n",
    "\n",
    "year_df = year_df.replace({-9999.0: np.nan})\n",
    "year_df.head(5)\n",
    "\n",
    "cols = year_df.columns\n",
    "year_df[cols[0]] = pd.Series(year_df[cols[0]].values).interpolate(method='nearest').values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cols = year_df.columns\n",
    "year_df.reset_index(inplace=True)\n",
    "\n",
    "for s in np.arange(0,10,1):\n",
    "\n",
    "       site = cols[s]\n",
    "\n",
    "       fig, ax = plt.subplots(figsize=(22, 12))\n",
    "       ax.plot(year_df.index, year_df[site])\n",
    "\n",
    "       ax.set(xlabel='date', ylabel='SWE',\n",
    "              title=f'{site} SWE time series')\n",
    "       #ax.grid()\n",
    "       plt.xticks(rotation=70)\n",
    "       #fig.savefig(\"test.png\")\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "DFpath = f'{HOME}/SWEMLv2.0/data/SNOTEL_Data'\n",
    "snotel =  pd.read_parquet(os.path.join(DFpath, 'ground_measures.parquet'))\n",
    "\n",
    "#find location average peak swe and divide dataframe by this number\n",
    "#snotel = snotel/snotel.max(0)\n",
    "snotel = snotel.T\n",
    "\n",
    "#change bad values = 7.65006, 9.60454, 27.139000,22.172265, 31.247021\t  change - values to 0\n",
    "cols = snotel.columns\n",
    "for col in cols:\n",
    "    snotel[col][(snotel[col]> 7.65) & (snotel[col]< 7.651)] = 0\n",
    "    snotel[col][(snotel[col]> 9.604) & (snotel[col]< 9.605)] = 0\n",
    "    snotel[col][(snotel[col]> 27.139) & (snotel[col]< 23.140)] = 0\n",
    "    snotel[col][(snotel[col]> 22.172265) & (snotel[col]< 22.172266)] = 0\n",
    "    snotel[col][(snotel[col]> 31.242265) & (snotel[col]< 31.242266)] = 0\n",
    "    snotel[col][snotel[col]<0] = 0\n",
    "snotel.reset_index(inplace = True)\n",
    "\n",
    "#build in data checking script to fix outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel.loc[250:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "snotel_path = f\"{HOME}/SWEMLv2.0/data/SNOTEL_Data\"\n",
    "year_df = pd.read_parquet(f\"{snotel_path}/ground_measures_dp.parquet\")\n",
    "\n",
    "year_df = year_df.replace({-9999.0: np.nan})\n",
    "year_df.head(5)\n",
    "\n",
    "\n",
    "site = cols[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 12))\n",
    "ax.plot(snotel.index, snotel[site])\n",
    "\n",
    "ax.set(xlabel='date', ylabel='SWE',\n",
    "       title=f'{site} SWE time series')\n",
    "#ax.grid()\n",
    "plt.xticks(rotation=70)\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc60848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(s, window, thresh=2, return_all=False):\n",
    "    roll = s.rolling(window=window, min_periods=1, center=True)\n",
    "    avg = roll.mean()\n",
    "    std = roll.std(ddof=0)\n",
    "    z = s.sub(avg).div(std)   \n",
    "    m = z.between(-thresh, thresh)\n",
    "    \n",
    "    if return_all:\n",
    "        return z, avg, std, m\n",
    "    return s.where(m, avg)\n",
    "\n",
    "\n",
    "N = 1000\n",
    "np.random.seed(1)\n",
    "#df = pd.DataFrame({'MW': np.sin(np.linspace(0, 10, num=N))+np.random.normal(scale=0.6, size=N)})\n",
    "\n",
    "df =pd.DataFrame(snotel[cols[0]])\n",
    "\n",
    "z, avg, std, m = zscore(df[cols[0]], window=2, return_all=True)\n",
    "\n",
    "ax = plt.subplots(figsize=(22, 12))\n",
    "\n",
    "df[cols[0]].plot(label='data')\n",
    "avg.plot(label='mean')\n",
    "df.loc[~m, cols[0]].plot(label='outliers', marker='o', ls='')\n",
    "avg[~m].plot(label='replacement', marker='o', ls='')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "np.random.seed(1)\n",
    "df = pd.DataFrame({'MW': np.sin(np.linspace(0, 10, num=N))+np.random.normal(scale=0.6, size=N)})\n",
    "\n",
    "z, avg, std, m = zscore(df['MW'], window=50, return_all=True)\n",
    "\n",
    "ax = plt.subplots(figsize=(22, 12))\n",
    "\n",
    "df['MW'].plot(label='data')\n",
    "avg.plot(label='mean')\n",
    "df.loc[~m, 'MW'].plot(label='outliers', marker='o', ls='')\n",
    "avg[~m].plot(label='replacement', marker='o', ls='')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377546f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
