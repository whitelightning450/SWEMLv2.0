{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685d109",
   "metadata": {},
   "source": [
    "# Data Processing script for the NSM/SWEML v2.0\n",
    "This .ipynb script uses python module for retrieving NASA ASO observations, locating nearest SNOTEL sites, connecting SNOTEL obs with ASO obs, and add geospatial features to the ML training/testing/hindcast dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASOget import ASODownload, ASODataProcessing\n",
    "\n",
    "# Inputs for fetching ASO data for a region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "version = '1'\n",
    "time_start = '2013-04-02T00:00:00Z'\n",
    "time_end = '2019-07-19T23:59:59Z'\n",
    "region = 'S_Sierras'\n",
    "folder_name = f\"SWE_Data/{region}\"\n",
    "output_res = 100 #desired spatial resoultion in meters (m)\n",
    "directory = \"SWE_Data\"\n",
    "\n",
    "#Get ASO data\n",
    "data_tool = ASODownload(short_name, version)\n",
    "selected_region = data_tool.select_region(region)  # Call select_region on the instance, S Sierras is #2. There may be a need to modify this in order to cover CONUS, create region folders...\n",
    "\n",
    "print(f\"Fetching file URLs in progress for {selected_region} from {time_start} to {time_end}\")\n",
    "url_list = data_tool.cmr_search(time_start, time_end, data_tool.bounding_box)\n",
    "data_tool.cmr_download(directory, region)\n",
    "\n",
    "print('Converting .tif to csv')\n",
    "data_processor = ASODataProcessing()\n",
    "data_processor.convert_tiff_to_csv(folder_name, output_res, region) #Takes ~3 mins, can it be multithreaded?\n",
    "\n",
    "#Applying polygon geometries, please be patient, this step can take a few minutes...Converting to GeoDataFrame\n",
    "input_folder = f\"ASO/{output_res}M_SWE_csv/{region}\"\n",
    "metadata_file = f\"grid_cells_meta.csv\"\n",
    "output_folder = f\"Processed_SWE/{region}\"\n",
    "data_processor.process_folder(input_folder, metadata_file, output_folder) #Takes ~20 minutes. Can this be multithreaded?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329ea64",
   "metadata": {},
   "source": [
    "# Code for generating ML dataframe using nearest in situ monitoring sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacde450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GeoDF\n",
    "\n",
    "# GeoDF used to create a dataframe for ML model development. Its function is to connect in situ observations to gridded locations\n",
    "region = 'S_Sierras' #Should be done in above code block\n",
    "output_res = 100\n",
    "\n",
    "#load snotel meta location data, use haversive function\n",
    "GeoDF.fetch_snotel_sites_for_cellids(region) # Using known up to date sites, can this be threaded?\n",
    "\n",
    "# Get geophysical attributes for each site, need to see how to add output resolution\n",
    "gdf = GeoDF.GeoSpatial(region)\n",
    "gdf = gdf.head(100)\n",
    "#use geodataframe with lat/long meta of all sites to determine slope, aspect, and elevation\n",
    "metadf = GeoDF.extract_terrain_data_threaded(gdf, region)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573a5df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting site observations with nearest monitoring network obs\n",
      "Loading observations from 2013-2019\n",
      "Loading goeospatial meta data for grids in S_Sierras\n",
      "Loading 100M resolution grids for S_Sierras region\n",
      "Processing datetime component of SNOTEL observation dataframe\n",
      "Loading all available processed ASO observations for the S_Sierras at 100M resolution\n",
      "Connecting 2 timesteps of observations for S_Sierras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job complete for connecting SNOTEL obs to sites/dates, processing into dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120433/120433 [03:58<00:00, 504.74it/s]\n",
      "100%|██████████| 120433/120433 [03:58<00:00, 504.46it/s]\n",
      "2it [04:30, 135.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting dataframe with geospatial features...\n"
     ]
    }
   ],
   "source": [
    "import Obs_to_DF\n",
    "region = \"S_Sierras\"\n",
    "output_res = 100\n",
    "\n",
    "#Connect nearest snotel observations with ASO data,\n",
    "finaldf = Obs_to_DF.Nearest_Snotel_2_obs_MultiProcess(region, output_res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b8874c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>cen_lat</th>\n",
       "      <th>cen_lon</th>\n",
       "      <th>geometry</th>\n",
       "      <th>Elevation_m</th>\n",
       "      <th>Slope_Deg</th>\n",
       "      <th>Aspect_Deg</th>\n",
       "      <th>swe</th>\n",
       "      <th>nearest_site_1</th>\n",
       "      <th>nearest_site_2</th>\n",
       "      <th>nearest_site_3</th>\n",
       "      <th>nearest_site_4</th>\n",
       "      <th>nearest_site_5</th>\n",
       "      <th>nearest_site_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11N_cell_-119.59073383567106_38.18624284828164</td>\n",
       "      <td>2013-04-03</td>\n",
       "      <td>38.185854</td>\n",
       "      <td>-119.590255</td>\n",
       "      <td>POINT (-119.5902546710551 38.18585423359679)</td>\n",
       "      <td>3167.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.972643</td>\n",
       "      <td>44.52</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>18.90</td>\n",
       "      <td>18.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11N_cell_-119.58959364631137_38.186209698720205</td>\n",
       "      <td>2013-04-03</td>\n",
       "      <td>38.185854</td>\n",
       "      <td>-119.589255</td>\n",
       "      <td>POINT (-119.58925467105512 38.18585423359679)</td>\n",
       "      <td>3168.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.536205</td>\n",
       "      <td>44.52</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>18.90</td>\n",
       "      <td>18.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11N_cell_-119.59309813700962_38.184509449472536</td>\n",
       "      <td>2013-04-03</td>\n",
       "      <td>38.184854</td>\n",
       "      <td>-119.593255</td>\n",
       "      <td>POINT (-119.5932546710551 38.1848542335968)</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>0.719424</td>\n",
       "      <td>44.52</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>18.90</td>\n",
       "      <td>18.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11N_cell_-119.59195797185825_38.18447632413384</td>\n",
       "      <td>2013-04-03</td>\n",
       "      <td>38.184854</td>\n",
       "      <td>-119.592255</td>\n",
       "      <td>POINT (-119.59225467105512 38.1848542335968)</td>\n",
       "      <td>3158.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.618731</td>\n",
       "      <td>44.52</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>18.90</td>\n",
       "      <td>18.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11N_cell_-119.59081780857791_38.184443187748315</td>\n",
       "      <td>2013-04-03</td>\n",
       "      <td>38.184854</td>\n",
       "      <td>-119.591255</td>\n",
       "      <td>POINT (-119.59125467105513 38.1848542335968)</td>\n",
       "      <td>3165.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.471944</td>\n",
       "      <td>44.52</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>18.90</td>\n",
       "      <td>18.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240861</th>\n",
       "      <td>11N_cell_-119.27205242391986_37.7395618848736</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.98</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.82974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240862</th>\n",
       "      <td>11N_cell_-119.27091951465975_37.73952599896746</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.101353</td>\n",
       "      <td>13.98</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.82974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240863</th>\n",
       "      <td>11N_cell_-119.26978660741051_37.739490102201515</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>13.98</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.82974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240864</th>\n",
       "      <td>11N_cell_-119.26978660741051_37.739490102201515</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.307413</td>\n",
       "      <td>13.98</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.82974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240865</th>\n",
       "      <td>11N_cell_-119.26865370217274_37.73945419457581</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.387032</td>\n",
       "      <td>13.98</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.82974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240866 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                cell_id       Date    cen_lat  \\\n",
       "0        11N_cell_-119.59073383567106_38.18624284828164 2013-04-03  38.185854   \n",
       "1       11N_cell_-119.58959364631137_38.186209698720205 2013-04-03  38.185854   \n",
       "2       11N_cell_-119.59309813700962_38.184509449472536 2013-04-03  38.184854   \n",
       "3        11N_cell_-119.59195797185825_38.18447632413384 2013-04-03  38.184854   \n",
       "4       11N_cell_-119.59081780857791_38.184443187748315 2013-04-03  38.184854   \n",
       "...                                                 ...        ...        ...   \n",
       "240861    11N_cell_-119.27205242391986_37.7395618848736 2013-04-29        NaN   \n",
       "240862   11N_cell_-119.27091951465975_37.73952599896746 2013-04-29        NaN   \n",
       "240863  11N_cell_-119.26978660741051_37.739490102201515 2013-04-29        NaN   \n",
       "240864  11N_cell_-119.26978660741051_37.739490102201515 2013-04-29        NaN   \n",
       "240865   11N_cell_-119.26865370217274_37.73945419457581 2013-04-29        NaN   \n",
       "\n",
       "           cen_lon                                       geometry  \\\n",
       "0      -119.590255   POINT (-119.5902546710551 38.18585423359679)   \n",
       "1      -119.589255  POINT (-119.58925467105512 38.18585423359679)   \n",
       "2      -119.593255    POINT (-119.5932546710551 38.1848542335968)   \n",
       "3      -119.592255   POINT (-119.59225467105512 38.1848542335968)   \n",
       "4      -119.591255   POINT (-119.59125467105513 38.1848542335968)   \n",
       "...            ...                                            ...   \n",
       "240861         NaN                                            NaN   \n",
       "240862         NaN                                            NaN   \n",
       "240863         NaN                                            NaN   \n",
       "240864         NaN                                            NaN   \n",
       "240865         NaN                                            NaN   \n",
       "\n",
       "        Elevation_m  Slope_Deg  Aspect_Deg       swe  nearest_site_1  \\\n",
       "0            3167.0        0.0       180.0  0.972643           44.52   \n",
       "1            3168.0        2.0        18.0  0.536205           44.52   \n",
       "2            3156.0        5.0       342.0  0.719424           44.52   \n",
       "3            3158.0        5.0        34.0  0.618731           44.52   \n",
       "4            3165.0        8.0        32.0  0.471944           44.52   \n",
       "...             ...        ...         ...       ...             ...   \n",
       "240861          NaN        NaN         NaN  0.000000           13.98   \n",
       "240862          NaN        NaN         NaN  0.101353           13.98   \n",
       "240863          NaN        NaN         NaN  1.565556           13.98   \n",
       "240864          NaN        NaN         NaN  1.307413           13.98   \n",
       "240865          NaN        NaN         NaN  1.387032           13.98   \n",
       "\n",
       "        nearest_site_2  nearest_site_3  nearest_site_4  nearest_site_5  \\\n",
       "0                49.00             0.0             0.1           18.90   \n",
       "1                49.00             0.0             0.1           18.90   \n",
       "2                49.00             0.0             0.1           18.90   \n",
       "3                49.00             0.0             0.1           18.90   \n",
       "4                49.00             0.0             0.1           18.90   \n",
       "...                ...             ...             ...             ...   \n",
       "240861            3.47             0.0             8.5            5.72   \n",
       "240862            3.47             0.0             8.5            5.72   \n",
       "240863            3.47             0.0             8.5            5.72   \n",
       "240864            3.47             0.0             8.5            5.72   \n",
       "240865            3.47             0.0             8.5            5.72   \n",
       "\n",
       "        nearest_site_6  \n",
       "0             18.80000  \n",
       "1             18.80000  \n",
       "2             18.80000  \n",
       "3             18.80000  \n",
       "4             18.80000  \n",
       "...                ...  \n",
       "240861         5.82974  \n",
       "240862         5.82974  \n",
       "240863         5.82974  \n",
       "240864         5.82974  \n",
       "240865         5.82974  \n",
       "\n",
       "[240866 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "#ASO observations\n",
    "aso_swe_files_folder_path = f\"{HOME}/SWEMLv2.0/data/Processed_SWE/{region}\"\n",
    "\n",
    "aso_swe_files = []\n",
    "for aso_swe_file in tqdm(os.listdir(aso_swe_files_folder_path)):  #add file names to aso_swe_files\n",
    "    aso_swe_files.append(aso_swe_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aso_swe_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7de7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Geospatial meta data\n",
    "print(f\"Loading goeospatial meta data for grids in {region}\")\n",
    "geodf_path = f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}\"\n",
    "\n",
    "aso_gdf = pd.read_csv(f\"{geodf_path}/{region}_metadata.parquet\")\n",
    "aso_gdf\n",
    "\n",
    "df = pd.merge(finaldf, aso_gdf, on = 'cell_id', how = 'left')\n",
    "cols = [\n",
    "    'cell_id', 'Date',  'cen_lat', 'cen_lon', 'geometry', 'Elevation_m', 'Slope_Deg',\n",
    "       'Aspect_Deg', 'swe', 'nearest_site 1', 'nearest site 2', 'nearest site 3', 'nearest site 4', \n",
    "       'nearest site 5', 'nearest site 6'\n",
    "      ]\n",
    "df = df[cols]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec78384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db66374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.merge(finaldf, aso_gdf, on = 'cell_id', how = 'left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de858a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81392905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcb6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with metadata\n",
    "req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "            'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "# Column renaming and ordering\n",
    "Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                    'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                    'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                    'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "# Save the merged data to a new file\n",
    "output_filename = f\"{HOME}/SWEML/data/NSMv2.0/data/TrainingDFs/Merged_aso_snotel_data.parquet\"\n",
    "Result.to_csv(output_filename, index=False)\n",
    "display(Result.head(10))\n",
    "print(\"Processed and saved data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'S_Sierras'\n",
    "ASO_meta_loc_DF = pd.read_csv(f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{region}/ASO_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c57e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect nearest snotel with ASO data, this should be last for now, need to add geophysical characteristics to the site first, then this...\n",
    "finaldf = GeoDF.Nearest_Snotel_2_obs(region, output_res, dropna = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadb296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fcbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b02171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9ca2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b0d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ff590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662e5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277563f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236591a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "    print(lat, lon, regions, tiles)\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "    #print(signed_asset)\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "    #print(tilearray)\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f281ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/Merged_aso_nearest_sites1.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f6975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050495d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b59f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aee4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093705e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\"\"\"\n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    with rasterio.open(input_file_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.out_meta\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "                         \n",
    "        with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "def download_viirs_sca(input_dir, output_dir, shapefile_path):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(shapefile_path, 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    \n",
    "    # Iterate through each year directory in the input directory\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        if os.path.isdir(year_folder_path):\n",
    "            # Extract year from the folder name (assuming folder names like 'WY2013')\n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):        \n",
    "                if file_name.endswith('.tif'):   \n",
    "                    parts = file_name.split('_')\n",
    "                    output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                    output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                    input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                    crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                    print(f\"Processed and saved {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_directory = r\"/home/vgindi/VIIRS_Data\"\n",
    "    output_directory = r\"/home/vgindi/VIIRS_Sierras\"\n",
    "    shapefile_path = r\"/home/vgindi/Provided_Data/low_sierras_points.shp\"\n",
    "    download_viirs_sca(input_directory, output_directory, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\"\"\"\n",
    "def processing_VIIRS(input_file, output_res):\n",
    "    try:\n",
    "        # Define the output file path for TIFFs using the original file name\n",
    "        output_folder_tiff = os.path.join(\"/home/vgindi/Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(input_dir, output_res):\n",
    "    # Iterate over subdirectories in the input directory\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(\"/home/vgindi/Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/vgindi/VIIRS_Sierras\"\n",
    "    output_res = 100  # Desired resolution in meters\n",
    "    process_and_convert_viirs(input_directory, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(self, row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "    # Import the metadata into a pandas DataFrame\n",
    "    pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    # Drop coordinates columns\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                         'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                         'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                         'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        # Drop rows where 'cell_id' is NaN\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        # Save the processed DataFrame to a CSV file\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"\"\n",
    "    metadata_path = r\"\"\n",
    "    output_folder = r\"\"\n",
    "    process_folder(input_folder, metadata_path, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
