{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582b19b-a266-4a82-9f9d-7d5ad1e3a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# Dataframe Packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Vector Packages\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# Raster Packages\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rioxarray.merge import merge_arrays\n",
    "import rasterstats as rs\n",
    "import osgeo\n",
    "from osgeo import gdalconst\n",
    "\n",
    "# Data Access Packages\n",
    "import earthaccess as ea\n",
    "import h5py\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from pystac_client import Client\n",
    "import richdem as rd\n",
    "import planetary_computer\n",
    "from planetary_computer import sign\n",
    "\n",
    "# General Packages\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from retrying import retry\n",
    "import fiona\n",
    "import re\n",
    "import s3fs\n",
    "\n",
    "#need to mamba install gdal, earthaccess \n",
    "#pip install pystac_client, richdem, planetary_computer, dask, distributed, retrying\n",
    "\n",
    "#connecting to AWS\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "import NSIDC_Data\n",
    "'''\n",
    "To create .netrc file:\n",
    "import earthaccess\n",
    "earthaccess.login(persist=True)\n",
    "open file and change machine to https://urs.earthdata.nasa.gov\n",
    "\n",
    "'''\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"SWEML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'national-snow-model'\n",
    "#S3 = boto3.resource('S3', config=Config(signature_version=UNSIGNED))\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6edbc-c701-42b3-89c9-4649ac4dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASODataTool:\n",
    "    def __init__(self, short_name, version, polygon='', filename_filter=''):\n",
    "        self.short_name = short_name\n",
    "        self.version = version\n",
    "        self.polygon = polygon\n",
    "        self.filename_filter = filename_filter\n",
    "        self.url_list = []\n",
    "        self.CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
    "        self.CMR_PAGE_SIZE = 2000\n",
    "        self.CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
    "                             '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
    "                             '&scroll=true&page_size={1}'.format(self.CMR_URL, self.CMR_PAGE_SIZE))\n",
    "\n",
    "    def cmr_search(self, time_start, time_end, bounding_box):\n",
    "        try:\n",
    "            if not self.url_list:\n",
    "                self.url_list = NSIDC_Data.cmr_search(\n",
    "                    self.short_name, self.version, time_start, time_end,\n",
    "                    bounding_box=self.bounding_box, polygon=self.polygon,\n",
    "                    filename_filter=self.filename_filter, quiet=False)\n",
    "            return self.url_list\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "\n",
    "    def cmr_download(self, directory):\n",
    "        dpath = f\"{HOME}/SWEML/data/NSMv2.0/data/ASO/{directory}\"\n",
    "        if not os.path.exists(dpath):\n",
    "            os.makedirs(dpath, exist_ok=True)\n",
    "\n",
    "        NSIDC_Data.cmr_download(self.url_list, dpath, False)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bounding_box(region):\n",
    "        try:\n",
    "            regions = pd.read_pickle(f\"{HOME}/SWEML/data/PreProcessed/RegionVal.pkl\")\n",
    "        except:\n",
    "            print('File not local, getting from AWS S3.')\n",
    "            key = f\"data/PreProcessed/RegionVal.pkl\"            \n",
    "            S3.meta.client.download_file(BUCKET_NAME, key,f\"{HOME}/SWEML/data/PreProcessed/RegionVal.pkl\")\n",
    "            regions = pd.read_pickle(f\"{HOME}/SWEML/data/PreProcessed/RegionVal.pkl\")\n",
    "\n",
    "\n",
    "        \n",
    "        superset = []\n",
    "\n",
    "        superset.append(regions[region])\n",
    "        superset = pd.concat(superset)\n",
    "        superset = gpd.GeoDataFrame(superset, geometry=gpd.points_from_xy(superset.Long, superset.Lat, crs=\"EPSG:4326\"))\n",
    "        bounding_box = list(superset.total_bounds)\n",
    "\n",
    "        return f\"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}\"\n",
    "\n",
    "class ASODownload(ASODataTool):\n",
    "    def __init__(self, short_name, version, polygon='', filename_filter=''):\n",
    "        super().__init__(short_name, version, polygon, filename_filter)\n",
    "        self.region_list =    [ 'N_Sierras',\n",
    "                                'S_Sierras',\n",
    "                                'Greater_Yellowstone',\n",
    "                                'N_Co_Rockies',\n",
    "                                'SW_Mont',\n",
    "                                'SW_Co_Rockies',\n",
    "                                'GBasin',\n",
    "                                'N_Wasatch',\n",
    "                                'N_Cascade',\n",
    "                                'S_Wasatch',\n",
    "                                'SW_Mtns',\n",
    "                                'E_WA_N_Id_W_Mont',\n",
    "                                'S_Wyoming',\n",
    "                                'SE_Co_Rockies',\n",
    "                                'Sawtooth',\n",
    "                                'Ca_Coast',\n",
    "                                'E_Or',\n",
    "                                'N_Yellowstone',\n",
    "                                'S_Cascade',\n",
    "                                'Wa_Coast',\n",
    "                                'Greater_Glacier',\n",
    "                                'Or_Coast'  ]\n",
    "\n",
    "    def select_region(self):\n",
    "        print(\"Select a region by entering its index:\")\n",
    "        for i, region in enumerate(self.region_list, start=1):\n",
    "            print(f\"{i}. {region}\")\n",
    "\n",
    "        try:\n",
    "            user_input = int(input(\"Enter the index of the region: \"))\n",
    "            if 1 <= user_input <= len(self.region_list):\n",
    "                selected_region = self.region_list[user_input - 1]\n",
    "                self.bounding_box = self.get_bounding_box(selected_region)\n",
    "                print(f\"You selected: {selected_region}\")\n",
    "                print(f\"Bounding Box: {self.bounding_box}\")\n",
    "            else:\n",
    "                print(\"Invalid index. Please select a valid index.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid index.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050667d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    short_name = 'ASO_50M_SWE'\n",
    "    version = '1'\n",
    "\n",
    "    data_tool = ASODownload(short_name, version)\n",
    "    time_start = '2013-04-02T00:00:00Z'\n",
    "    time_end = '2019-07-19T23:59:59Z'\n",
    "    \n",
    "    selected_region = data_tool.select_region()  # Call select_region on the instance\n",
    "    directory = \"SWE_Data\"\n",
    "\n",
    "    print(f\"Fetching file URLs in progress for {selected_region} from {time_start} to {time_end}\")\n",
    "    url_list = data_tool.cmr_search(time_start, time_end, data_tool.bounding_box)\n",
    "    data_tool.cmr_download(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec476660",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get all SWE_csv into the input folder\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b8ea0-b7c7-4c08-8cf7-69398a1493f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASODataProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def processing_tiff(input_file, output_res):\n",
    "        try:\n",
    "            date = os.path.splitext(input_file)[0].split(\"_\")[-1]\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_folder = os.path.join(os.getcwd(), \"Processed_Data\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            output_file = os.path.join(output_folder, f\"ASO_100M_{date}.tif\")\n",
    "    \n",
    "            ds = gdal.Open(input_file)\n",
    "            if ds is None:\n",
    "                print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "                return None\n",
    "            \n",
    "            # Reproject and resample\n",
    "            gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "    \n",
    "            # Read the processed TIFF file using rasterio\n",
    "            rds = rxr.open_rasterio(output_file)\n",
    "            rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "            rds.name = \"data\"\n",
    "            df = rds.to_dataframe().reset_index()\n",
    "            return df\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_tiff_to_csv(input_folder, output_res):\n",
    "\n",
    "        curr_dir = os.getcwd()\n",
    "        folder_path = os.path.join(curr_dir, input_folder)\n",
    "        \n",
    "        # Check if the folder exists and is not empty\n",
    "        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "            print(f\"The folder '{input_folder}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        if not os.listdir(folder_path):\n",
    "            print(f\"The folder '{input_folder}' is empty.\")\n",
    "            return\n",
    "    \n",
    "        tiff_files = [filename for filename in os.listdir(folder_path) if filename.endswith(\".tif\")]\n",
    "    \n",
    "        # Create CSV files from TIFF files\n",
    "        for tiff_filename in tiff_files:\n",
    "            \n",
    "            # Open the TIFF file\n",
    "            tiff_filepath = os.path.join(folder_path, tiff_filename)\n",
    "            df = ASODataProcessing.processing_tiff(tiff_filepath, output_res)\n",
    "    \n",
    "            if df is not None:\n",
    "                # Get the date from the TIFF filename\n",
    "                date = os.path.splitext(tiff_filename)[0].split(\"_\")[-1]\n",
    "    \n",
    "                # Define the CSV filename and folder\n",
    "                csv_filename = f\"ASO_SWE_{date}.csv\"\n",
    "                csv_folder = os.path.join(curr_dir, \"Processed_Data\", \"SWE_csv\")\n",
    "                os.makedirs(csv_folder, exist_ok=True)\n",
    "                csv_filepath = os.path.join(csv_folder, csv_filename)\n",
    "    \n",
    "                # Save the DataFrame as a CSV file\n",
    "                df.to_csv(csv_filepath, index=False)\n",
    "    \n",
    "                print(f\"Converted '{tiff_filename}' to '{csv_filename}'\")\n",
    "                \n",
    "    def create_polygon(self, row):\n",
    "        return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "\n",
    "    def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "        # Import the metadata into a pandas DataFrame\n",
    "        '''\n",
    "        input_folder = f\"{HOME}/data/NSMv2.0/data/Processed_Data/SWE_csv\"\n",
    "        metadata_path = f\"{HOME}/data/NSMv2.0/data/Provided_Data/grid_cells_meta.csv\"\n",
    "        output_folder = f\"{HOME}/data/NSMv2.0/data/Processed_SWE\"\n",
    "        '''\n",
    "        try:\n",
    "            pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "        except:\n",
    "            key = \"NSMv2.0\"+metadata_path.split(\"NSMv2.0\",1)[1]        \n",
    "            S3.meta.client.download_file(BUCKET_NAME, key,metadata_path)\n",
    "            pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "\n",
    "        # Get all SWE_csv into the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "            \n",
    "    \n",
    "        # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "        pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "        pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "    \n",
    "        # Convert the DataFrame to a GeoDataFrame\n",
    "        metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "    \n",
    "        # Drop coordinates columns\n",
    "        metadata_df = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                             'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                             'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                             'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "    \n",
    "        # List all CSV files in the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    \n",
    "        for csv_file in csv_files:\n",
    "            input_aso_path = os.path.join(input_folder, csv_file)\n",
    "            output_aso_path = os.path.join(output_folder, csv_file)\n",
    "    \n",
    "            # Check if the output file already exists\n",
    "            if os.path.exists(output_aso_path):\n",
    "                print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "                continue\n",
    "    \n",
    "            # Process each CSV file\n",
    "            aso_swe_df = pd.read_csv(input_aso_path)\n",
    "    \n",
    "            # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "            geometry = [Point(xy) for xy in zip(aso_swe_df['x'], aso_swe_df['y'])]\n",
    "            aso_swe_geo = gpd.GeoDataFrame(aso_swe_df, geometry=geometry)\n",
    "\n",
    "            result = gpd.sjoin(aso_swe_geo, metadata_df, how='left', predicate='within', op = 'intersects')\n",
    "    \n",
    "            # Select specific columns for the final DataFrame\n",
    "            Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "            Final_df.rename(columns={'data': 'swe'}, inplace=True)\n",
    "    \n",
    "            # Drop rows where 'cell_id' is NaN\n",
    "            if Final_df['cell_id'].isnull().values.any():\n",
    "                Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "    \n",
    "            # Save the processed DataFrame to a CSV file\n",
    "            Final_df.to_csv(output_aso_path, index=False)\n",
    "            print(f\"Processed {csv_file}\")\n",
    "            \n",
    "    def converting_ASO_to_standardized_format(self, input_folder, output_csv):\n",
    "        \n",
    "        # Initialize an empty DataFrame to store the final transformed data\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "        # Iterate through all CSV files in the directory\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "    \n",
    "                # Extract the time frame from the filename\n",
    "                time_frame = filename.split('_')[-1].split('.')[0]\n",
    "    \n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(file_path)\n",
    "    \n",
    "                # Rename the 'SWE' column to the time frame for clarity\n",
    "                df = df.rename(columns={'SWE': time_frame})\n",
    "    \n",
    "                # Merge or concatenate the data into the final DataFrame\n",
    "                if final_df.empty:\n",
    "                    final_df = df\n",
    "                else:\n",
    "                    final_df = pd.merge(final_df, df, on='cell_id', how='outer')\n",
    "    \n",
    "        # Save the final transformed DataFrame to a single CSV file\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #data_processor = ASODataProcessing()\n",
    "    #folder_name = \"SWE_Data\"\n",
    "    #output_res = 0.001\n",
    "    data_processor.convert_tiff_to_csv(folder_name, output_res)\n",
    "    input_folder = f\"{HOME}/data/v2.0/Processed_Data/SWE_csv\"\n",
    "    metadata_path = f\"{HOME}/data/v2.0/Provided_Data/grid_cells_meta.csv\"\n",
    "    output_folder = f\"{HOME}/data/v2.0/Processed_SWE\"\n",
    "\n",
    "    data_processor.process_folder(input_folder, metadata_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c91c7a-d327-4dbf-8669-1e667c0e4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aso_snotel_geometry(aso_swe_file, folder_path):\n",
    "    \n",
    "    aso_file = pd.read_csv(os.path.join(folder_path, aso_swe_file))\n",
    "    aso_file.set_index('cell_id', inplace=True)\n",
    "    aso_geometry = [Point(xy) for xy in zip(aso_file['x'], aso_file['y'])]\n",
    "    aso_gdf = gpd.GeoDataFrame(aso_file, geometry=aso_geometry)\n",
    "    \n",
    "    return aso_gdf\n",
    "\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    lon1 = np.radians(lon1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    r = 6371.0\n",
    "    # Distance calculation\n",
    "    distances = r * c\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6, distance_cache=None):\n",
    "\n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    for idx, aso_row in aso_gdf.iterrows():\n",
    "        cell_id = idx\n",
    "\n",
    "        # Check if distances for this cell_id are already calculated and cached\n",
    "        if cell_id in distance_cache:\n",
    "            nearest_snotel[idx] = distance_cache[cell_id]\n",
    "        else:\n",
    "            # Calculate Haversine distances between the grid cell and all SNOTEL locations\n",
    "            distances = haversine_vectorized(\n",
    "                aso_row.geometry.y, aso_row.geometry.x,\n",
    "                snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "\n",
    "            # Store the nearest stations in the cache\n",
    "            nearest_snotel[idx] = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "            distance_cache[cell_id] = nearest_snotel[idx]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def calculate_distances_for_cell(aso_row, snotel_gdf, n=6):\n",
    "   \n",
    "    distances = haversine_vectorized(\n",
    "        aso_row.geometry.y, aso_row.geometry.x,\n",
    "        snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "    \n",
    "    nearest_sites = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "    \n",
    "    return nearest_sites\n",
    "\n",
    "def calculate_nearest_snotel_parallel(aso_gdf, snotel_gdf, n = 6, distance_cache = None):\n",
    "    \n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    with ProcessPoolExecutor(max_workers = 16) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, aso_row in aso_gdf.iterrows():\n",
    "            if idx not in distance_cache:\n",
    "                # Submit the task for parallel execution\n",
    "                futures.append(executor.submit(calculate_distances_for_cell, aso_row, snotel_gdf, n))\n",
    "            else:\n",
    "                nearest_snotel[idx] = distance_cache[idx]\n",
    "\n",
    "        # Retrieve results as they are completed\n",
    "        for future in tqdm(futures):\n",
    "            result = future.result()\n",
    "  \n",
    "            cell_id = result[0]  \n",
    "            nearest_snotel[cell_id] = result[1]\n",
    "            distance_cache[cell_id] = result[1]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def fetch_snotel_sites_for_cellids(aso_swe_files_folder_path, metadata_path, snotel_data_path):\n",
    "    \n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    #metadata_df['geometry'] = metadata_df['geometry'].apply(wkt.loads)\n",
    "    \n",
    "    def create_polygon(row):\n",
    "        return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "        \n",
    "    metadata_df = metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    metadata_df['geometry'] = metadata_df.apply(create_polygon, axis=1)\n",
    "    \n",
    "    metadata = gpd.GeoDataFrame(metadata_df, geometry='geometry')\n",
    "    snotel_data = pd.read_csv(snotel_data_path)\n",
    "\n",
    "    date_columns = snotel_data.columns[1:]\n",
    "    new_column_names = {col: pd.to_datetime(col, format='%Y-%m-%d').strftime('%Y%m%d') for col in date_columns}\n",
    "    snotel_data_f = snotel_data.rename(columns=new_column_names)\n",
    "\n",
    "    snotel_file = pd.read_csv(\"/home/vgindi/Provided_Data/ground_measures_metadata.csv\")\n",
    "    snotel_geometry = [Point(xy) for xy in zip(snotel_file['longitude'], snotel_file['latitude'])]\n",
    "    snotel_gdf = gpd.GeoDataFrame(snotel_file, geometry=snotel_geometry)\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for aso_swe_file in os.listdir(aso_swe_files_folder_path):\n",
    "\n",
    "        if os.path.isdir(os.path.join(aso_swe_files_folder_path, aso_swe_file)):\n",
    "            continue\n",
    "\n",
    "        timestamp = aso_swe_file.split('_')[-1].split('.')[0]\n",
    "        print(f\"Processing file with timestamp: {timestamp}\")\n",
    "\n",
    "        aso_gdf = load_aso_snotel_geometry(aso_swe_file, aso_swe_files_folder_path)\n",
    "        aso_swe_data = pd.read_csv(os.path.join(aso_swe_files_folder_path, aso_swe_file))\n",
    "\n",
    "        # Calculating nearest SNOTEL sites\n",
    "        nearest_snotel, distance_cache = calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6)\n",
    "        print(f\"calculated nearest snotel for file with timestamp {timestamp}\")\n",
    "\n",
    "        transposed_data = {}\n",
    "\n",
    "        if timestamp in new_column_names.values():\n",
    "            for idx, aso_row in aso_gdf.iterrows():    \n",
    "                cell_id = idx\n",
    "                station_ids = nearest_snotel[cell_id]\n",
    "                selected_snotel_data = snotel_data_f[['station_id', timestamp]].loc[snotel_data_f['station_id'].isin(station_ids)]\n",
    "                station_mapping = {old_id: f\"nearest site {i+1}\" for i, old_id in enumerate(station_ids)}\n",
    "                \n",
    "                # Rename the station IDs in the selected SNOTEL data\n",
    "                selected_snotel_data['station_id'] = selected_snotel_data['station_id'].map(station_mapping)\n",
    "\n",
    "                # Transpose and set the index correctly\n",
    "                transposed_data[cell_id] = selected_snotel_data.set_index('station_id').T\n",
    "\n",
    "            transposed_df = pd.concat(transposed_data, axis=0)\n",
    "            \n",
    "            # Reset index and rename columns\n",
    "            transposed_df = transposed_df.reset_index()\n",
    "            transposed_df.rename(columns={'level_0': 'cell_id', 'level_1': 'Date'}, inplace = True)\n",
    "            transposed_df['Date'] = pd.to_datetime(transposed_df['Date'])\n",
    "        \n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "            merged_df = pd.merge(aso_swe_data, transposed_df, how='left', on=['cell_id', 'Date'])\n",
    "        \n",
    "            final_df = pd.concat([final_df, merged_df], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "    \n",
    "            # No need to merge in this case, directly concatenate\n",
    "            final_df = pd.concat([final_df, aso_swe_data], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Merge with metadata\n",
    "    req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "    Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "    # Column renaming and ordering\n",
    "    Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "    Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                     'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                     'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                     'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "    # Save the merged data to a new file\n",
    "    output_filename = r\"/home/vgindi/Provided_Data/Merged_aso_snotel_data.csv\"\n",
    "    Result.to_csv(output_filename, index=False)\n",
    "    print(\"Processed and saved data\")\n",
    "    \n",
    "def main():\n",
    "    aso_swe_files_folder_path = r\"/home/vgindi/Processed_SWE\"\n",
    "    metadata_path = r\"/home/vgindi/Provided_Data/grid_cells_meta_idx.csv\"\n",
    "    snotel_data_path = r\"/home/vgindi/Provided_Data/ground_measures_train_featuresALLDATES.parquet\"\n",
    "    fetch_snotel_sites_for_cellids(aso_swe_files_folder_path, metadata_path, snotel_data_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c69770-62f8-4f8f-88ee-ffaac26aaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pd.read_csv(r'/home/vgindi/Provided_Data/Merged_aso_snotel_data.csv')\n",
    "Result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "    #print(signed_asset)\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "    #print(tilearray)\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)\n",
    "\n",
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/Merged_aso_nearest_sites1.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\"\"\"\n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    with rasterio.open(input_file_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.out_meta\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "                         \n",
    "        with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "def download_viirs_sca(input_dir, output_dir, shapefile_path):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(shapefile_path, 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    \n",
    "    # Iterate through each year directory in the input directory\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        if os.path.isdir(year_folder_path):\n",
    "            # Extract year from the folder name (assuming folder names like 'WY2013')\n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):        \n",
    "                if file_name.endswith('.tif'):   \n",
    "                    parts = file_name.split('_')\n",
    "                    output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                    output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                    input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                    crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                    print(f\"Processed and saved {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_directory = r\"/home/vgindi/VIIRS_Data\"\n",
    "    output_directory = r\"/home/vgindi/VIIRS_Sierras\"\n",
    "    shapefile_path = r\"/home/vgindi/Provided_Data/low_sierras_points.shp\"\n",
    "    download_viirs_sca(input_directory, output_directory, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\"\"\"\n",
    "def processing_VIIRS(input_file, output_res):\n",
    "    try:\n",
    "        # Define the output file path for TIFFs using the original file name\n",
    "        output_folder_tiff = os.path.join(\"/home/vgindi/Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(input_dir, output_res):\n",
    "    # Iterate over subdirectories in the input directory\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(\"/home/vgindi/Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/vgindi/VIIRS_Sierras\"\n",
    "    output_res = 100  # Desired resolution in meters\n",
    "    process_and_convert_viirs(input_directory, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(self, row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "    # Import the metadata into a pandas DataFrame\n",
    "    pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    # Drop coordinates columns\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                         'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                         'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                         'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        # Drop rows where 'cell_id' is NaN\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        # Save the processed DataFrame to a CSV file\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"\"\n",
    "    metadata_path = r\"\"\n",
    "    output_folder = r\"\"\n",
    "    process_folder(input_folder, metadata_path, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
