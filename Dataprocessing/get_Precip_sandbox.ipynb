{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f4ef6-25f4-400d-aa3d-f4332925e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dee95a-3ffa-49b7-b36d-bff6110cecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/cg1/Golitzin/SWEMLv2.0/Dataprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104bbd3e-c669-4c0c-8db2-50afc662bedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ee #pip install earthengine-api\n",
    "import geemap\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures as cf\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pickle as pkl\n",
    "# ee.Authenticate()\n",
    "# ee.Initialize()\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pydaymet as daymet\n",
    "import pynldas2 as nldas\n",
    "import pygridmet as gridmet\n",
    "from pygridmet import GridMET\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "import pyproj\n",
    "from rasterio.plot import show\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import contextily as cx\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043a7c8-fe99-4fcd-9e26-a915029bffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95ebf2-ea4e-44c8-812a-67446cebda37",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJ_LIB'] =pyproj.datadir.get_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02d8dd-22bc-4296-a031-7274adf49bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load access key\n",
    "#HOME = os.getcwd()\n",
    "HOME = os.chdir('..')\n",
    "HOME = os.getcwd()\n",
    "#HOME = os.path.expanduser('~')\n",
    "\n",
    "import utils.EE_funcs as EE_funcs\n",
    "\n",
    "KEYPATH = \"utils/AWSaccessKeys.csv\"\n",
    "print(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "if os.path.isfile(f\"{HOME}/{KEYPATH}\") == True:\n",
    "    ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "    #start session\n",
    "    SESSION = boto3.Session(\n",
    "        aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "        aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    "    )\n",
    "    S3 = SESSION.resource('s3')\n",
    "    #AWS BUCKET information\n",
    "    BUCKET_NAME = 'national-snow-model'\n",
    "    #S3 = boto3.resource('S3', config=Config(signature_version=UNSIGNED))\n",
    "    BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "else:\n",
    "    print(\"no AWS credentials present, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288fc43-0960-4706-9782-dd5b5ea8e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASO_Key=pd.read_csv(f\"{HOME}/utils/ASONameKey.csv\",header=3)\n",
    "ASO_Key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f077b2-74dc-4e32-9435-050aa92f2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f'{HOME}/Predictions/NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/1000M_Resolution/10_fSCA_Thresh/2024/HoldWYsout_1000M_Resolution_Taylor_2024-04-04.parquet')\n",
    "pb = np.sum(-test['XGBoost_swe_cm'] + test['ASO_swe_cm'])/np.sum(test['ASO_swe_cm'])*100\n",
    "# (test['XGBoost_swe_cm'] - test['ASO_swe_cm']).describe()\n",
    "pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83db79-4087-4b27-9092-ec10ee147175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filename_parse(filename):\n",
    "    date = next(element for element in os.path.splitext(filename)[0].split(\"_\") if element.startswith('20'))\n",
    "    if date[4].isnumeric() == False:\n",
    "        date_singleday = os.path.splitext(date)[0].split(\"-\")[0]\n",
    "        datetime_object = datetime.strptime(date_singleday, \"%Y%b%d\")\n",
    "        date = datetime_object.strftime('%Y%m%d')\n",
    "    #identify basin from site code if applicable, else identify basin from name\n",
    "    if filename[:12] == \"ASO_50M_SWE_\":\n",
    "        # print(file[12:18])\n",
    "        sitecode = filename[12:18]\n",
    "        index = ASO_Key['SITE CODE']==sitecode\n",
    "        sitename=(ASO_Key.loc[index,'SITE NAME']).item().replace(\" \",\"_\")\n",
    "        # print(sitename)\n",
    "        newfilename = f\"{sitename}_{sitecode}_{date}\"\n",
    "        # print(newfilename)\n",
    "    else:\n",
    "        sitename = os.path.splitext(filename)[0].split(\"_\")[1]\n",
    "        newfilename = f\"{sitename}_{date}\"\n",
    "    return(date, newfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f7468-7248-49d1-bf30-02179dacf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting stuck here b/c file structure is still governed by metadata for entire WY regardless of basin\n",
    "# is there a way to set up the pipeline so the file dependencies are agnostic to basin and date? \n",
    "training_df_path = f\"{HOME}/data/TrainingDFs/{2013}/{1000}M_Resolution/VIIRSGeoObsDFs/{20}_fSCA_Thresh/VIIRS_GeoObsDF_20130403.parquet\"\n",
    "training_df = pd.read_parquet(training_df_path)\n",
    "meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63267da-2a47-4ddd-9a4c-3da2f8280ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daymet_precip(WY,output_res,thresh):\n",
    "    \n",
    "    # set start date for precip obs to 10-1 of previous year\n",
    "    WY_start = datetime(WY-1, 10, 1)\n",
    "    obs_start = WY_start.strftime('%Y-%m-%d')\n",
    "    print(\"Water Year start date:\",obs_start)\n",
    "    \n",
    "    # select basins, dates by ASO observation\n",
    "    ASO_dir = f\"{HOME}/data/ASO/{WY}/Raw_ASO_Data\"\n",
    "    files = [filename for filename in os.listdir(ASO_dir)\n",
    "             if filename.endswith(\".tif\")\n",
    "            ]\n",
    "    print(files)\n",
    "    for file in files:\n",
    "        filepath = f'{ASO_dir}/{file}'\n",
    "        date, newfilename = filename_parse(file)\n",
    "        obs_end = f'{date[:4]}-{date[4:6]}-{date[6:]}'\n",
    "        print(\"Getting precipitation data for\",obs_end)\n",
    "        with rxr.open_rasterio(filepath) as src:\n",
    "            # reproject to WGS84\n",
    "            transformed = src.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))\n",
    "            left, bottom, right, top = transformed.rio.bounds()\n",
    "            # add some padding to bbox\n",
    "            left -= 0.1\n",
    "            bottom -= 0.1\n",
    "            right += 0.1\n",
    "            top += 0.1\n",
    "            bbox = rasterio.coords.BoundingBox(left, bottom, right, top)\n",
    "            print(bbox)  \n",
    "        obs_precip = daymet.get_bygeom(bbox,dates=(obs_start,obs_end),variables=\"prcp\",crs=4326)\n",
    "        obs_precip_transformed = obs_precip.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))\n",
    "          \n",
    "        # load previous training DF to extract metadata for specific observation\n",
    "        training_df_path = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh/VIIRS_GeoObsDF_{date}.parquet\"\n",
    "        training_df = pd.read_parquet(training_df_path)\n",
    "        meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "        # coordinates get rounded in get_VIIRS script, reassess later if need more precision\n",
    "        print(meta['cen_lon'].min(),meta['cen_lon'].max(),meta['cen_lat'].min(),meta['cen_lat'].max())\n",
    "        precip_arr = []\n",
    "        season_precip_cm = []\n",
    "        nsites = len(meta)\n",
    "        for i in range(nsites):\n",
    "            lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "            cellid = meta.iloc[i]['cell_id']\n",
    "            if ((lon>bbox[0] and lon<bbox[2]) and (lat>bbox[1] and lat<bbox[3])):\n",
    "                # print('got here')\n",
    "                prcp = obs_precip_transformed.sel(x=lon,y=lat,method='nearest')['prcp']\n",
    "                season_precip = np.round(np.array(prcp.values).sum()/10,2)\n",
    "            # if season_precip >= 0:\n",
    "                precip_arr.append([cellid,lat,lon,np.array(prcp.values)])\n",
    "                season_precip_cm.append(season_precip)\n",
    "        precip_df = pd.DataFrame(precip_arr,columns = ['cell_id','cen_lat','cen_lon','precip'])\n",
    "        precip_df['season_precip_cm'] = season_precip_cm    \n",
    "        \n",
    "        # print(precip_df.head())\n",
    "        \n",
    "        # save raw data for each basin and date\n",
    "        precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_Daymet_Precip\"\n",
    "        if not os.path.exists(precip_data_path):\n",
    "            os.makedirs(precip_data_path, exist_ok=True)\n",
    "            \n",
    "        table = pa.Table.from_pandas(precip_df)\n",
    "        pq.write_table(table, f\"{precip_data_path}/Daymet_{newfilename}.parquet\", compression='BROTLI')\n",
    "        \n",
    "    # return season_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff72934-ea4e-4080-a880-b8f72ed585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daymet_precip_multithreading(WY,output_res,thresh):\n",
    "    # set start date for precip obs to Oct 1 of previous year\n",
    "    WY_start = datetime(WY-1, 10, 1)\n",
    "    obs_start = WY_start.strftime('%Y-%m-%d')\n",
    "    print(\"Water Year start date:\",obs_start)\n",
    "    \n",
    "    # select basins, dates by training DF\n",
    "    training_df_dir = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "    files = [filename for filename in os.listdir(training_df_dir)\n",
    "             if filename.endswith(\".parquet\")\n",
    "            ]\n",
    "    \n",
    "    with cf.ThreadPoolExecutor(max_workers=CPUS/2) as executor: \n",
    "        # Start the load operations and mark each future with its process function\n",
    "        {executor.submit(get_daymet_precip_single_date, (file,training_df_dir,obs_start,WY,output_res,thresh)): \\\n",
    "            file for file in tqdm(files)}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d37909-4205-403b-b06e-b33f397b1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daymet_precip_single_date(file,training_df_dir,obs_start,WY,output_res,thresh):\n",
    "    # get daymet precip by grabbing bounding box from previous training DF for basin + date\n",
    "    print(file)\n",
    "    filepath = f'{training_df_dir}/{file}'\n",
    "    #Get timestamp\n",
    "    timestamp = file.split('_')[-1].split('.')[0]\n",
    "    #Get region\n",
    "    region = file.split('_')[-2]\n",
    "    # print(timestamp,region)\n",
    "    obs_end = f'{timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:]}'\n",
    "        \n",
    "    print(f\"Getting precipitation data for {obs_end} at {region}\")\n",
    "        \n",
    "    training_df = pd.read_parquet(filepath)\n",
    "    # get bounding box by min/max coordinates\n",
    "    left, right = training_df['cen_lon'].min(), training_df['cen_lon'].max()\n",
    "    bottom, top = training_df['cen_lat'].min(), training_df['cen_lat'].max()\n",
    "    # add some padding to bbox\n",
    "    left -= 0.1\n",
    "    bottom -= 0.1\n",
    "    right += 0.1\n",
    "    top += 0.1\n",
    "    bbox = rasterio.coords.BoundingBox(left, bottom, right, top)\n",
    "    print(bbox)\n",
    "       \n",
    "    # get precip from Daymet server from beginning of WY through observation date and reproject\n",
    "    obs_precip = daymet.get_bygeom(bbox,dates=(obs_start,obs_end),variables=\"prcp\",crs=4326)\n",
    "    obs_precip_transformed = obs_precip.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))  \n",
    "    # print(bbox)    \n",
    "     \n",
    "    # extract metadata \n",
    "    meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "    # coordinates get rounded in get_VIIRS script, reassess later if need more precision\n",
    "    # print(meta['cen_lon'].min(),meta['cen_lon'].max(),meta['cen_lat'].min(),meta['cen_lat'].max())\n",
    "    precip_arr = []\n",
    "    season_precip_cm = []\n",
    "    nsites = len(meta)\n",
    "    for i in range(nsites):\n",
    "        lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "        cellid = meta.iloc[i]['cell_id']\n",
    "        prcp = obs_precip_transformed.sel(x=lon,y=lat,method='nearest')['prcp']\n",
    "        season_precip = np.round(np.array(prcp.values).sum()/10,2)\n",
    "        # if season_precip >= 0:\n",
    "        precip_arr.append([cellid,lat,lon,np.array(prcp.values)])\n",
    "        season_precip_cm.append(season_precip)\n",
    "    precip_df = pd.DataFrame(precip_arr,columns = ['cell_id','cen_lat','cen_lon','precip'])\n",
    "    precip_df['season_precip_cm'] = season_precip_cm    \n",
    "    # print(season_precip_cm)\n",
    "    # print(precip_df.head())\n",
    "        \n",
    "        # save raw data for each basin and date\n",
    "    precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_Daymet_Precip\"\n",
    "    if not os.path.exists(precip_data_path):\n",
    "        os.makedirs(precip_data_path, exist_ok=True)\n",
    "            \n",
    "    table = pa.Table.from_pandas(precip_df)\n",
    "    pq.write_table(table, f\"{precip_data_path}/Daymet_{region}_{timestamp}.parquet\", compression='BROTLI')\n",
    "        \n",
    "    # return season_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96717e-0bcd-4c58-943e-7567eef6ebb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81269b6-407c-4cda-95a0-4502629f7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_daymet_precip(2013,750,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efab7f2-0adb-49f1-9ec1-b28eb1177731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_daymet_precip_df(WY,output_res,thresh):\n",
    "#     # get daymet precip by grabbing bounding box from previous training DF for basin + date\n",
    "#     # set start date for precip obs to Oct 1 of previous year\n",
    "#     WY_start = datetime(WY-1, 10, 1)\n",
    "#     obs_start = WY_start.strftime('%Y-%m-%d')\n",
    "#     print(\"Water Year start date:\",obs_start)\n",
    "    \n",
    "#     # select basins, dates by training DF\n",
    "#     training_df_dir = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "#     files = [filename for filename in os.listdir(training_df_dir)\n",
    "#              if filename.endswith(\".parquet\")\n",
    "#             ]\n",
    "#     # print(files)\n",
    "#     for file in files:\n",
    "#         filepath = f'{training_df_dir}/{file}'\n",
    "#         #Get timestamp\n",
    "#         timestamp = file.split('_')[-1].split('.')[0]\n",
    "#         #Get region\n",
    "#         region = file.split('_')[-2]\n",
    "#         # print(timestamp,region)\n",
    "#         obs_end = f'{timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:]}'\n",
    "        \n",
    "#         print(f\"Getting precipitation data for {obs_end} at {region}\")\n",
    "        \n",
    "#         training_df = pd.read_parquet(filepath)\n",
    "#         # get bounding box by min/max coordinates\n",
    "#         left, right = training_df['cen_lon'].min(), training_df['cen_lon'].max()\n",
    "#         bottom, top = training_df['cen_lat'].min(), training_df['cen_lat'].max()\n",
    "#         # add some padding to bbox\n",
    "#         left -= 0.1\n",
    "#         bottom -= 0.1\n",
    "#         right += 0.1\n",
    "#         top += 0.1\n",
    "#         bbox = rasterio.coords.BoundingBox(left, bottom, right, top)\n",
    "#         print(bbox)    \n",
    "\n",
    "#         # get precip from Daymet server from beginning of WY through observation date and reproject\n",
    "#         obs_precip = daymet.get_bygeom(bbox,dates=(obs_start,obs_end),variables=\"prcp\",crs=\"epsg:4326\")\n",
    "#         obs_precip_transformed = obs_precip.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))  \n",
    "#         print(bbox)    \n",
    "        \n",
    "#         # extract metadata \n",
    "#         meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "#         # coordinates get rounded in get_VIIRS script, reassess later if need more precision\n",
    "#         # print(meta['cen_lon'].min(),meta['cen_lon'].max(),meta['cen_lat'].min(),meta['cen_lat'].max())\n",
    "#         precip_arr = []\n",
    "#         season_precip_cm = []\n",
    "#         nsites = len(meta)\n",
    "#         for i in range(nsites):\n",
    "#             lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "#             cellid = meta.iloc[i]['cell_id']\n",
    "#             # if ((lon>bbox[0] and lon<bbox[2]) and (lat>bbox[1] and lat<bbox[3])):\n",
    "#                 # print('got here')\n",
    "#             prcp = obs_precip_transformed.sel(x=lon,y=lat,method='nearest')['prcp']\n",
    "#             season_precip = np.round(np.array(prcp.values).sum()/10,2)\n",
    "#             # if season_precip >= 0:\n",
    "#             precip_arr.append([cellid,lat,lon,np.array(prcp.values)])\n",
    "#             season_precip_cm.append(season_precip)\n",
    "#         precip_df = pd.DataFrame(precip_arr,columns = ['cell_id','cen_lat','cen_lon','precip'])\n",
    "#         precip_df['season_precip_cm'] = season_precip_cm    \n",
    "#         # print(season_precip_cm)\n",
    "#         # print(precip_df.head())\n",
    "        \n",
    "#         # save raw data for each basin and date\n",
    "#         precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_Daymet_Precip\"\n",
    "#         if not os.path.exists(precip_data_path):\n",
    "#             os.makedirs(precip_data_path, exist_ok=True)\n",
    "            \n",
    "#         table = pa.Table.from_pandas(precip_df)\n",
    "#         pq.write_table(table, f\"{precip_data_path}/Daymet_{region}_{timestamp}.parquet\", compression='BROTLI')\n",
    "        \n",
    "#     # return season_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe6b1d-c8a2-48fc-a4d7-f5433496b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyriver_precip_df(WY,output_res,thresh,dataset):\n",
    "    # get precip by grabbing bounding box from previous training DF for basin + date\n",
    "    # set start date for precip obs to Oct 1 of previous year\n",
    "    valid = ['daymet','gridmet','nldas']\n",
    "    if dataset.lower() not in valid:\n",
    "        raise ValueError(\"dataset must be one of %r.\" % valid)\n",
    "    if dataset.lower() == 'daymet':\n",
    "        dataset = 'Daymet'\n",
    "    elif dataset.lower() == 'nldas':\n",
    "        dataset = 'NLDAS'\n",
    "    elif dataset.lower() == 'gridmet':\n",
    "        dataset = 'gridMET'\n",
    "        \n",
    "    WY_start = datetime(WY-1, 10, 1)\n",
    "    obs_start = WY_start.strftime('%Y-%m-%d')\n",
    "    print(\"Water Year start date:\",obs_start)\n",
    "    \n",
    "    # select basins, dates by training DF\n",
    "    training_df_dir = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "    files = [filename for filename in os.listdir(training_df_dir)\n",
    "             if filename.endswith(\".parquet\")\n",
    "            ]\n",
    "    # print(files)\n",
    "    for file in files:\n",
    "        filepath = f'{training_df_dir}/{file}'\n",
    "        #Get timestamp\n",
    "        timestamp = file.split('_')[-1].split('.')[0]\n",
    "        #Get region\n",
    "        region = file.split('_')[-2]\n",
    "        # print(timestamp,region)\n",
    "        obs_end = f'{timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:]}'\n",
    "        \n",
    "        print(f\"Getting precipitation data for {obs_end} at {region}\")\n",
    "        \n",
    "        training_df = pd.read_parquet(filepath)\n",
    "        # get bounding box by min/max coordinates\n",
    "        left, right = training_df['cen_lon'].min(), training_df['cen_lon'].max()\n",
    "        bottom, top = training_df['cen_lat'].min(), training_df['cen_lat'].max()\n",
    "        # add some padding to bbox\n",
    "        left -= 0.1\n",
    "        bottom -= 0.1\n",
    "        right += 0.1\n",
    "        top += 0.1\n",
    "        bbox = rasterio.coords.BoundingBox(left, bottom, right, top)\n",
    "        # print(bbox)    \n",
    "\n",
    "        # get precip from appropriate server from beginning of WY through observation date and reproject\n",
    "        if dataset == 'Daymet':\n",
    "            var = \"prcp\"\n",
    "            obs_precip = daymet.get_bygeom(bbox,dates=(obs_start,obs_end),variables=var,crs=\"epsg:4326\")\n",
    "        elif dataset == 'gridMET':\n",
    "            var = 'pr'\n",
    "            obs_precip = gridmet.get_bygeom(bbox,dates=(obs_start,obs_end),variables=var,crs=\"epsg:4326\")\n",
    "        elif dataset == 'NLDAS':\n",
    "            var = \"prcp\"\n",
    "            obs_precip = nldas.get_bygeom(bbox,obs_start,obs_end,variables=var,geo_crs=4326,source='netcdf')\n",
    "        obs_precip_transformed = obs_precip.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))   \n",
    "        \n",
    "        # extract metadata \n",
    "        meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "        precip_arr = []\n",
    "        season_precip_cm = []\n",
    "        nsites = len(meta)\n",
    "        for i in range(nsites):\n",
    "            lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "            cellid = meta.iloc[i]['cell_id']\n",
    "            prcp = obs_precip_transformed.sel(x=lon,y=lat,method='nearest')[var]\n",
    "            season_precip = np.round(np.array(prcp.values).sum()/10,2) # precip given in mm, convert to cm \n",
    "            # if season_precip >= 0:\n",
    "            precip_arr.append([cellid,lat,lon,np.array(prcp.values)])\n",
    "            season_precip_cm.append(season_precip)\n",
    "        precip_df = pd.DataFrame(precip_arr,columns = ['cell_id','cen_lat','cen_lon','precip'])\n",
    "        precip_df['season_precip_cm'] = season_precip_cm    \n",
    "        \n",
    "        # save raw data for each basin and date\n",
    "        precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_{dataset}_Precip\"\n",
    "        if not os.path.exists(precip_data_path):\n",
    "            os.makedirs(precip_data_path, exist_ok=True)\n",
    "            \n",
    "        table = pa.Table.from_pandas(precip_df)\n",
    "        pq.write_table(table, f\"{precip_data_path}/{dataset}_{region}_{timestamp}.parquet\", compression='BROTLI')\n",
    "        \n",
    "    # return season_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07085026-b3c7-4672-8bfb-6dda1f83bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hyriver_precip_df(2019,1000,10,'nldas')\n",
    "# get_hyriver_precip_df(2017,1000,10,'gridmet')\n",
    "# nldas.get_bygeom("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a15b44-0e41-4efc-a416-1cc064feab83",
   "metadata": {},
   "source": [
    "#### Use HydroShare module to import AORC precip from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54178c4e-30e1-4861-9d57-8427f71cc908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable name to retrieve data (look at the following table for valid variable names)\n",
    "variable_name = 'APCP_surface'\n",
    "# User-defined aggregation interval - valid values are 'hour','day','month','year'\n",
    "agg_interval = 'day'\n",
    "# Start date - In Year-Month-Day format the earliest start date can be '1979-02-01'\n",
    "start_datetime = '2016-10-01'\n",
    "# End date - In Year-Month-Day format the latest end date can be '2023-01-31'\n",
    "end_datetime = '2017-08-31'\n",
    "\n",
    "## Create a list of years to retrieve data \n",
    "WY = 2017\n",
    "WY_start = datetime(WY-1, 10, 1)\n",
    "# obs_start = WY_start.strftime('%Y-%m-%d')\n",
    "start_yr = WY_start.year # datetime.strptime(start_datetime, '%Y-%m-%d').year\n",
    "end_yr = datetime.strptime(end_datetime, '%Y-%m-%d').year\n",
    "yrs = list(range(start_yr, end_yr+1))\n",
    "\n",
    "## Loading data (AORC data are organized by years, look at https://noaa-nws-aorc-v1-1-1km.s3.amazonaws.com/index.html)\n",
    "# Base URL\n",
    "base_url = f's3://noaa-nws-aorc-v1-1-1km'\n",
    "# Creating a connection to Amazon S3 bucket using the s3fs library (https://s3fs.readthedocs.io/en/latest/api.html).\n",
    "s3_out = s3fs.S3FileSystem(anon=True)              # access S3 as if it were a file system. \n",
    "fileset = [s3fs.S3Map(                             # maps each year's Zarr dataset from S3 to a local-like object.\n",
    "            root=f\"s3://{base_url}/{yr}.zarr\",     # Zarr dataset for each year\n",
    "            s3=s3_out,                             # connection\n",
    "            check=False                            # checking if the dataset exists before trying to load it\n",
    "        ) for yr in yrs]                           # loops through each year\n",
    "\n",
    "## Load data for specified years and veriable of interest using the xarray library\n",
    "ds_yrs = xr.open_mfdataset(fileset, engine='zarr')\n",
    "da_yrs_var = ds_yrs[variable_name]\n",
    "variable_long_name = da_yrs_var.attrs.get('long_name')\n",
    "da_yrs_var\n",
    "left,bottom,right,top = -119.892, 37.64, -119.107, 38.281\n",
    "da_bbox = da_yrs_var.sel(latitude=slice(bottom, top), longitude=slice(left, right))\n",
    "\n",
    "if variable_name == 'APCP_surface':\n",
    "    units = f\"mm/{agg_interval}\"\n",
    "    # Temporal aggregation\n",
    "    da_bbox_TimeAgg = da_bbox.loc[dict(time=slice(start_datetime, end_datetime))].resample(time='d').sum()\n",
    "\n",
    "obs_precip_transformed = da_bbox_TimeAgg.rio.reproject(rasterio.crs.CRS.from_epsg('4326')) #.load()  \n",
    "obs_precip_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9cd17-7e1e-4886-8a20-5641e10daae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_precip_transformed.sel(time='2017-01-05').plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97839ba-a32b-4050-a76d-2b3c8c5d1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_transformed.sel(x=-119.555,y=37.95,time='2017-01-05',method='nearest').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ab6c7-d052-406d-ad2e-b387368b7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aorc_precip(WY,output_res,thresh):\n",
    "    # define start of water year\n",
    "    WY_start = datetime(WY-1, 10, 1)\n",
    "    obs_start = WY_start.strftime('%Y-%m-%d')\n",
    "    print(\"Water Year start date:\",obs_start)\n",
    "\n",
    "    ## Create a list of years to retrieve data \n",
    "    yrs = [WY-1, WY]\n",
    "    \n",
    "    ## Loading data (AORC data are organized by years, look at https://noaa-nws-aorc-v1-1-1km.s3.amazonaws.com/index.html)\n",
    "    # Grab data for entire WY before clipping to each basin per training DFs \n",
    "    # Base URL\n",
    "    base_url = f's3://noaa-nws-aorc-v1-1-1km'\n",
    "    # Creating a connection to Amazon S3 bucket using the s3fs library (https://s3fs.readthedocs.io/en/latest/api.html).\n",
    "    s3_out = s3fs.S3FileSystem(anon=True)              # access S3 as if it were a file system. \n",
    "    fileset = [s3fs.S3Map(                             # maps each year's Zarr dataset from S3 to a local-like object.\n",
    "                root=f\"s3://{base_url}/{yr}.zarr\",     # Zarr dataset for each year\n",
    "                s3=s3_out,                             # connection\n",
    "                check=False                            # checking if the dataset exists before trying to load it\n",
    "            ) for yr in yrs]                           # loops through each year\n",
    "    \n",
    "    ## Load data for specified years and variable of interest using the xarray library\n",
    "    var = 'APCP_surface'\n",
    "    ds_yrs = xr.open_mfdataset(fileset, engine='zarr')\n",
    "    da_yrs_var = ds_yrs[var].rio.write_crs(4326,inplace=True).fillna(0)\n",
    "    variable_long_name = da_yrs_var.attrs.get('long_name')\n",
    "    # Temporal aggregation\n",
    "    da_TimeAgg = da_yrs_var.resample(time='d').sum()\n",
    "    \n",
    "    # select basins, dates by training DF\n",
    "    training_df_dir = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "    files = [filename for filename in os.listdir(training_df_dir)\n",
    "             if filename.endswith(\".parquet\")\n",
    "            ]\n",
    "    # print(files)\n",
    "    for file in files:\n",
    "        filepath = f'{training_df_dir}/{file}'\n",
    "        #Get timestamp\n",
    "        timestamp = file.split('_')[-1].split('.')[0]\n",
    "        #Get region\n",
    "        region = file.split('_')[-2]\n",
    "        obs_end = f'{timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:]}'\n",
    "        \n",
    "        print(f\"Getting precipitation data for {obs_end} at {region}\")\n",
    "        \n",
    "        training_df = pd.read_parquet(filepath)\n",
    "        # get bounding box by min/max coordinates\n",
    "        left, right = training_df['cen_lon'].min(), training_df['cen_lon'].max()\n",
    "        bottom, top = training_df['cen_lat'].min(), training_df['cen_lat'].max()\n",
    "        # add some padding to bbox\n",
    "        left -= 0.1\n",
    "        bottom -= 0.1\n",
    "        right += 0.1\n",
    "        top += 0.1  \n",
    "        da_WYagg = da_TimeAgg.loc[dict(time=slice(obs_start, obs_end))]\n",
    "        da_bbox = da_WYagg.sel(latitude=slice(bottom, top), longitude=slice(left, right))\n",
    "        \n",
    "        obs_precip_transformed = da_bbox.rio.reproject(rasterio.crs.CRS.from_epsg('4326')).load()  \n",
    "        \n",
    "        # extract metadata \n",
    "        meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "        precip_arr = []\n",
    "        season_precip_cm = []\n",
    "        nsites = len(meta)\n",
    "        for i in range(nsites):\n",
    "            lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "            cellid = meta.iloc[i]['cell_id']\n",
    "            prcp = obs_precip_transformed.sel(x=lon,y=lat,method='nearest') \n",
    "            season_precip = np.round(np.array(prcp.values).sum()/10,2) # precip given in mm, convert to cm \n",
    "            # if season_precip >= 0:\n",
    "            precip_arr.append([cellid,lat,lon,np.array(prcp.values)])\n",
    "            season_precip_cm.append(season_precip)\n",
    "        precip_df = pd.DataFrame(precip_arr,columns = ['cell_id','cen_lat','cen_lon','precip'])\n",
    "        precip_df['season_precip_cm'] = season_precip_cm  \n",
    "        # print(season_precip_cm)\n",
    "        \n",
    "        # save raw data for each basin and date\n",
    "        precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_AORC_Precip\"\n",
    "        if not os.path.exists(precip_data_path):\n",
    "            os.makedirs(precip_data_path, exist_ok=True)\n",
    "            \n",
    "        table = pa.Table.from_pandas(precip_df)\n",
    "        pq.write_table(table, f\"{precip_data_path}/AORC_{region}_{timestamp}.parquet\", compression='BROTLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5807ac4-e316-4a49-af8f-1d74256e3abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "years = np.arange(2013,2025)\n",
    "for WY in years:\n",
    "    get_aorc_precip(WY,1000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec51ae",
   "metadata": {},
   "source": [
    "#### Try PRISM packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393950ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _progress_hook(block_num, block_size, total_size, t):\n",
    "    \"\"\"\n",
    "    Callback function to update tqdm progress bar during file download.\n",
    "    \"\"\"\n",
    "    downloaded = block_num * block_size\n",
    "    if total_size > 0:\n",
    "        t.update(min(block_size, total_size - t.n))\n",
    "    else:\n",
    "        t.update(downloaded - t.n)\n",
    "\n",
    "def prism_download(start,stop,path,var):\n",
    "    while start <= stop:\n",
    "        day = start.strftime(\"%Y%m%d\")\n",
    "        url = f\"{base_url}/{var}/{day}?format=nc\"\n",
    "        output_file = os.path.join(path, day)\n",
    "\n",
    "        with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=f'Downloading {day}') as t:\n",
    "            urllib.request.urlretrieve(url, output_file, reporthook=lambda block_num, block_size, total_size: _progress_hook(block_num, block_size, total_size, t))\n",
    "\n",
    "        start += timedelta(days=1)\n",
    "\n",
    "def unzip_prism(start,stop,zipped_path,unzipped_path):\n",
    "    while start <= stop:\n",
    "        day = start.strftime(\"%Y%m%d\")\n",
    "        zip_file_path = os.path.join(zipped_path, day)\n",
    "\n",
    "        # Check if the ZIP file exists\n",
    "        if os.path.exists(zip_file_path):\n",
    "            # Unzip the file\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzipped_path)\n",
    "                print(f\"UNZIP file for {day} is completed.\")\n",
    "            os.remove(zip_file_path)\n",
    "        else:\n",
    "            print(f\"ZIP file for {day} not found.\")\n",
    "\n",
    "        start += timedelta(days=1)\n",
    "\n",
    "year = 2013\n",
    "def get_prism(WY):\n",
    "    prism_path = f'{HOME}/data/Precipitation/{WY}/prism_data/'\n",
    "    zipped_path = f'{prism_path}/zipped'\n",
    "    unzipped_path = f'{prism_path}/unzipped'\n",
    "    os.makedirs(zipped_path,exist_ok=True)\n",
    "    os.makedirs(unzipped_path,exist_ok=True)\n",
    "\n",
    "    base_url = \"https://services.nacse.org/prism/data/get/us/4km/\"\n",
    "    var = \"ppt\"\n",
    "    start = datetime.strptime(f\"{WY-1}-10-01\", \"%Y-%m-%d\")\n",
    "    stop = datetime.strptime(f\"{WY}-09-30\", \"%Y-%m-%d\")\n",
    "    \n",
    "    prism_download(start,stop,zipped_path,var)\n",
    "    unzip_prism(start,stop,zipped_path,unzipped_path)\n",
    "\n",
    "    date_idx = pd.Index(pd.date_range(start=start,end=stop),name='time')\n",
    "    timeser = xr.open_mfdataset(f'{unzipped_path}/*.nc', \n",
    "                                    combine='nested',\n",
    "                                    concat_dim=[date_idx,]\n",
    "                                    )\n",
    "    timeser.to_netcdf(path=f'{prism_path}/{WY}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef46e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prism(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [filename for filename in os.listdir(prism_path) if filename.endswith('.nc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab40445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = xr.open_dataset(f'{prism_path}/prism_ppt_us_25m_20121215.nc')\n",
    "# # test.crs\n",
    "# test = test.rio.write_crs('EPSG:4269')\n",
    "# test_reproj = test.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))\n",
    "# # test['Band1'].plot.pcolormesh()\n",
    "# test['Band1'].sel(lat=40.723857, lon=-111.884616,method='nearest')\n",
    "\n",
    "date_idx = pd.Index(pd.date_range(start=\"2014-10-01\",end=\"2015-09-30\"),name='time')\n",
    "path = f'{HOME}/data/Precipitation/2015/prism_data/unzipped'\n",
    "# date_idx\n",
    "test_timeser = xr.open_mfdataset(f'{path}/*.nc', \n",
    "                                 combine='nested',\n",
    "                                 concat_dim=[date_idx,]\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timeser['Band1'].sel(lat=40.5,lon=-118.5,method='nearest').plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e077783",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=f'{HOME}/data/Precipitation/2015/prism_data/'\n",
    "test_timeser.to_netcdf(path=f'{path}/2015.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2a8aa",
   "metadata": {},
   "source": [
    "#### Plot results for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054027e-da98-4502-a605-db7d6b53ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f'{HOME}/data/Precipitation/2013/500M_gridMET_Precip/gridMET_USCATB_20130403.parquet')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667d262-94be-42d6-b28d-f4ce19dfe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = pd.read_parquet(f'{HOME}/data/Precipitation/2019/1000M_gridMET_Precip/gridMET_USCOGT_20190408.parquet')\n",
    "test2 = pd.read_parquet(f'{HOME}/data/Precipitation/2019/1000M_Daymet_Precip/Daymet_USCOGT_20190408.parquet')\n",
    "test3 = pd.read_parquet(f'{HOME}/data/Precipitation/2019/1000M_NLDAS_Precip/NLDAS_USCOGT_20190408.parquet')\n",
    "test4 = pd.read_parquet(f'{HOME}/data/Precipitation/2019/1000M_AORC_Precip/AORC_USCOGT_20190408.parquet')\n",
    "gdf  = gpd.GeoDataFrame(test['season_precip_cm'],\n",
    "                       geometry=gpd.points_from_xy(test['cen_lon'], test['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "gdf2 = gpd.GeoDataFrame(test2['season_precip_cm'],\n",
    "                       geometry=gpd.points_from_xy(test2['cen_lon'], test2['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "gdf3 = gpd.GeoDataFrame(test3['season_precip_cm'],\n",
    "                       geometry=gpd.points_from_xy(test2['cen_lon'], test2['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "gdf4 = gpd.GeoDataFrame(test4['season_precip_cm'],\n",
    "                       geometry=gpd.points_from_xy(test2['cen_lon'], test2['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "# print(test.head)\n",
    "# print(test2.head)\n",
    "# gdf3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd374802-42cf-42ad-b98f-bb39c68dec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(2,2,figsize=(8,8),layout='tight',dpi=150)\n",
    "gdf.plot(ax=axs[0][0],column='season_precip_cm',cmap='viridis',legend=True)\n",
    "gdf2.plot(ax=axs[0][1],column='season_precip_cm',cmap='viridis',legend=True)\n",
    "gdf3.plot(ax=axs[1][0],column='season_precip_cm',cmap='viridis',legend=True)\n",
    "gdf4.plot(ax=axs[1][1],column='season_precip_cm',cmap='viridis',legend=True)\n",
    "axs[0][0].set_title('GridMET')\n",
    "axs[0][1].set_title('Daymet')\n",
    "axs[1][0].set_title('NLDAS')\n",
    "axs[1][1].set_title('AORC')\n",
    "plt.suptitle('USCOGT 2019-04-08')\n",
    "plt.savefig(f'{HOME}/Images/Precip_Comparison_20190408_Taylor.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cafe0e-bfb1-448d-817f-e8e4420f9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,2,figsize=(12,5),layout='tight')\n",
    "gdf3.plot(ax=axs[0],column='season_precip_cm',cmap='viridis',legend=True)\n",
    "axs[0].set_title('NLDAS - New Method')\n",
    "gdf4.plot(ax=axs[1],column='NLDAS',cmap='viridis',legend=True)\n",
    "axs[1].set_title('NLDAS - Prev Method')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86b166-f86d-4e95-87a7-1b89f2e60f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = rasterio.coords.BoundingBox(left=-119.896, bottom=37.644999, right=-119.102, top=38.284)\n",
    "obs_start = '2016-10-01'\n",
    "obs_end = '2017-07-18'\n",
    "obs_precip_nl = nldas.get_bygeom(geom,obs_start,obs_end,variables=\"prcp\",geo_crs=4326,source='netcdf')\n",
    "obs_precip_gm = gridmet.get_bygeom(geom,(obs_start,obs_end),variables=\"pr\",crs=4326,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a055f8-265d-4031-bff9-2a2e8a7e0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_nl = obs_precip_nl.rename({'y':'lat',\n",
    "                                      'x':'lon'}) #.prcp.sel(time='2024-01-11').plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ce1ac-4212-4c8a-9df4-23494de6bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_nl.sum(dim=\"time\")['prcp'].plot.pcolormesh() #.sel(time='2024-01-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335c18a7-7633-4ff7-84d5-67c019d5d98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_gm.sum(dim=\"time\")['pr'].plot.pcolormesh() #sel(time='2024-01-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef83e9-d642-4de9-981d-6a461e22154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = rasterio.coords.BoundingBox(left=-119.896, bottom=37.644999999999996, right=-119.102, top=38.284)\n",
    "obs_start = '2023-10-01'\n",
    "obs_end = '2024-05-31'\n",
    "obs_precip = gridmet.get_bygeom(geom,dates=(obs_start,obs_end),variables=\"pr\",crs=\"epsg:4326\")\n",
    "obs_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdf122-e18a-4145-9b7f-660bfb71b4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7fc95-eed8-4f6a-b07b-e0d176155ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WYs = [2013]\n",
    "res = [750]\n",
    "for year in WYs:\n",
    "    for output_res in res:\n",
    "        get_daymet_precip_df(year,output_res,thresh=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c7d2c-c6c6-4749-a74b-5515ca1662c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set multiprocessing limits\n",
    "CPUS = len(os.sched_getaffinity(0))\n",
    "CPUS = int((CPUS/2)-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e98c26-c3d5-45d0-9c8b-ae744c40d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Precip_DF(WY,output_res,thresh,dataset):\n",
    "    print(f\"Adding precipitation features to ML dataframe for WY {WY}\")\n",
    "    precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_{dataset}_Precip\"\n",
    "    training_df_path = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "\n",
    "    #make precip df path\n",
    "    precip_df_path = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/PrecipVIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "    if not os.path.exists(precip_df_path):\n",
    "        os.makedirs(precip_df_path, exist_ok=True)\n",
    "\n",
    "    #Get list of dataframes\n",
    "    GeoObsDF_files = [filename for filename in os.listdir(training_df_path)] \n",
    "    # print(GeoObsDF_files)\n",
    "    \n",
    "    # Multiprocessing \n",
    "    with cf.ProcessPoolExecutor(max_workers=CPUS) as executor: \n",
    "        # Start the load operations and mark each future with its process function\n",
    "        [executor.submit(single_date_add_daymet_precip, (training_df_path, precip_data_path, geofile, precip_df_path, WY, dataset)) for geofile in GeoObsDF_files]\n",
    "        \n",
    "    # for geofile in GeoObsDF_files:\n",
    "        # single_date_add_daymet_precip((training_df_path, precip_data_path, geofile, precip_df_path, WY, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0472f9-e16d-4978-9e27-a9c4031b8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "WY=2015\n",
    "output_res=1000\n",
    "thresh=20\n",
    "dataset = 'Daymet'\n",
    "precip_data_path = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_{dataset}_Precip\"\n",
    "training_df_path = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{thresh}_fSCA_Thresh\"\n",
    "pptfiles = [filename for filename in os.listdir(precip_data_path) if filename.endswith('.parquet')]\n",
    "pptfiles_dates = []\n",
    "for i in range(len(pptfiles)):\n",
    "    pptfiles_dates.append(pptfiles[i].split('_')[-1].split('.parquet')[0])\n",
    "\n",
    "pptfiles_dates = np.array(pptfiles_dates)\n",
    "unique_dates = np.unique(pptfiles_dates)\n",
    "print(f'there are {len(unique_dates)} unique dates')\n",
    "\n",
    "for i,date in enumerate(unique_dates):\n",
    "    # print(date)\n",
    "    idxarr = np.where(pptfiles_dates == date)\n",
    "    # print(idxarr)\n",
    "    # print(idxarr[0])\n",
    "    date_obs = []\n",
    "    for idx in idxarr[0]:\n",
    "        # print(idx)\n",
    "        # print(pptfiles[idx])\n",
    "        date_obs.append(pptfiles[idx])\n",
    "    print(i, date, date_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70df21-7aab-4dab-90f3-e427cc79ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_date_add_daymet_precip(args):\n",
    "    training_df_path, precip_data_path, geofile, precip_df_path, WY, dataset = args\n",
    "    #get date information\n",
    "    # print(geofile)\n",
    "    date = geofile.split('_')[-1].split('.parquet')[0]\n",
    "    region = geofile.split('_')[-2]\n",
    "    # print(region,date)\n",
    "    region_date = f\"{region}_{date}\"\n",
    "    # print(region_date)\n",
    "    year = date[:4]\n",
    "    mon = date[4:6]\n",
    "    day = date[6:]\n",
    "    strdate = f\"{year}-{mon}-{day}\"\n",
    "    print(f\"Connecting precipitation to ASO observations for {WY} on {strdate} at {region}\")\n",
    "    \n",
    "    GDF = pd.read_parquet(os.path.join(training_df_path, geofile))\n",
    "    GDF.set_index('cell_id', inplace = True)\n",
    "    GDF['season_precip_cm'] = 0.0\n",
    "    \n",
    "    # get precip filenames\n",
    "    pptfiles = [filename for filename in os.listdir(precip_data_path) if filename.endswith('.parquet')]\n",
    "    # print(pptfiles)\n",
    "    # need to connect GDF to precip file(s) by date and basin\n",
    "    # this is clunky but will work for now\n",
    "    pptfiles_region_date = []\n",
    "    for i in range(len(pptfiles)):\n",
    "        pptfile_date = pptfiles[i].split('_')[-1].split('.parquet')[0]\n",
    "        pptfile_region = pptfiles[i].split('_')[-2]\n",
    "        pptfile_reg_date = f\"{pptfile_region}_{pptfile_date}\"\n",
    "        pptfiles_region_date.append(pptfile_reg_date)\n",
    "    # unique_dates = np.unique(pptfiles_dates)\n",
    "    \n",
    "    ppt_filename = [filename for filename in pptfiles if region_date in filename]\n",
    "    # print(region_date, ppt_filename)\n",
    "    \n",
    "    ppt_filepath = f\"{precip_data_path}/{ppt_filename[0]}\"\n",
    "    ppt = pd.read_parquet(ppt_filepath)\n",
    "        \n",
    "#     # get unique cells\n",
    "    sites = list(GDF.index)\n",
    "    for site in sites:\n",
    "        # print(site)\n",
    "        # print(ppt[ppt['cell_id']== site])\n",
    "        try:\n",
    "            GDF.loc[site,'season_precip_cm'] = round(ppt['season_precip_cm'][ppt['cell_id']== site].values[0],1)\n",
    "        except:\n",
    "            print(f\"{site} is bad, delete file from folder and rerun the get precipitation script\")\n",
    "    # print(ppt['season_precip_cm'].mean())\n",
    "    #Convert DataFrame to Apache Arrow Table\n",
    "    table = pa.Table.from_pandas(GDF)\n",
    "    # Parquet with Brotli compression\n",
    "    pq.write_table(table, f\"{precip_df_path}/Precip{dataset if dataset == 'Daymet' else ''}_{geofile}\", compression='BROTLI')\n",
    "#          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## figure out how to interpolate 1000m daymet precip to 750m \n",
    "\n",
    "test = pd.read_parquet(f'{HOME}/data/Precipitation/2024/1000M_Daymet_Precip/Daymet_BigThompson_20240421.parquet')\n",
    "gdf  = gpd.GeoDataFrame(test['season_precip_cm'],\n",
    "                       geometry=gpd.points_from_xy(test['cen_lon'], test['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "test.head()\n",
    "# # test.loc[:,['cen_lat','cen_lon','season_precip_cm']]\n",
    "# xtest = xr.DataArray(test['season_precip_cm'].values,\n",
    "#                      dims=['x','y'],\n",
    "#                      coords={'x':test['cen_lon'],'y':test['cen_lat']})\n",
    "# xtest\n",
    "\n",
    "gdf.plot(column='season_precip_cm',cmap='viridis',legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_parquet(f'{HOME}/data/TrainingDFs/2024/750M_Resolution/Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh/Vegetation_Sturm_Season_VIIRS_GeoObsdf_American_20240211.parquet')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trainingdf = pd.read_parquet(f'{HOME}/data/TrainingDFs/2024/750M_Resolution/Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh/Vegetation_Sturm_Season_VIIRS_GeoObsdf_BigThompson_20240421.parquet')\n",
    "# test_trainingdf.head()\n",
    "df_x = test_trainingdf['cen_lon'].sort_values().unique()\n",
    "df_y = test_trainingdf['cen_lat'].sort_values().unique()\n",
    "test_data = pd.read_parquet(f'{HOME}/data/Precipitation/2024/1000M_Daymet_Precip/Daymet_BigThompson_20240421.parquet')\n",
    "test_grid = test_data[['cen_lon','cen_lat']].values\n",
    "precip = test_data['season_precip_cm']\n",
    "\n",
    "interpolator = LinearNDInterpolator(test_grid,precip)\n",
    "X,Y = np.meshgrid(df_x,df_y)\n",
    "precip_interp = interpolator(X.flat,Y.flat).reshape(X.shape)\n",
    "xarr = xr.DataArray(precip_interp,dims=['lat','lon'],coords=[df_y,df_x])\n",
    "xarr = xarr.interpolate_na(dim=\"lon\", method=\"linear\", fill_value=\"extrapolate\").interpolate_na(dim=\"lat\", method=\"linear\", fill_value=\"extrapolate\")\n",
    "# extract metadata \n",
    "meta = test_trainingdf[['cell_id','cen_lat','cen_lon']]\n",
    "season_precip_cm = []\n",
    "nsites = len(meta)\n",
    "for i in range(nsites):\n",
    "    lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "    cellid = meta.iloc[i]['cell_id']\n",
    "    prcp = xarr.sel(lon=lon,lat=lat,method='nearest').item()\n",
    "    prcp=np.round(prcp,1)\n",
    "    # if season_precip >= 0:\n",
    "    season_precip_cm.append(prcp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f099349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df_path=f'{HOME}/data/TrainingDFs/2024/750M_Resolution/Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh'\n",
    "precip_df_path=f'{HOME}/data/TrainingDFs/2024/750M_Resolution/Daymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh'\n",
    "os.makedirs(precip_df_path,exist_ok=True)\n",
    "training_files = [filename for filename in os.listdir(training_df_path) if ~fnmatch.fnmatch(filename,'*Conejos_20240508.parquet')]\n",
    "for file in training_files:\n",
    "    date = file.split('_')[-1].split('.parquet')[0]\n",
    "    region = file.split('_')[-2]\n",
    "    # print(region,date)\n",
    "    region_date = f\"{region}_{date}\"\n",
    "\n",
    "    # get training df from previous step and extract cell locations\n",
    "    training_df = pd.read_parquet(f'{training_df_path}/Vegetation_Sturm_Season_VIIRS_GeoObsdf_{region_date}.parquet')\n",
    "    meta = training_df[['cell_id','cen_lat','cen_lon']]\n",
    "    df_x = meta['cen_lon'].sort_values().unique()\n",
    "    df_y = meta['cen_lat'].sort_values().unique()\n",
    "    # get 1000M daymet precip and extract locations and values \n",
    "    prcp_df_path = f'{HOME}/data/Precipitation/2024/1000M_Daymet_Precip/Daymet_{region_date}.parquet'\n",
    "    if os.path.exists(prcp_df_path)==False:\n",
    "        print('skipping',region_date)\n",
    "        continue\n",
    "    prcp_df = pd.read_parquet(f'{HOME}/data/Precipitation/2024/1000M_Daymet_Precip/Daymet_{region_date}.parquet')\n",
    "    prcp_grid = prcp_df[['cen_lon','cen_lat']].values\n",
    "    precip = prcp_df['season_precip_cm']\n",
    "    interpolator = LinearNDInterpolator(prcp_grid,precip)\n",
    "    X,Y = np.meshgrid(df_x,df_y)\n",
    "    precip_interp = interpolator(X.flat,Y.flat).reshape(X.shape)\n",
    "    xarr = xr.DataArray(precip_interp,dims=['lat','lon'],coords=[df_y,df_x])\n",
    "    xarr = xarr.interpolate_na(dim=\"lon\", method=\"linear\", fill_value=\"extrapolate\").interpolate_na(dim=\"lat\", method=\"linear\", fill_value=\"extrapolate\")\n",
    "    # match cell ids from training df to cell ids in interpolated precip df\n",
    "    season_precip_cm = []\n",
    "    nsites = len(meta)\n",
    "    for i in range(nsites):\n",
    "        lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "        cellid = meta.iloc[i]['cell_id']\n",
    "        prcp = xarr.sel(lon=lon,lat=lat,method='nearest').item()\n",
    "        prcp=np.round(prcp,1)\n",
    "        # if season_precip >= 0:\n",
    "        season_precip_cm.append(prcp)\n",
    "    training_df['Daymet'] = season_precip_cm\n",
    "    table = pa.Table.from_pandas(training_df)\n",
    "    # Parquet with Brotli compression\n",
    "    pq.write_table(table, f\"{precip_df_path}/PrecipDaymet_{file}\", compression='BROTLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_precip_files = [filename for filename in os.listdir(precip_df_path)] \n",
    "for file in new_precip_files:\n",
    "    df = pd.read_parquet(f'{precip_df_path}/{file}')\n",
    "    gdf = gpd.GeoDataFrame(df['Daymet'],\n",
    "                       geometry=gpd.points_from_xy(df['cen_lon'], df['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "    fig,ax=plt.subplots();\n",
    "    gdf.plot(column='Daymet',cmap='viridis',legend=True,ax=ax)\n",
    "    ax.set_title(file)\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xarr.plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550519d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(f'{HOME}/data/TrainingDFs/2024/750M_Resolution/Daymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/10_fSCA_Thresh/PrecipDaymet_Vegetation_Sturm_Season_VIIRS_GeoObsdf_WindyGap_20240321.parquet')\n",
    "gdf  = gpd.GeoDataFrame(test['Daymet'],\n",
    "                       geometry=gpd.points_from_xy(test['cen_lon'], test['cen_lat']), \n",
    "                        crs=\"EPSG:4326\")\n",
    "test.head()\n",
    "\n",
    "gdf.plot(column='Daymet',cmap='viridis',legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e4b58-6475-4d9c-b38f-b413d6725c3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WYs = [2024]\n",
    "for year in WYs:\n",
    "    Make_Precip_DF(year,output_res=1000,thresh=10,dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3c14d-06f3-4ec2-acd9-c1a600f53df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = f'{HOME}/data/Precipitation/2024/1000M_Daymet_Precip/Daymet_WindyGap_20240414.parquet'\n",
    "# test3 = f'{HOME}/data/TrainingDFs/2017/1000M_Resolution/PrecipVIIRSGeoObsDFs/10_fSCA_Thresh/PrecipDaymet_VIIRS_GeoObsdf_USCATB_20170727.parquet'\n",
    "test1 = pd.read_parquet(test1)\n",
    "# test3 = pd.read_parquet(test3)\n",
    "print(test1.loc[0,'precip'].shape)\n",
    "test1.head()\n",
    "# test3[test3['season_precip_cm'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5e096-eb91-420e-9ef0-3b22ab5d6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = f'/uufs/chpc.utah.edu/common/home/civil-group1/Johnson/SWEMLv2.0/data/TrainingDFs/Southwest/1000M_Resolution/VIIRSGeoObsDFs/20_fSCA_Thresh/VIIRS_GeoObsDF_20150403.parquet'\n",
    "# test2 = f'{HOME}/data/TrainingDFs/2015/1000M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/PrecipDaymet_VIIRS_GeoObsDF_20150403.parquet'\n",
    "test2 = pd.read_parquet(test2)\n",
    "# test2[test2[test_site]>0]\n",
    "test2.shape\n",
    "# test2.loc['2015_1000M_38.19_-119.801']\n",
    "# test2.iloc[int(test2.shape[0]/2):int(test2.shape[0]/2)+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57baa95-4d50-4a51-bc21-94d6cd5f6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e16dc0-01b0-44ef-a727-8dd23fc9ae2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file\n",
    "with rxr.open_rasterio(file) as src:\n",
    "            # reproject to WGS84\n",
    "            transformed = src.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))\n",
    "            left, bottom, right, top = transformed.rio.bounds()\n",
    "            # add some padding to bbox\n",
    "            left -= 0.1\n",
    "            bottom -= 0.1\n",
    "            right += 0.1\n",
    "            top += 0.1\n",
    "            bbox = rasterio.coords.BoundingBox(left, bottom, right, top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22daa90-4442-445f-a354-efe2b0d01da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WY = 2024\n",
    "filepath = f\"{HOME}/data/ASO/{WY}/Raw_ASO_Data\"\n",
    "files = [filename for filename in os.listdir(filepath) \n",
    "    if filename.endswith(\".tif\")\n",
    "    ]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eda539-68b2-40a4-97de-e87e212f16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"{filepath}/{files[-1]}\"\n",
    "tiff = rasterio.open(file)\n",
    "show(tiff)\n",
    "left,bottom,right,top = tiff.bounds\n",
    "tiff.bounds\n",
    "tiff.crs # looks like this is in UTM zone, probs want WGS84 for consistency? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbec49-4122-481f-88da-d8c55295fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff.crs == rasterio.CRS.from_epsg(32611)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c24420-880e-463f-886e-856ccbdad048",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = rxr.open_rasterio(file)\n",
    "transformed = raster.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))\n",
    "transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880d45a-584c-4234-ac33-a215fb324b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "left, bottom, right, top = transformed.rio.bounds()\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf63ba-02da-4b2d-87d8-49cd78b06cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.where(transformed > -1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fd7db-d13c-419c-a746-2b8fd0effdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = daymet.get_bygeom(tiff.bounds,dates=('2023-10-01','2024-05-27'),variables=\"prcp\",crs=32611)\n",
    "daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f708c73-2002-4991-9620-b301c05f83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f337852-1ede-4729-8878-6af67b05ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_transformed = daily.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d73129-2bd6-42d9-ab10-ca9f6471d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_transformed.sel(time='2013-01-09')['prcp'].plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053fddf-2220-4f70-a946-074ff70c7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cg1/Golitzin/SWEMLv2.0/data/TrainingDFs/2013/1000M_Resolution/2013_metadata.parquet\n",
    "WY = [2015]\n",
    "output_res=1000\n",
    "meta_path = f\"{HOME}/data/TrainingDFs/{WY[0]}/{output_res}M_Resolution/{WY[0]}_metadata.parquet\"\n",
    "# ASO_meta_path = f\"{HOME}/data/TrainingDFs/{region}/{output_res}M_Resolution/ASO_meta.parquet\"\n",
    "meta = pd.read_parquet(meta_path)\n",
    "# ASO_meta = pd.read_parquet(ASO_meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484c2b4-24a7-4fc3-a731-e9a29655ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06caa88a-cba7-4f24-9635-93ca32bfd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['cen_lat'].min(),meta['cen_lat'].max(),meta['cen_lon'].min(),meta['cen_lon'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bd186-57bd-4d80-89ec-de78d6aaacaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random point within study area to check precip values \n",
    "test_lat, test_lon = meta.iloc[500]['cen_lat'],meta.iloc[500]['cen_lon']\n",
    "test_lat,test_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284e3eb-56b7-485f-9083-f080bcf2e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check values, are the data real? \n",
    "daily_transformed.sel(x=test_lon,y=test_lat,method='nearest')['prcp'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b92801-b33e-46ee-b5b3-9ace7dff8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASO_Key=pd.read_csv(f\"{HOME}/../ASOKey.csv\",header=3)\n",
    "ASO_Key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3acfb-9e25-4d0b-b712-eccbb8e860a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse dates and site name from ASO observation\n",
    "# code borrowed from ASOfigs\n",
    "def filename_parse(filename):\n",
    "    date = next(element for element in os.path.splitext(filename)[0].split(\"_\") if element.startswith('20'))\n",
    "    if date[4].isnumeric() == False:\n",
    "        date_singleday = os.path.splitext(date)[0].split(\"-\")[0]\n",
    "        datetime_object = datetime.strptime(date_singleday, \"%Y%b%d\")\n",
    "        date = datetime_object.strftime('%Y%m%d')\n",
    "    #identify basin from site code if applicable, else identify basin from name\n",
    "    if filename[:12] == \"ASO_50M_SWE_\":\n",
    "        # print(file[12:18])\n",
    "        sitecode = filename[12:18]\n",
    "        index = ASO_Key['SITE CODE']==sitecode\n",
    "        sitename=(ASO_Key.loc[index,'SITE NAME']).item().replace(\" \",\"_\")\n",
    "        # print(sitename)\n",
    "        newfilename = f\"ASO_{sitename}_{sitecode}_{date}\"\n",
    "        # print(newfilename)\n",
    "    else:\n",
    "        sitename = os.path.splitext(filename)[0].split(\"_\")[1]\n",
    "        newfilename = f\"ASO_{sitename}_{date}\"\n",
    "    return(date, newfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce676c-623c-433b-880b-ab76357ce1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is going to be very annoying\n",
    "# fix later (7/16)\n",
    "filename = files[-1] \n",
    "date, filename_std = filename_parse(filename)\n",
    "date = datetime.strptime(date,'%Y%m%d')\n",
    "date\n",
    "date_str = datetime.strftime(date, '%Y-%m-%d')\n",
    "date_str, filename_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bd10c-a936-4ad3-8869-cc7cc4b5e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_std[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f1d98-8d0c-49e8-8294-57c72c778376",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_start = datetime.strftime(datetime(date.year-1, 10, 1),'%Y-%m-%d')\n",
    "obs_end = date_str\n",
    "obs_start,obs_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0854bc-2242-4a19-ad44-c46a5ac273ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb8dfe0-e1e5-4667-b4ed-bcb07dfa0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "left,bottom,right,top = tiff.bounds\n",
    "# add some padding to make sure all values in ASO obs are represented in daymet file\n",
    "new_bounds = left-1500,bottom-1500,right+1500,top+1500\n",
    "new_bounds[0]\n",
    "new_bounds = rasterio.coords.BoundingBox(new_bounds[0],new_bounds[1],new_bounds[2],new_bounds[3])\n",
    "print(tiff.bounds,'\\n', new_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446043a-31b5-46ae-bfed-2ce28b152313",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip = daymet.get_bygeom(new_bounds,dates=(obs_start,obs_end),variables=\"prcp\",crs=32611)\n",
    "obs_precip_transformed = obs_precip.rio.reproject(rasterio.crs.CRS.from_epsg('4326'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dadd53-6d82-4e43-b144-d9e181580795",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ad3f0-1e4f-4bb9-b772-9f6f47259c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_transformed.sel(time='2013-01-09')['prcp'].plot.pcolormesh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46f427-59b0-4e51-beec-bb8b38e808d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_precip_transformed.sel(x=test_lon,y=test_lat,method='nearest')['prcp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301755e1-63aa-456f-b37d-c588c974f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretically want seasonally accumulated precip up until ASO obs date for each obs\n",
    "# possibly better to get daily precip values, then sum later when adding to training DF\n",
    "# want to include basin label and date in any precip filename (per RJ)\n",
    "precip_arr = []\n",
    "\n",
    "nsites = len(meta)\n",
    "# nsites = 100 # for now\n",
    "for i in range(nsites):\n",
    "    lat, lon = meta.iloc[i]['cen_lat'],meta.iloc[i]['cen_lon']\n",
    "    cellid = meta.iloc[i].name\n",
    "    prcp = obs_precip_transformed.sel(x=lon,y=lat,method='nearest')['prcp']\n",
    "    precip_arr.append([cellid,lat,lon,np.array(prcp.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68667a-6694-4c82-9ff3-5fe6d5552ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_df = pd.DataFrame(precip_arr,columns = ['cell_id','cen_lat','cen_lon','precip'])\n",
    "precip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8910d2-ca5a-44cd-b6c4-7d8a10600cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precippath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762a3af-d993-4b83-b3c7-c5b7ea7140eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if values exist & are real\n",
    "# choose a random cell and plot precip over WY\n",
    "test_idx = np.random.randint(1,nsites,size=1)\n",
    "print(test_idx[0])\n",
    "print(precip_df.iloc[test_idx[0]]['precip'].sum())\n",
    "plt.plot(precip_df.iloc[test_idx[0]]['precip'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190a58d-718e-4e92-b642-49a369dde9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_precip_cm = []\n",
    "for i in range(nsites):\n",
    "    season_precip_cm.append(np.round(precip_df.iloc[i]['precip'].sum()/10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26ce7c-0048-4e35-945f-a04030f2c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(season_precip_cm[:50])\n",
    "print(trainingDF.iloc[:50]['season_precip_cm'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcfced-a4f4-4819-81e9-1b9363dfe327",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_df['season_precip_cm'] = season_precip_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa789cc-d3f5-44da-838d-328985eabefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f87b75-69b4-4ce3-b0f5-987878c4a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2013\n",
    "output_res = 1000\n",
    "\n",
    "Precippath = f\"{HOME}/data/Precipitation/{year}/{output_res}M_Daymet_Precip\"\n",
    "if not os.path.exists(Precippath):\n",
    "    os.makedirs(Precippath, exist_ok=True)\n",
    "\n",
    "#Convert DataFrame to Apache Arrow Table\n",
    "table = pa.Table.from_pandas(precip_df)\n",
    "# Parquet with Brotli compression\n",
    "pq.write_table(table, f\"{Precippath}/Daymet_{filename_std[4:]}.parquet\", compression='BROTLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07977d91-b474-4e06-a6a2-71e9dc6108f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_filepath = f'{HOME}/data/Precipitation/Southwest/1000M_NLDAS_Precip/sites/Southwest_1000M_37.816_-119.259.parquet'\n",
    "# just wanna see what the old \"sites\" parquet looks like \n",
    "# just season accumulated precip for a single cell for all ASO obs dates at that cellid \n",
    "test_filepath = f'{HOME}/data/Precipitation/Southwest/1000M_NLDAS_Precip/sites/NLDAS_PPT_Southwest_1000M_38.181_-119.585.parquet'\n",
    "test_file = pd.read_parquet(test_filepath)\n",
    "test_file[test_file['datetime']=='2013-06-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a33836-d6cf-41ea-85ec-90d19a396e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4746ae-caa2-4327-b3d3-98cf0e1e11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check structure of training DF w precip\n",
    "filepath = f'{HOME}/data/TrainingDFs/2013/1000M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/Precip_VIIRS_GeoObsDF_20130608.parquet'\n",
    "trainingDF = pd.read_parquet(filepath)\n",
    "trainingDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a553693-cd61-43a5-8f87-5ae2001cf9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare random entry in new precip DF to old training DF\n",
    "test_idx = np.random.randint(1,nsites,size=1)[0]\n",
    "print('testing index',test_idx)\n",
    "print('total daymet precip =',precip_df.iloc[test_idx]['precip'].sum()/10)\n",
    "print(precip_df.iloc[test_idx]['cell_id'])\n",
    "print(trainingDF.iloc[test_idx].name)\n",
    "print('total NLDAS precip =',trainingDF.iloc[test_idx]['season_precip_cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be213eda-9534-47e1-9da6-0a95971f5b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbdd026-a9e0-4d13-b41f-fa199e66182c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47bfb8f-e693-472f-8cf2-9a827d613440",
   "metadata": {},
   "outputs": [],
   "source": [
    "timedelt = datetime(2013,5,3)-datetime(2012,10,1)\n",
    "timedelt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1239b2-973c-466e-b74c-64250a7e8bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{HOME}/data/TrainingDFs/{region}/{output_res}M_Resolution/VIIRSGeoObsDFs/{20}_fSCA_Thresh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f1d53-ce13-434d-a07d-fbb7e7e8a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "WY = 2013\n",
    "output_res = 1000\n",
    "threshold = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ccfb3-2b6b-40ff-8955-e2c9aab9aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Precip_DF(WY, output_res, threshold):\n",
    "\n",
    "    print(f\"Adding precipitation features to ML dataframe for {WY}.\")\n",
    "    Precippath = f\"{HOME}/data/Precipitation/{WY}/{output_res}M_Daymet_Precip/\"\n",
    "    DFpath = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/VIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "\n",
    "    #make precip df path\n",
    "    PrecipDFpath = f\"{HOME}/data/TrainingDFs/{WY}/{output_res}M_Resolution/PrecipVIIRSGeoObsDFs/{threshold}_fSCA_Thresh\"\n",
    "    if not os.path.exists(PrecipDFpath):\n",
    "        os.makedirs(PrecipDFpath, exist_ok=True)\n",
    "\n",
    "    #Get list of dataframes\n",
    "    GeoObsDF_files = [filename for filename in os.listdir(DFpath)]\n",
    "    \n",
    "    for geofile in GeoObsDF_files:\n",
    "        single_date_add_precip((DFpath, Precippath, geofile, PrecipDFpath, WY))\n",
    "    # print(GeoObsDF_files)\n",
    "    # with cf.ProcessPoolExecutor(max_workers=CPUS) as executor: \n",
    "    #     # Start the load operations and mark each future with its process function\n",
    "    #     [executor.submit(single_date_add_precip, (DFpath, Precippath, geofile, PrecipDFpath, WY)) for geofile in GeoObsDF_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58076e79-c512-4a03-9cab-635862f5fb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Make_Precip_DF(WY,output_res,threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f93aba-8d3c-47a5-82d6-c610161522c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    pptfiles = [filename for filename in os.listdir(Precippath) if filename.endswith('parquet')]\n",
    "    print(pptfiles)\n",
    "    \n",
    "pptfiles[0].split('_')[-1].split('.parquet')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82196b-3844-4fa5-bc1d-768a4c07e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_date_add_precip(args):\n",
    "    DFpath, Precippath, geofile, PrecipDFpath, WY = args\n",
    "    #get date information\n",
    "    date = geofile.split('VIIRS_GeoObsDF_')[-1].split('.parquet')[0]\n",
    "    year = date[:4]\n",
    "    mon = date[4:6]\n",
    "    day = date[6:]\n",
    "    strdate = f\"{year}-{mon}-{day}\"\n",
    "    print(f\"Connecting precipitation to ASO observations for {WY} on {strdate}\")\n",
    "    \n",
    "    GDF = pd.read_parquet(os.path.join(DFpath, geofile))\n",
    "    GDF.set_index('cell_id', inplace = True)\n",
    "    GDF['season_precip_cm'] = 0.0\n",
    "    \n",
    "    # get precip filenames\n",
    "    pptfiles = [filename for filename in os.listdir(Precippath)]\n",
    "    print(pptfiles)\n",
    "    \n",
    "    ppt_idx = -1\n",
    "    # connect GDF to correct precip file by date\n",
    "    for i in range(len(pptfiles)):\n",
    "        ppt_date = pptfiles[i].split('_')[-1].split('.parquet')[0]\n",
    "        if ppt_date == date:\n",
    "            ppt_idx = i\n",
    "            break\n",
    "    if ppt_idx > -1:     \n",
    "        ppt = pd.read_parquet(f\"{Precippath}/{pptfiles[ppt_idx]}\")\n",
    "    else:\n",
    "        raise Exception('Failed to connect precip observations to dataframe')\n",
    "        \n",
    "    #get unique cells\n",
    "    sites = list(GDF.index)\n",
    "    for site in sites:\n",
    "        try:\n",
    "            GDF.loc[site,'season_precip_cm'] = round(ppt['season_precip_cm'][ppt['cell_id']== site].values[0],1)\n",
    "        except:\n",
    "            print(f\"{site} is bad, delete file from folder and rerun the get precipitation script\")\n",
    "\n",
    "    #Convert DataFrame to Apache Arrow Table\n",
    "    table = pa.Table.from_pandas(GDF)\n",
    "    # Parquet with Brotli compression\n",
    "    pq.write_table(table, f\"{PrecipDFpath}/PrecipDaymet_{geofile}\", compression='BROTLI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680624d-9d45-4bab-98bc-c4853a799d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_training_df = pd.read_parquet(f'{HOME}/data/TrainingDFs/2013/1000M_Resolution/PrecipVIIRSGeoObsDFs/20_fSCA_Thresh/PrecipDaymet_VIIRS_GeoObsDF_20130608.parquet')\n",
    "new_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acaa22-84b5-4d80-9485-55f646029df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_df.iloc[1]['cell_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d640d8-f582-420d-bb22-d9e5b256d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_df.iloc[1]['precip'].sum()/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f5fc7-9726-40cb-94e3-a920dfacb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precip_df.iloc[1]['precip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8c2b2-3f69-4ef8-9554-ad24d1349b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_file = pd.read_parquet(f'{HOME}/data/Precipitation/Northwest/1000M_NLDAS_Precip/sites/NLDAS_PPT_Northwest_1000M_47.888_-123.856.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a0694-3dec-4975-a7d6-0cc5c3d348c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.iloc[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec023a-39f2-43dd-b214-58f871d011f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac72a7-1874-45f5-b2cf-85cce3a1a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta['cen_lat'].min(),meta['cen_lat'].max())\n",
    "print(meta['cen_lon'].min(),meta['cen_lon'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd7e4d-f803-42ba-9220-a48d2248685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat,long = meta[['cen_lat','cen_lon']].median()\n",
    "location = ee.Geometry.Point(long,lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82841526-cb49-4ade-932e-822c53c1237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat,long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdc3be-1818-47a6-88e7-740d4bc9dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate='2015-10'\n",
    "enddate='2019-07'\n",
    "precip = ee.ImageCollection('NASA/NLDAS/FORA0125_H002').select('total_precipitation').filterDate(startdate, enddate)\n",
    "daymet_precip = ee.ImageCollection(\"NASA/ORNL/DAYMET_V4\").select('prcp').filterDate(startdate,enddate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0019d8b-b594-4667-82f3-c8d136e9021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_poi = precip.getRegion(location,scale=1000).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2b58f-6839-4fed-bf11-d81c6db44501",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_precip_poi = daymet_precip.getRegion(location,scale=1000).getInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6f159-65f8-4a0c-aec4-b60ceea6bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe8c94-b5f9-48ee-a8c8-afbaded33414",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_gdf = geemap.ee_to_df(daymet_precip_poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441710d-237e-4698-8383-54b73f99f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_precip = EE_funcs.ee_array_to_df(precip_poi,['total_precipitation'])\n",
    "daymet_site_precip = EE_funcs.ee_array_to_df(daymet_precip_poi,['prcp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5d0bb-1c68-4ead-92f0-5132ebf041c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_resample = 'D'\n",
    "kgm2_to_cm = 0.1\n",
    "\n",
    "site_precip.set_index('datetime', inplace = True)\n",
    "site_precip = site_precip.resample(temporal_resample).sum()\n",
    "site_precip.reset_index(inplace = True)\n",
    "\n",
    "        #make columns for cms\n",
    "site_precip['total_precipitation'] = site_precip['total_precipitation']*kgm2_to_cm\n",
    "site_precip.rename(columns={'total_precipitation':'daily_precipitation_cm'}, inplace = True)\n",
    "site_precip.pop('time')\n",
    "site_precip.set_index('datetime',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7ebd8-545e-49ea-a883-3566692d23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WYdict = {2016,2019}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274278e-ae09-4fcd-bb7c-49d481dda0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_daymet = ee.ImageCollection('NASA/ORNL/DAYMET_V4').select('prcp').filterDate(startdate, enddate)\n",
    "precip_daymet_poi = precip_daymet.getRegion(location,scale=1000).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ec843-260b-40f7-8580-192f5082b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_precip_daymet = EE_funcs.ee_array_to_df(precip_daymet_poi,['prcp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e3614-2fd0-490c-b417-ad729cf03d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_precip_daymet['prcp'] /= 10\n",
    "site_precip_daymet.pop('time')\n",
    "site_precip_daymet.set_index('datetime',inplace=True)\n",
    "site_precip_daymet.rename(columns={'prcp':'daily_precipitation_cm'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d286e-aba7-44d0-b052-5032a896a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_precip_daymet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbeeef-23f8-43ab-b4cf-1853d4af1625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb68965-cb08-48fb-b2d8-49f4b70b8b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66056a8-6724-47b7-93eb-438ab5dd4fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "sweml_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
