{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push/Pull to AWS\n",
    "* Training DF files\n",
    "* then push other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#set path directory\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"SWEMLv2.0/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'national-snow-model'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training DF regional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload nearest-snotel dictionaries\n",
    "regionlist = ['SouthernRockies', 'Southwest', 'Northwest']\n",
    "output_res = 300\n",
    "\n",
    "for region in regionlist:\n",
    "    print(region)\n",
    "    head_folder = f\"SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution/\"\n",
    "    head_folder_dir = f\"{HOME}/{head_folder}\"\n",
    "\n",
    "    #Get files only\n",
    "    files = [f for f in listdir(head_folder_dir) if isfile(join(head_folder_dir, f))]\n",
    "    for file in tqdm_notebook(files):\n",
    "        S3.meta.client.upload_file(Filename= f\"{head_folder_dir}{file}\", Bucket=BUCKET_NAME, Key=f\"{head_folder}{file}\")\n",
    "\n",
    "    #get list of directories to download\n",
    "    dirs = [ f.path for f in os.scandir(head_folder_dir) if f.is_dir() ]\n",
    "    for dir in dirs:\n",
    "        print(f\"Folder name: {dir.split('300M_Resolution/')[-1]}\")\n",
    "        awsfolderpath = f\"SWEMLv2.0{dir.split('SWEMLv2.0')[-1]}\"\n",
    "        headfolderpath = f\"{HOME}/{awsfolderpath}\"\n",
    "        files = [f for f in listdir(headfolderpath) if isfile(join(headfolderpath, f))]\n",
    "        for file in tqdm_notebook(files):\n",
    "            S3.meta.client.upload_file(Filename= f\"{headfolderpath}/{file}\", Bucket=BUCKET_NAME, Key=f\"{awsfolderpath}/{file}\")\n",
    "\n",
    "        #Get subfolders\n",
    "        subdirs = [ f.path for f in os.scandir(headfolderpath) if f.is_dir() ]\n",
    "        for dir in subdirs:\n",
    "            print(f\"Folder name: {dir.split('300M_Resolution/')[-1]}\")\n",
    "            awsfolderpath2 = f\"SWEMLv2.0{dir.split('SWEMLv2.0')[-1]}\"\n",
    "            headfolderpath2 = f\"{HOME}/{awsfolderpath2}\"\n",
    "            files = [f for f in listdir(headfolderpath2) if isfile(join(headfolderpath2, f))]\n",
    "            for file in tqdm_notebook(files):\n",
    "                S3.meta.client.upload_file(Filename= f\"{headfolderpath2}/{file}\", Bucket=BUCKET_NAME, Key=f\"{awsfolderpath2}/{file}\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "for region in regionlist:\n",
    "    path = f\"SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution/\"\n",
    "\n",
    "    #Make directory if it does not exist\n",
    "    if not os.path.exists(f\"{HOME}/{path}\"):\n",
    "        print(\"Path not present, making\")\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    #Identify the potential files to download\n",
    "    files = [objects.key for objects in BUCKET.objects.filter(Prefix=f\"SWEMLv2.0/data/TrainingDFs/{region}/{output_res}M_Resolution\")]\n",
    "    \n",
    "    for file in files:\n",
    "        #check to see if the file is there\n",
    "        if os.path.exists(f\"{HOME}/{file}\") == False:\n",
    "            print('Downloading file')\n",
    "            S3.Bucket(BUCKET_NAME).download_file(file, f\"{path}/{file}\")\n",
    "\n",
    "        else:\n",
    "            print('File already present...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
