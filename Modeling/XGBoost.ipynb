{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from model_scripts import Simple_Eval, dataloader, dataprocessor, xgb_model #had to pip install xgboost\n",
    "HOME = os.path.expanduser('~')\n",
    "\n",
    "modelname = 'XGBoost'\n",
    "model_path = f\"{HOME}/SWEMLv2.0/Model/{modelname}\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "print(f\"{modelname} development script, {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "regionlist = ['SouthernRockies', 'Southwest', 'Northwest']\n",
    "output_res = '300M_Resolution'\n",
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "fSCA_thresh = '20_fSCA_Thresh'\n",
    "\n",
    "alldata = dataloader.get_ML_Data(regionlist, output_res, DataFrame, fSCA_thresh)\n",
    "alldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out a test condition by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out 3-29-2019 in Southwest\n",
    "TestArea = alldata[alldata['Date'] == '2019-03-29']\n",
    "TestArea.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#remove Test area data from training/testing dataset\n",
    "df = pd.concat([alldata, TestArea]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data\n",
    "df = dataprocessor.data_clean(df, regionlist)\n",
    "df.head()\n",
    "\n",
    "#temporary for seasonality relationship\n",
    "df.fillna(1, inplace = True)\n",
    "\n",
    "#convert dates to datetime format\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "\n",
    "input_columns = [\n",
    "            'cen_lat',\t\n",
    "            'cen_lon',\t\n",
    "            'Elevation_m',\t\n",
    "            'Slope_Deg',\t\n",
    "            'Aspect_Deg',\t\n",
    "            'ns_1',\t\n",
    "            'ns_2',\t\n",
    "            'ns_3',\t\n",
    "            'ns_4',\t\n",
    "            'ns_5',\t\n",
    "            'ns_6',\t\n",
    "            'VIIRS_SCA', \n",
    "            'hasSnow',\n",
    "            'season_precip_cm',\n",
    "            'region_class',\n",
    "            'DOS', \n",
    "            'WY_week',\n",
    "            'ns_1_week_mean', \n",
    "            'ns_2_week_mean', \n",
    "            'ns_3_week_mean', \n",
    "            'ns_4_week_mean',\n",
    "            'ns_5_week_mean', \n",
    "            'ns_6_week_mean', \n",
    "            'Seasonal_ns_1_rel',\n",
    "            'Seasonal_ns_2_rel',\n",
    "            'Seasonal_ns_3_rel', \n",
    "            'Seasonal_ns_4_rel',\n",
    "            'Seasonal_ns_5_rel', \n",
    "            'Seasonal_ns_6_rel',\n",
    "            'sturm_value'\n",
    "]\n",
    "\n",
    "years = False\n",
    "splitratio = 0.33\n",
    "test_years = [2019]\n",
    "target = 'swe_cm'\n",
    "\n",
    "#fit a scaler,save, and scale the training data\n",
    "x_train, y_train, x_test, y_test = dataprocessor.xgb_processor(\n",
    "                                                    regionlist,\n",
    "                                                      df, \n",
    "                                                      years, \n",
    "                                                      splitratio,\n",
    "                                                      test_years, \n",
    "                                                      target, \n",
    "                                                      input_columns, \n",
    "                                                      model_path, \n",
    "                                                      scalertype = 'MinMax'\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "tries = 1 #what is tries?\n",
    "hyperparameters = {\n",
    "    'max_depth': range (5, 21, 5),\n",
    "    'n_estimators': range(200, 1500, 500),\n",
    "    'eta': [0.1,]\n",
    "}\n",
    "perc_data = 0.25 # percent of training data used to identify optimial hyperparameters\n",
    "\n",
    "Use_fSCA_Threshold = True\n",
    "\n",
    "xgb_model.XGB_Train(model_path, \n",
    "                    input_columns, \n",
    "                    x_train, \n",
    "                    y_train, \n",
    "                    tries, \n",
    "                    hyperparameters,\n",
    "                    perc_data)\n",
    "\n",
    "#Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES\n",
    "PredsDF = pd.DataFrame()\n",
    "PredsDF = xgb_model.XGB_Predict(\n",
    "                    model_path, \n",
    "                    modelname, \n",
    "                    x_test,\n",
    "                    y_test,\n",
    "                    Use_fSCA_Threshold\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(Simple_Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a sample to determine model skill - Take 1000 from each modeling domain\n",
    "n_samples = 1000\n",
    "SampleDF, PredsDF = Simple_Eval.SamplePreds(regionlist, PredsDF, df, n_samples)\n",
    "\n",
    "\n",
    "savfig = False\n",
    "figname = 'Model-Testing-Split-Performance'\n",
    "\n",
    " #Evaluate model performance of the different models\n",
    "prediction_columns = [f\"{modelname}_swe_cm\"]\n",
    "Eval_DF = Simple_Eval.Simple_Eval(regionlist,\n",
    "                                SampleDF,\n",
    "                                prediction_columns, \n",
    "                                modelname, \n",
    "                                savfig, \n",
    "                                figname,\n",
    "                                plots = False, \n",
    "                                keystats = False        \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppath = f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/{output_res}/{fSCA_thresh}\"\n",
    "if not os.path.exists(Ppath):\n",
    "    os.makedirs(Ppath, exist_ok=True)\n",
    "\n",
    "\n",
    "#save the model predictions\n",
    "table = pa.Table.from_pandas(PredsDF)\n",
    "# Parquet with Brotli compression\n",
    "pq.write_table(table, f\"{Ppath}/Test_preds.parquet\", compression='BROTLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on the held out date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Prep prediction data\n",
    "y_test_Area = pd.DataFrame(TestArea['swe_cm'])\n",
    "dropcols = ['cell_id',\t'Date', 'swe_cm', 'region']\n",
    "x_test_Area = TestArea.drop(columns=dropcols)\n",
    "x_test_Area = x_test_Area[input_columns]\n",
    "\n",
    "#make a prediction\n",
    "holdoutdate = xgb_model.XGB_Predict(\n",
    "                    model_path, \n",
    "                    modelname, \n",
    "                    x_test_Area,\n",
    "                    y_test_Area,\n",
    "                    Use_fSCA_Threshold\n",
    "                    )\n",
    "#Add geospatial information to prediction DF\n",
    "EvalDF = pd.concat([TestArea, holdoutdate], axis=1)\n",
    "#EvalDF.drop(['index'], axis=1, inplace=True)\n",
    "EvalDF = EvalDF.loc[:,~EvalDF.columns.duplicated()].copy()\n",
    "\n",
    "EvalDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ppath = f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/{output_res}/{fSCA_thresh}\"\n",
    "if not os.path.exists(Ppath):\n",
    "    os.makedirs(Ppath, exist_ok=True)\n",
    "\n",
    "\n",
    "#save the model predictions\n",
    "table = pa.Table.from_pandas(EvalDF)\n",
    "# Parquet with Brotli compression\n",
    "pq.write_table(table, f\"{Ppath}/All_Feats_HoldOut_03-29-2019.parquet\", compression='BROTLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def SpatialAnalysis(EvalDF):\n",
    "    #Convert to a geopandas DF\n",
    "    Pred_Geo = gpd.GeoDataFrame(EvalDF, geometry = gpd.points_from_xy(EvalDF.cen_lon, EvalDF.cen_lat))\n",
    "\n",
    "    Pred_Geo.plot(column='Elevation_m',\n",
    "                  legend=False,\n",
    "                )\n",
    "    \n",
    "SpatialAnalysis(EvalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
