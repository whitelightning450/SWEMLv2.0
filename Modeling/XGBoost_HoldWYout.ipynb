{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWEMLv2.0 Hold Water Years out script\n",
    "This design of the script is to align with the SWE test bed set up by Andy Wood and Ethan Ritchie. The testbed uses ASO scenes from WYs 2019-2021 to evaluate SWE model estimate predictive performance within the NextGen HydroFabric framework. In addition to the SWE testbed, this script supportd direct comparison of x M resolution model predictions with the respective ASO image (e.g., not within the hydrofabric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from model_scripts import Simple_Eval, dataloader, dataprocessor2, xgb_model, SSWEET, DF_to_GeoTiff\n",
    "HOME = os.getcwd()\n",
    "\n",
    "modelname = 'XGBoost'\n",
    "model_path = f\"{HOME}/SWEMLv2.0/Model/{modelname}\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "print(f\"{modelname} development script, {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  input_columns = [\n",
    "                        'cen_lat',\t\n",
    "                        'cen_lon',\t\n",
    "                        'Elevation_m',\t\n",
    "                        'Slope_Deg',\t\n",
    "                        # 'Aspect_Deg',\t\n",
    "                        'Aspect_N',\n",
    "                        'Aspect_W',\n",
    "                        'ns_1',\t\n",
    "                        'ns_2',\t\n",
    "                        'ns_3',\t\n",
    "                        'ns_4',\t\n",
    "                        'ns_5',\t\n",
    "                        'ns_6',\t\n",
    "                        'VIIRS_SCA', \n",
    "                        'hasSnow',\n",
    "                        # 'Daymet',\n",
    "                        'NLDAS',\n",
    "                        # 'PRISM',\n",
    "                        # 'gridMET',\n",
    "                        # 'AORC',\n",
    "                        # 'season_precip_cm',\n",
    "                     #   'region_class',\n",
    "                        'DOS', \n",
    "                        'WY_week',\n",
    "                        'ns_1_week_mean', \n",
    "                        'ns_2_week_mean', \n",
    "                        'ns_3_week_mean', \n",
    "                        'ns_4_week_mean',\n",
    "                        'ns_5_week_mean', \n",
    "                        'ns_6_week_mean', \n",
    "                        'ns_1_anomoly',\n",
    "                        'ns_2_anomoly',\n",
    "                        'ns_3_anomoly', \n",
    "                        'ns_4_anomoly',\n",
    "                        'ns_5_anomoly', \n",
    "                        'ns_6_anomoly',\n",
    "                        'sturm_value',\n",
    "                    # 'vegetation_value'\n",
    "            ]\n",
    "    \n",
    "file_path = f\"{HOME}/SWEMLv2.0/data/ASO/date_basin.pkl\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model on all years but the testing year of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Testing for this model is on multiple scenes from WY 2019-2021. \n",
    "Model training includes all other ASO imagery.\n",
    "'''\n",
    "#Model set up\n",
    "DataFrame = 'AORCgridMETNLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs'\n",
    "fSCA_thresh = '10_fSCA_Thresh' # from the literature, I think this needs to be relaxed to 5 or 10%\n",
    "remove0swe = True #set to False to use all data, set to True to remove values under a certain threshold\n",
    "removeswe_thresh = 5 #value in cm to remove ASO obs/rows from. most papers use 10 cm...\n",
    "# Years = ['2013','2014','2015','2016','2017','2018','2019']\n",
    "Years =   [\n",
    "    '2013','2014','2015', '2016', \n",
    "    '2017', '2018', '2019', '2020', '2021','2022', '2023'\n",
    "] \n",
    "Train = True #set to true if you are training a model, False if you just want to make predictions or adjust marker size of images, etc.\n",
    "DataUsed = [False] #use all data for training (True) and test with HOO, or have training/testing and HOO (False) - False leading to better model\n",
    "SampleMethods =  [False] #Randomly sample all data before splitting for train/test (TRue), split train/test first, then randomize training (False)  - False leading to better model\n",
    "Res = ['750M_Resolution'] #'300M_Resolution', '500M_Resolution', '750M_Resolution', '1000M_Resolution'\n",
    "tries = 1 \n",
    "hyperparameters = {\n",
    "    'max_depth': [3,4,6,8,10],     #[8], #range (7, 9,1), #7-good\n",
    "    'n_estimators': [500,1000,3500,5000,10000],#  [20,500,3500],#[3500], #range(3000, 3600, 500),  #5500 - at max tested estimator, test higher and lower values on HOO data\n",
    "    'eta': [0.1],\n",
    "    'objective': ['reg:squarederror'], #try the 'reg:pseudohubererror', huber_slope=1.0, or 'reg:absoluteerror' or 'reg:squaredlogerror'\n",
    "     #'objective': ['reg:squarederror'], \n",
    "    # 'huber_slope':.02,\n",
    "     #'max_leaves':[32], #change max_depth to 0 when using this\n",
    "     #'grow_policy':['lossguide'],\n",
    "     #'min_child_weight': [1],\n",
    "       'random_state' : [42],\n",
    "       'n_jobs' : [-1]\n",
    "}\n",
    "gridsearch = True #False is good for quick analysis, set to True to optimize the model, True doesnt seem to work for the pseudohubererror\n",
    "perc_data = 0.30 # percent of training data used to identify optimal hyperparameters\n",
    "Use_fSCA_Threshold = True\n",
    "\n",
    "#set figure params\n",
    "markersize = 50\n",
    "\n",
    "#train each model and predict\n",
    "for output_res in Res:\n",
    "    for dataused in DataUsed:\n",
    "        for standard in SampleMethods:\n",
    "            for year in Years:\n",
    "                model_path = f\"{HOME}/SWEMLv2.0/Model/{modelname}/{DataFrame}/{year}\"\n",
    "                if not os.path.exists(model_path):\n",
    "                    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "                sim  =  f\"Test_WY{year}\" \n",
    "                #load data\n",
    "                WY_list = ['2013', '2014', '2015', '2016', '2017', '2018', '2019' , '2020', '2021','2022', '2023'\n",
    "                ]\n",
    "                WY_list.remove(year)\n",
    "                alldata = dataloader.get_ML_Data(WY_list, output_res, DataFrame, fSCA_thresh)\n",
    "\n",
    "                #small adjustments to correct some data\n",
    "                #aspect\n",
    "                # alldata['Aspect_Deg'] = (-alldata['Aspect_Deg']+270)%360\n",
    "                # df = alldata[alldata['Aspect_Deg']>= 0]\n",
    "                # df['Aspect_Deg'] = abs(df['Aspect_Deg']-90)+270\n",
    "                # alldata.update(df)\n",
    "                # alldata['Aspect_Deg'] = abs(alldata['Aspect_Deg'])\n",
    "\n",
    "                '''for Vegetation_Sturm_Seasonality_PrecipVIIRSGeoObsDFs, ASO obs\n",
    "                put SW 2018-04-22, 2018-03-04, San Joaquin, in bad data folder seems too big for peaks\n",
    "                in Southern Rockies 2019-04-07, East River, way too high of values, put in bad data folder'''\n",
    "\n",
    "                #clean the data\n",
    "                df = alldata.copy()\n",
    "                df = dataprocessor2.data_clean(df, WY_list)\n",
    "                df.head()\n",
    "\n",
    "                #temporary for seasonality relationship\n",
    "                df.fillna(1, inplace = True)\n",
    "                # temporary for daymet dfs\n",
    "                df.reset_index(inplace=True)\n",
    "                df.drop(columns='index',inplace=True)\n",
    "\n",
    "                #convert dates to datetime format\n",
    "                df.Date = pd.to_datetime(df.Date)\n",
    "\n",
    "                years = False\n",
    "                splitratio = 0.1\n",
    "                test_years = [2019] #this currently does not matter\n",
    "                target = 'swe_cm'\n",
    "                #remove the large amounts of zero values\n",
    "\n",
    "                df = dataloader.remove0swe(df, remove0swe, removeswe_thresh)\n",
    "                \n",
    "                if Train == True:\n",
    "\n",
    "                  #fit a scaler,save, and scale the training data\n",
    "                    x_train, y_train, x_test, y_test = dataprocessor2.xgb_processor(\n",
    "                                                                        WY_list,\n",
    "                                                                        df, \n",
    "                                                                        years, \n",
    "                                                                        splitratio,\n",
    "                                                                        test_years, \n",
    "                                                                        target, \n",
    "                                                                        input_columns, \n",
    "                                                                        model_path, \n",
    "                                                                        scalertype = 'Standard',\n",
    "                                                                        standard = standard,\n",
    "                                                                        alldata = dataused #setting this to true will use all available data to train the model, uses HOO to test, There will not be any plots\n",
    "                                                                        )\n",
    "\n",
    "                    #Train model\n",
    "                    bestparams = xgb_model.XGB_Train(model_path, \n",
    "                                        input_columns, \n",
    "                                        x_train, \n",
    "                                        y_train, \n",
    "                                        tries, \n",
    "                                        hyperparameters,\n",
    "                                        perc_data,\n",
    "                                        gridsearch)\n",
    "\n",
    "\n",
    "                print(f\"Making predictions on held out WY{year}, at {output_res}M resolution.\")\n",
    "                #load performance dataframes\n",
    "                try:\n",
    "                    print('Loading previous performance DataFrames')\n",
    "                    PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}_{output_res}.parquet\")\n",
    "                    VDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/VDF_{sim}_{output_res}.parquet\")\n",
    "                    SWEDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/SWEDF_{sim}_{output_res}.parquet\")\n",
    "\n",
    "                except:    \n",
    "                    print('No performance DataFrames found, making them...')\n",
    "                    PDF = pd.DataFrame()\n",
    "                    VDF = pd.DataFrame()\n",
    "                    SWEDF = pd.DataFrame()\n",
    "\n",
    "\n",
    "                #make a predition, save prediction, evaluate scene, add to overal model performance list, make cdf\n",
    "                directory_path = f\"{HOME}/SWEMLv2.0/data/TrainingDFs/{year}/{output_res}/{DataFrame}/{fSCA_thresh}\"\n",
    "                #get all processed filesnames\n",
    "                files = [filename for filename in os.listdir(directory_path) if filename.endswith('.parquet')]\n",
    "               # remfile = '.ipynb_checkpoints'\n",
    "                # for entry in os.listdir(directory_path):\n",
    "                #     files.append(entry)\n",
    "                #files.remove(remfile)\n",
    "                #make predictions on each date\n",
    "                for file in files:\n",
    "                    #Get watershed name\n",
    "                    watershed = file.split('_')[-2]\n",
    "                    \n",
    "                    #read file and split into x_test, y_test\n",
    "                    TestArea = pd.read_parquet(f\"{directory_path}/{file}\")\n",
    "                    TestArea.reset_index(inplace=True)\n",
    "                    testdate = TestArea['Date'][0]\n",
    "                    testdate = testdate.strftime(\"%Y-%m-%d\")\n",
    "                    #print watershed and date for easy reference\n",
    "                    print(f\"Making prediction on the {watershed} watershed for {testdate}\")\n",
    "                    \n",
    "                    \n",
    "                    TestArea['swe_cm'] = TestArea['swe_m'] *100\n",
    "                    #remove outliers\n",
    "                    TestArea = TestArea[TestArea['swe_cm']<300]\n",
    "\n",
    "                    #Prep prediction data\n",
    "                    y_test_Area = pd.DataFrame(TestArea['swe_cm'])\n",
    "                    dropcols = ['cell_id','Date', 'swe_cm', 'swe_m']\n",
    "                    x_test_Area = TestArea.drop(columns=dropcols)\n",
    "                    x_test_Area = x_test_Area[input_columns]\n",
    "\n",
    "                     #make a prediction\n",
    "                    holdoutdate = xgb_model.XGB_Predict(\n",
    "                                        model_path, \n",
    "                                        modelname, \n",
    "                                        x_test_Area,\n",
    "                                        y_test_Area,\n",
    "                                        Use_fSCA_Threshold\n",
    "                                        )\n",
    "                    #Add geospatial information to prediction DF\n",
    "                    EvalDF = pd.concat([TestArea, holdoutdate], axis=1)\n",
    "                    EvalDF = EvalDF.loc[:,~EvalDF.columns.duplicated()].copy()\n",
    "\n",
    "                    # EvalDF.head(5)\n",
    "\n",
    "                    Ppath = f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/{output_res}/{fSCA_thresh}/{year}\"\n",
    "                    if not os.path.exists(Ppath):\n",
    "                        os.makedirs(Ppath, exist_ok=True)\n",
    "\n",
    "                    #save the model predictions\n",
    "                    table = pa.Table.from_pandas(EvalDF)\n",
    "                    # Parquet with Brotli compression\n",
    "                    pq.write_table(table, f\"{Ppath}/HoldWYsout_{output_res}_{watershed}_{testdate}.parquet\", compression='BROTLI')\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    #Begin Evaluation\n",
    "                    savfig = True\n",
    "                    markersize = 75\n",
    "                    figlocation = f\"{HOME}/SWEMLv2.0/Evaluation/Figures/{DataFrame}\"\n",
    "                    if not os.path.exists(figlocation):\n",
    "                        os.makedirs(figlocation,exist_ok=True)\n",
    "\n",
    "                    #Change obs/pred column names if necessary\n",
    "                    EvalDF.rename(columns={'ASO_swe_cm':'y_test','XGBoost_swe_cm':'y_pred'}, inplace=True)\n",
    "\n",
    "                    #just get swe >1cm\n",
    "                    EvalDF = EvalDF[EvalDF['y_test']>1]\n",
    "\n",
    "                    #Parity Plot\n",
    "                    Performance = SSWEET.parityplot(EvalDF, savfig, watershed, testdate, sim, DataFrame)\n",
    "                    Performance\n",
    "\n",
    "                    #Error vs Elevation\n",
    "                    # SSWEET.Model_Vs(EvalDF,'Elevation_m', 'Error', savfig, watershed, testdate, sim)\n",
    "\n",
    "                    #make path if it does not exist for evaluation figures\n",
    "                    figpath = f\"./SWEMLv2.0/Evaluation/Figures/{DataFrame}/{sim}/{year}/{output_res}\"\n",
    "                    if not os.path.exists(figpath):\n",
    "                        os.makedirs(figpath, exist_ok=True)\n",
    "\n",
    "#                     #spatial\n",
    "                    cmap = 'viridis' # use seismic for error, viridis or blues for preds/obs\n",
    "                    var =  'y_pred' #'error'\n",
    "                    var_short = 'Pred'\n",
    "                    Title = f'SWEMLv2.0 Model {var_short} {testdate} \\n {watershed} River Basin, {output_res}'\n",
    "                    variant = 'World_Imagery'\n",
    "                    figname = f\"{figpath}/{watershed}_spatial_{var}_{testdate}.png\"\n",
    "                    SSWEET.SpatialAnalysis(EvalDF, markersize, cmap, var,Title, savfig, variant, figname)\n",
    "\n",
    "#                     #spatial\n",
    "                    cmap = 'viridis' # use seismic for error, viridis or blues for preds/obs\n",
    "                    var =  'y_test' #'error'\n",
    "                    var_short = 'test'\n",
    "                    Title = f'SWEMLv2.0 Model {var_short} {testdate} \\n {watershed} River Basin, {output_res}'\n",
    "                    variant = 'World_Imagery'\n",
    "                    figname = f\"{figpath}/{watershed}_spatial_{var}_{testdate}.png\"\n",
    "                    SSWEET.SpatialAnalysis(EvalDF, markersize, cmap, var,Title, savfig, variant, figname)\n",
    "\n",
    "\n",
    "#                     #spatial\n",
    "                    cmap = 'seismic' # use seismic for error, viridis or blues for preds/obs\n",
    "                    var =  'error'\n",
    "                    var_short = 'Error'\n",
    "                    Title = f'SWEMLv2.0 Model {var_short} {testdate} \\n {watershed} River Basin, {output_res}'\n",
    "                    variant = 'World_Imagery'\n",
    "                    figname = f\"{figpath}/{watershed}_spatial_{var}_{testdate}.png\"\n",
    "                    # SSWEET.SpatialAnalysis(EvalDF, markersize, cmap, var,Title, savfig, variant, figname)\n",
    "\n",
    "                    # cols =['Volume_Difference', 'Predicted_Volume', 'Observed_Volume']\n",
    "                    # scaler = 10000\n",
    "                    # ylab = f'Frozen Water Volume (x{scaler}m3)'\n",
    "                    # ncol = 3\n",
    "                    # Title = f'Volumetric Frozen Water Content at Low, Mid, and High Elevation Bands \\n {watershed} River Basin {testdate}, {sim}, {output_res}'\n",
    "                    # figname = f\"{figpath}/{watershed}_SWEVol_{var}_{testdate}.png\"\n",
    "\n",
    "                    # Vdf = SSWEET.barplot(EvalDF, cols, scaler, ylab, ncol, Title, savfig, figname)\n",
    "                    # Vdf['watershed'] = watershed\n",
    "                    # Vdf['Date'] = testdate\n",
    "\n",
    "#                     cols =['Mean_SWE_Prediction', 'Mean_SWE_Observation']\n",
    "#                     scaler = 1\n",
    "#                     ylab = f'Snow-Water-Equivalent (cm)'\n",
    "#                     ncol = 3\n",
    "#                     Title = f'Mean SWE at Low, Mid, and High Elevation Bands \\n {watershed} River Basin, {testdate} ,  {sim}, {output_res}'\n",
    "#                     figname = f\"{figpath}/{watershed}_MeanSWE_{var}_{testdate}.png\"\n",
    "\n",
    "                    # SWEdf = SSWEET.barplot(EvalDF, cols, scaler, ylab, ncol, Title, savfig, figname)\n",
    "                    # SWEdf['watershed'] = watershed\n",
    "                    # SWEdf['Date'] = testdate\n",
    "\n",
    "\n",
    "                    cols = ['RMSE', 'KGE', 'PBias', 'R', 'R2', 'MAE']\n",
    "                    Performance = Performance[cols]\n",
    "                    Performance['watershed'] = watershed\n",
    "                    Performance['Date'] = testdate\n",
    "                    Performance['Resolution'] = output_res\n",
    "                    Performance['Objective'] = hyperparameters['objective']\n",
    "                    \n",
    "                    # Performance['n_estimators'] = bestparams['n_estimators']\n",
    "                    # Performance['max_depth'] = bestparams['max_depth']\n",
    "                    # Performance['eta'] = bestparams['eta']\n",
    "                    # Performance['alldata'] = dataused\n",
    "                    # Performance['standard_training'] = standard\n",
    "\n",
    "                    PDF = pd.concat([PDF, Performance])\n",
    "                    # VDF = pd.concat([VDF, Vdf])\n",
    "                    # SWEDF = pd.concat([SWEDF, SWEdf])\n",
    "\n",
    "                    #save Performance Dataframes\n",
    "                    xgb_model.XGB_Perf_Save(PDF, f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/\", f\"PDF_{sim}_{output_res}\")\n",
    "                    # xgb_model.XGB_Perf_Save(VDF, f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/\", f\"VDF_{sim}_{output_res}\")\n",
    "                    # xgb_model.XGB_Perf_Save(SWEDF, f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/\", f\"SWEDF_{sim}_{output_res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_scripts import SSWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial\n",
    "cmap = 'viridis' # use seismic for error, viridis or blues for preds/obs\n",
    "var =  'NLDAS' #'error'\n",
    "var_short = 'Precip'\n",
    "sim = \"Test_WY2021\" \n",
    "output_res = 1000\n",
    "year = 2021\n",
    "figpath = f\"./SWEMLv2.0/Evaluation/Figures/{sim}/{year}/{output_res}\"\n",
    "# Title = f'SWEMLv2.0 Model {var_short} {testdate} \\n {watershed} River Basin, {output_res}'\n",
    "# variant = 'OpenStreetMap'\n",
    "# figname = f\"{figpath}/{watershed}_spatial_{var}_{testdate}.png\"\n",
    "SSWEET.SpatialAnalysis(EvalDF, markersize, cmap, var,Title, savfig, variant, figname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_res = '1000M_Resolution'\n",
    "SSWEET.plot_cdf('KGE',output_res, title=\"SWEMLv2.0 WY Hold Out Cumulative Distribution Function for KGE\",\n",
    "                     xlabel=\"KGE Value\", ylabel=\"Cumulative Probability\")# Notes from model input variable analysis\n",
    "* Observed SWE skewed to the right, way more low swe values in training than high values\n",
    "* lots of -9999 values for snotel observations - need to see if bad data processing or unavailable snotel values, this is messing up the site swe anomoaly, mean, and observed values.\n",
    "* Snotel magnitude is incorrect. find out why snotel data is the wrong magnitude and develop method to only select sites with good values - e.g. remove sites that have incomplete or bad data from the obs dataframe and remove from the gridcell location to snotel dictionary.\n",
    "* change name of rel column to swe anomoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_res = '1000M_Resolution'\n",
    "SSWEET.plot_cdf('KGE',output_res, title=\"SWEMLv2.0 WY Hold Out Cumulative Distribution Function for KGE\",\n",
    "                     xlabel=\"KGE Value\", ylabel=\"Cumulative Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2021_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2023_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2017_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2023_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2019_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2020_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2021_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2022_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2023_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DataFrame = 'Sturm_Seasonality_PrecipVIIRSGeoObsDFs'\n",
    "sim  =  f\"Test_WY2024_1000M_Resolution\" \n",
    "PDF = pd.read_parquet(f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/PDF_{sim}.parquet\")\n",
    "PDF = PDF.sort_values(by =['KGE', 'RMSE'], ascending =[False, False])\n",
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF[PDF['watershed']=='Taylor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF plot\n",
    "output_res = '1000M_Resolution'\n",
    "SSWEET.plot_cdf('KGE',output_res, title=\"SWEMLv2.0 WY Hold Out Cumulative Distribution Function for KGE\",\n",
    "                     xlabel=\"KGE Value\", ylabel=\"Cumulative Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save predictions as geotiff to share with Andy Wood\n",
    "DataFrame = 'NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs'\n",
    "fSCA_thresh = '10_fSCA_Thresh' \n",
    "res= 1000\n",
    "output_res = f\"{res}M_Resolution\"\n",
    "years = [ '2013',\n",
    "    # '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', \n",
    "         '2024'\n",
    "        ]\n",
    "for year in years:\n",
    "    print(year)\n",
    "    directory_path = f\"{HOME}/SWEMLv2.0/Predictions/{DataFrame}/{output_res}/{fSCA_thresh}/{year}\"\n",
    "    #file = 'HoldWYsout_1000M_Resolution_USCATB_2014-04-20.parquet'\n",
    "    Pred_col = 'XGBoost_swe_cm'\n",
    "\n",
    "    DF_to_GeoTiff.dataframe_to_geotiff(directory_path, Pred_col, Pred_col,\n",
    "                                 resolution_meters=1000, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "with rasterio.open(f'{HOME}/SWEMLv2.0/Predictions/NLDASDaymet_Vegetation_Sturm_Seasonality_VIIRSGeoObsDFs/1000M_Resolution/10_fSCA_Thresh/2019/GeoTiffs/HoldWYsout_1000M_Resolution_USCAKW_2019-03-17.tif') as src:\n",
    "    show(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(gdf.XGBoost_swe_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "\n",
    "\n",
    "filepath = output_file\n",
    "\n",
    "# def plot_geotiff(filepath):\n",
    "#     \"\"\"\n",
    "#     Loads and plots a GeoTIFF file to visually inspect its content.\n",
    "\n",
    "#     Args:\n",
    "#         filepath (str or Path): The path to the GeoTIFF file.\n",
    "#     \"\"\"\n",
    "filepath = Path(filepath)\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "with rasterio.open(filepath) as src:\n",
    "    # Print some metadata to verify\n",
    "    print(f\"Successfully opened GeoTIFF: {filepath.name}\")\n",
    "    print(f\"  CRS: {src.crs}\")\n",
    "    print(f\"  Bounds: {src.bounds}\")\n",
    "    print(f\"  Resolution: {src.res}\")\n",
    "    print(f\"  Width: {src.width}, Height: {src.height}\")\n",
    "    print(f\"  Number of bands: {src.count}\")\n",
    "    # Correct ways to get the data type:\n",
    "    print(src.meta['dtype']) # From the metadata dictionary\n",
    "    print(src.read(1).dtype) # From the actual NumPy array of the first band\n",
    "    # print(f\"  Data type: {src.dtype}\")\n",
    "    # print(f\"  NoData value: {src.nodata}\")\n",
    "\n",
    "    # Read the first band of the raster data\n",
    "    # If your GeoTIFF has multiple bands, you might need to specify which one\n",
    "    # or loop through them. For a single-band raster (like precipitation), read(1) is common.\n",
    "    raster_data = src.read(1)\n",
    "    print(f\"\\nSuccessfully read band 1 data. Shape: {raster_data.shape}, Dtype: {raster_data.dtype}\")\n",
    "    print(f\"  Min value: {np.nanmin(raster_data):.2f}\")\n",
    "    print(f\"  Max value: {np.nanmax(raster_data):.2f}\")\n",
    "\n",
    "    # Create a plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # rasterio.plot.show returns an AxesImage object\n",
    "    image_plot = show(raster_data, ax=ax, transform=src.transform, cmap='viridis')\n",
    "\n",
    "    # Add a colorbar\n",
    "   # if ax.images: # Check if there are any images on the axes\n",
    "    cbar = fig.colorbar(ax.images[0], ax=ax) # Use the first image added to the axes\n",
    "    cbar.set_label('SWE (cm)')\n",
    "                \n",
    "\n",
    "    # Set title and labels\n",
    "    ax.set_title(f\"GeoTIFF Plot: {filepath.name}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "# except rasterio.errors.RasterioIOError as e:\n",
    "#     print(f\"Error reading GeoTIFF file: {e}. The file might be corrupted or not a valid GeoTIFF.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An unexpected error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "\n",
    "    # Create a dummy GeoDataFrame\n",
    "    dummy_gdf_data = {\n",
    "        'value': [10, 20, 30, 40],\n",
    "        'geometry': [Point(0.1, 0.1), Point(0.2, 0.2), Point(0.3, 0.3), Point(0.4, 0.4)]\n",
    "    }\n",
    "    dummy_gdf = gpd.GeoDataFrame(dummy_gdf_data, crs=\"EPSG:4326\")\n",
    "\n",
    "    output_dir = Path(\"temp_geotiff_plot_test\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    test_geotiff_path = output_dir / \"test_output.tif\"\n",
    "\n",
    "    # Call the geodataframe_to_geotiff function (assuming it's available in your environment)\n",
    "    # You would typically have this function defined in your script or imported.\n",
    "    # For this example, we'll use a placeholder if the actual function isn't run.\n",
    "    # If you've run the Canvas code, you can call it directly:\n",
    "    # from your_module import geodataframe_to_geotiff # if saved to a module\n",
    "    # created_file = geodataframe_to_geotiff(dummy_gdf, test_geotiff_path, 'value', resolution_meters=100)\n",
    "\n",
    "    # --- Placeholder for demonstration if geodataframe_to_geotiff is not run ---\n",
    "    # If the above function call is commented out or not available,\n",
    "    # this will create a simple raster directly for plotting demonstration.\n",
    "    if not test_geotiff_path.exists():\n",
    "        print(\"Creating a simple dummy GeoTIFF for plotting demonstration...\")\n",
    "        from rasterio.transform import Affine\n",
    "        transform = Affine(0.1, 0, 0, 0, -0.1, 10) # Simple transform\n",
    "        with rasterio.open(\n",
    "            test_geotiff_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=10,\n",
    "            width=10,\n",
    "            count=1,\n",
    "            dtype=np.float32,\n",
    "            crs='EPSG:4326',\n",
    "            transform=transform,\n",
    "            nodata=-9999\n",
    "        ) as dst:\n",
    "            dummy_data = np.random.rand(10, 10) * 100\n",
    "            dst.write(dummy_data, 1)\n",
    "        print(\"Dummy GeoTIFF created for plotting test.\")\n",
    "    # --- End Placeholder ---\n",
    "\n",
    "    # Now, plot the created GeoTIFF\n",
    "    plot_geotiff(test_geotiff_path)\n",
    "\n",
    "# finally:\n",
    "    # Clean up dummy files and directory\n",
    "    if 'output_dir' in locals() and output_dir.exists():\n",
    "        print(\"\\nCleaning up dummy files and directory...\")\n",
    "        for f in output_dir.iterdir():\n",
    "            f.unlink()\n",
    "        output_dir.rmdir()\n",
    "        print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # Create a dummy DataFrame (similar to what you'd get from Earth Engine sampling)\n",
    "#     print(\"Creating dummy DataFrame for demonstration...\")\n",
    "#     data = {\n",
    "#         'longitude': np.linspace(-120.0, -119.0, 10),\n",
    "#         'latitude': np.linspace(37.7, 38.2, 10),\n",
    "#         'precipitation_mm': np.random.rand(10) * 100 # Random precipitation values\n",
    "#     }\n",
    "#     # Create a grid-like DataFrame for better rasterization example\n",
    "#     lons = np.arange(-120.0, -119.0, 0.125) # NLDAS-like resolution\n",
    "#     lats = np.arange(37.7, 38.2, 0.125)\n",
    "    \n",
    "#     # Create a meshgrid for all combinations\n",
    "#     lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    \n",
    "#     # Flatten grids and create random values\n",
    "#     dummy_df_data = {\n",
    "#         'longitude': lon_grid.flatten(),\n",
    "#         'latitude': lat_grid.flatten(),\n",
    "#         'precipitation_mm': np.random.rand(len(lon_grid.flatten())) * 100\n",
    "#     }\n",
    "#     dummy_df = pd.DataFrame(dummy_df_data)\n",
    "\n",
    "#     # Add some missing data points to test nodata_value\n",
    "#     dummy_df.loc[::5, 'precipitation_mm'] = np.nan # Introduce some NaNs\n",
    "\n",
    "#     print(\"Dummy DataFrame Head:\")\n",
    "#     print(dummy_df.head())\n",
    "#     print(f\"Dummy DataFrame Shape: {dummy_df.shape}\")\n",
    "\n",
    "    # # Define output path\n",
    "    # output_dir = Path(\"output_rasters\")\n",
    "    # output_dir.mkdir(exist_ok=True)\n",
    "    # output_file = output_dir / \"monthly_precip_raster.tif\"\n",
    "\n",
    "    # Convert DataFrame to GeoTIFF\n",
    "    print(\"\\nAttempting to convert DataFrame to GeoTIFF...\")\n",
    "    created_file = dataframe_to_geotiff(\n",
    "        dummy_df,\n",
    "        output_file,\n",
    "        value_column='precipitation_mm',\n",
    "        resolution=0.125, # Explicitly set NLDAS resolution\n",
    "        nodata_value=-9999 # A common nodata value\n",
    "    )\n",
    "\n",
    "    if created_file:\n",
    "        print(f\"\\nGeoTIFF created successfully at: {created_file}\")\n",
    "        # Optional: Verify by opening with rasterio\n",
    "        try:\n",
    "            with rasterio.open(created_file) as src:\n",
    "                print(f\"Verified GeoTIFF: {src.meta}\")\n",
    "                print(f\"Bounds: {src.bounds}\")\n",
    "                print(f\"Nodata value: {src.nodata}\")\n",
    "                # Read a small part of the data\n",
    "                data_read = src.read(1)\n",
    "                print(f\"Data array shape: {data_read.shape}\")\n",
    "                print(f\"Data array sample (top-left): {data_read[0, 0:5]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying GeoTIFF: {e}\")\n",
    "\n",
    "    # Clean up dummy directory and files\n",
    "    print(\"\\nCleaning up dummy files and directory...\")\n",
    "    if output_dir.exists():\n",
    "        for f in output_dir.iterdir():\n",
    "            f.unlink()\n",
    "        output_dir.rmdir()\n",
    "    print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_310",
   "language": "python",
   "name": "sweml_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
